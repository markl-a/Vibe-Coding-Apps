"""
æ¨¡å‹è¨“ç·´è…³æœ¬
è¨“ç·´å¤šå€‹æ¨¡å‹ä¸¦æ¯”è¼ƒæ€§èƒ½
"""

import pandas as pd
import numpy as np
import argparse
import os
from churn_predictor import ChurnPredictor
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style('whitegrid')


def load_data(data_dir='data'):
    """è¼‰å…¥è¨“ç·´ã€é©—è­‰å’Œæ¸¬è©¦è³‡æ–™"""
    print("è¼‰å…¥è³‡æ–™...")
    train_df = pd.read_csv(os.path.join(data_dir, 'train_customers.csv'))
    val_df = pd.read_csv(os.path.join(data_dir, 'val_customers.csv'))
    test_df = pd.read_csv(os.path.join(data_dir, 'test_customers.csv'))

    print(f"è¨“ç·´é›†: {len(train_df)} ç­†")
    print(f"é©—è­‰é›†: {len(val_df)} ç­†")
    print(f"æ¸¬è©¦é›†: {len(test_df)} ç­†")

    return train_df, val_df, test_df


def train_model(model_type, train_df, val_df, test_df):
    """
    è¨“ç·´å–®å€‹æ¨¡å‹

    Args:
        model_type: æ¨¡å‹é¡å‹
        train_df: è¨“ç·´è³‡æ–™
        val_df: é©—è­‰è³‡æ–™
        test_df: æ¸¬è©¦è³‡æ–™

    Returns:
        è¨“ç·´å¥½çš„æ¨¡å‹å’Œè©•ä¼°æŒ‡æ¨™
    """
    print(f"\n{'='*60}")
    print(f"è¨“ç·´ {model_type.upper()} æ¨¡å‹")
    print(f"{'='*60}")

    # åˆå§‹åŒ–é æ¸¬å™¨
    predictor = ChurnPredictor(model_type=model_type)

    # é è™•ç†è³‡æ–™
    X_train, y_train = predictor.preprocess(train_df, is_training=True)
    X_val, y_val = predictor.preprocess(val_df, is_training=False)
    X_test, y_test = predictor.preprocess(test_df, is_training=False)

    # è¨“ç·´æ¨¡å‹
    predictor.train(X_train, y_train, X_val, y_val)

    # è©•ä¼°æ¨¡å‹
    print("\nåœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°:")
    metrics = predictor.evaluate(X_test, y_test)

    # è¦–è¦ºåŒ–
    print("\nç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨...")
    predictor.plot_confusion_matrix(X_test, y_test)
    predictor.plot_roc_curve(X_test, y_test)
    predictor.plot_feature_importance(top_n=15)

    # å„²å­˜æ¨¡å‹
    os.makedirs('models', exist_ok=True)
    model_path = f'models/{model_type}_model.pkl'
    predictor.save_model(model_path)

    return predictor, metrics


def compare_models(models_metrics):
    """
    æ¯”è¼ƒå¤šå€‹æ¨¡å‹çš„æ€§èƒ½

    Args:
        models_metrics: å­—å…¸ï¼Œæ ¼å¼ç‚º {model_name: metrics}
    """
    print(f"\n{'='*60}")
    print("æ¨¡å‹æ€§èƒ½æ¯”è¼ƒ")
    print(f"{'='*60}")

    # å»ºç«‹æ¯”è¼ƒè¡¨æ ¼
    comparison_df = pd.DataFrame(models_metrics).T
    comparison_df = comparison_df.round(4)

    print("\n", comparison_df)

    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_model = comparison_df['auc'].idxmax()
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model.upper()} (AUC = {comparison_df.loc[best_model, 'auc']:.4f})")

    # è¦–è¦ºåŒ–æ¯”è¼ƒ
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']

    for idx, metric in enumerate(metrics):
        ax = axes[idx // 3, idx % 3]
        data = comparison_df[metric].sort_values(ascending=False)
        bars = ax.barh(range(len(data)), data.values)

        # æœ€ä½³æ¨¡å‹ç”¨ä¸åŒé¡è‰²æ¨™è¨˜
        colors = ['#2ecc71' if model == best_model else '#3498db' for model in data.index]
        for bar, color in zip(bars, colors):
            bar.set_color(color)

        ax.set_yticks(range(len(data)))
        ax.set_yticklabels(data.index)
        ax.set_xlabel('åˆ†æ•¸')
        ax.set_title(metric.upper(), fontweight='bold')
        ax.grid(axis='x', alpha=0.3)

        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for i, v in enumerate(data.values):
            ax.text(v, i, f' {v:.4f}', va='center')

    # éš±è—æœ€å¾Œä¸€å€‹å­åœ–
    axes[1, 2].axis('off')

    plt.tight_layout()
    plt.savefig('models/model_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

    print(f"\næ¯”è¼ƒåœ–è¡¨å·²å„²å­˜: models/model_comparison.png")

    return best_model


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='è¨“ç·´å®¢æˆ¶æµå¤±é æ¸¬æ¨¡å‹')
    parser.add_argument('--data-dir', type=str, default='data',
                        help='è³‡æ–™ç›®éŒ„è·¯å¾‘')
    parser.add_argument('--models', type=str, nargs='+',
                        default=['random_forest', 'xgboost', 'lightgbm'],
                        help='è¦è¨“ç·´çš„æ¨¡å‹é¡å‹')
    parser.add_argument('--compare', action='store_true',
                        help='æ¯”è¼ƒæ‰€æœ‰æ¨¡å‹')

    args = parser.parse_args()

    print("="*60)
    print("å®¢æˆ¶æµå¤±é æ¸¬æ¨¡å‹è¨“ç·´")
    print("="*60)

    # è¼‰å…¥è³‡æ–™
    train_df, val_df, test_df = load_data(args.data_dir)

    # è¨“ç·´æ¨¡å‹
    trained_models = {}
    models_metrics = {}

    for model_type in args.models:
        try:
            predictor, metrics = train_model(model_type, train_df, val_df, test_df)
            trained_models[model_type] = predictor
            models_metrics[model_type] = metrics
        except Exception as e:
            print(f"\nâŒ è¨“ç·´ {model_type} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            continue

    # æ¯”è¼ƒæ¨¡å‹
    if args.compare and len(models_metrics) > 1:
        best_model_name = compare_models(models_metrics)

        # è¤‡è£½æœ€ä½³æ¨¡å‹
        best_model_path = f'models/{best_model_name}_model.pkl'
        import shutil
        shutil.copy(best_model_path, 'models/best_model.pkl')
        print(f"\næœ€ä½³æ¨¡å‹å·²è¤‡è£½åˆ°: models/best_model.pkl")

    print("\n" + "="*60)
    print("âœ… è¨“ç·´å®Œæˆ!")
    print("="*60)


if __name__ == '__main__':
    main()
