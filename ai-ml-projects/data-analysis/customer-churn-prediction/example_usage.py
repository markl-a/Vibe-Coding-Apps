"""
å®¢æˆ¶æµå¤±é æ¸¬ - å®Œæ•´ä½¿ç”¨ç¯„ä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨å®¢æˆ¶æµå¤±é æ¸¬ç³»çµ±é€²è¡Œæµå¤±é¢¨éšªè©•ä¼°
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix, classification_report
import warnings
warnings.filterwarnings('ignore')

# è¨­ç½®ä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ============================================================================
# 1. ç”Ÿæˆç¤ºä¾‹å®¢æˆ¶æµå¤±æ•¸æ“š
# ============================================================================
def generate_sample_churn_data(n_samples=1000, random_state=42):
    """
    ç”Ÿæˆç¤ºä¾‹é›»ä¿¡å®¢æˆ¶æµå¤±æ•¸æ“š
    """
    np.random.seed(random_state)

    data = {
        'customer_id': [f'CUST_{i:05d}' for i in range(n_samples)],
        'tenure': np.random.randint(1, 73, n_samples),  # ä½¿ç”¨æœå‹™æœˆæ•¸
        'monthly_charges': np.random.uniform(20, 120, n_samples),
        'total_charges': np.random.uniform(100, 10000, n_samples),
        'phone_service': np.random.choice([0, 1], n_samples),
        'streaming_tv': np.random.choice([0, 1], n_samples),
        'streaming_movies': np.random.choice([0, 1], n_samples),
        'tech_support': np.random.choice([0, 1], n_samples),
        'online_security': np.random.choice([0, 1], n_samples),
        'device_protection': np.random.choice([0, 1], n_samples),
        'paperless_billing': np.random.choice([0, 1], n_samples),
        'senior_citizen': np.random.choice([0, 1], n_samples),
        'partner': np.random.choice([0, 1], n_samples),
        'dependents': np.random.choice([0, 1], n_samples),
    }

    df = pd.DataFrame(data)

    # æ·»åŠ é¡åˆ¥è®Šæ•¸
    df['contract_type'] = np.random.choice(['Month-to-month', 'One year', 'Two year'], n_samples)
    df['internet_service'] = np.random.choice(['DSL', 'Fiber optic', 'No'], n_samples)
    df['payment_method'] = np.random.choice(['Electronic check', 'Mailed check', 'Bank transfer', 'Credit card'], n_samples)

    # ç”Ÿæˆæµå¤±ç›®æ¨™è®Šæ•¸
    # çŸ­æœŸåˆç´„ã€é«˜æœˆè²»ã€ç„¡é™„åŠ æœå‹™ -> é«˜æµå¤±é¢¨éšª
    churn_prob = (
        0.30 * (df['contract_type'] == 'Month-to-month').astype(int) +
        0.02 * (df['monthly_charges'] > 80).astype(int) +
        0.15 * ((df['tech_support'] + df['online_security'] + df['device_protection']) == 0).astype(int) +
        0.10 * (df['tenure'] < 12).astype(int) +
        0.05 * (df['senior_citizen'] == 1).astype(int) +
        0.02  # åŸºç¤æµå¤±ç‡
    )
    churn_prob = np.clip(churn_prob, 0, 1)
    df['churn'] = (np.random.random(n_samples) < churn_prob).astype(int)

    return df


# ============================================================================
# 2. æ•¸æ“šåˆ†æå’Œé è™•ç†
# ============================================================================
def analyze_and_preprocess_data(df):
    """
    åˆ†æå’Œé è™•ç†å®¢æˆ¶æµå¤±æ•¸æ“š
    """
    print("=" * 80)
    print("1. æ•¸æ“šåˆ†æå’Œé è™•ç†")
    print("=" * 80)

    # åŸºæœ¬ä¿¡æ¯
    print("\næ•¸æ“šé›†æ¦‚æ³:")
    print(f"  ç¸½å®¢æˆ¶æ•¸: {len(df)}")
    print(f"  ç‰¹å¾µæ•¸: {df.shape[1]}")

    # ç¼ºå¤±å€¼æª¢æŸ¥
    missing = df.isnull().sum()
    if missing.sum() > 0:
        print("\nç¼ºå¤±å€¼:")
        print(missing[missing > 0])
    else:
        print("\nâœ… ç„¡ç¼ºå¤±å€¼")

    # æµå¤±ç‡
    churn_rate = df['churn'].mean()
    print(f"\næµå¤±ç‡:")
    print(f"  ä¿ç•™å®¢æˆ¶: {(df['churn'] == 0).sum()} ({(df['churn'] == 0).mean()*100:.1f}%)")
    print(f"  æµå¤±å®¢æˆ¶: {(df['churn'] == 1).sum()} ({(df['churn'] == 1).mean()*100:.1f}%)")

    # æ•¸æ“šé è™•ç†
    df_processed = df.copy()

    # ç·¨ç¢¼é¡åˆ¥è®Šæ•¸
    le_contract = LabelEncoder()
    le_internet = LabelEncoder()
    le_payment = LabelEncoder()

    df_processed['contract_type'] = le_contract.fit_transform(df_processed['contract_type'])
    df_processed['internet_service'] = le_internet.fit_transform(df_processed['internet_service'])
    df_processed['payment_method'] = le_payment.fit_transform(df_processed['payment_method'])

    return df_processed


# ============================================================================
# 3. ç‰¹å¾µåˆ†æ
# ============================================================================
def analyze_features(df):
    """
    åˆ†æç‰¹å¾µèˆ‡æµå¤±çš„é—œä¿‚
    """
    print("\nç‰¹å¾µèˆ‡æµå¤±çš„é—œä¿‚åˆ†æ:")

    # è¨ˆç®—æ¯å€‹ç‰¹å¾µçš„æµå¤±ç‡
    feature_churn_rates = {}

    for col in df.columns:
        if col not in ['customer_id', 'churn']:
            if df[col].dtype in ['int64', 'float64']:
                # å°æ–¼æ•¸å€¼ç‰¹å¾µï¼Œè¨ˆç®—ç›¸é—œæ€§
                if col not in ['monthly_charges', 'total_charges', 'tenure']:
                    continue
                mean_churn = df[df[col] == 1]['churn'].mean() if df[col].sum() > 0 else 0
                feature_churn_rates[f'{col}=1'] = mean_churn
            else:
                for val in df[col].unique():
                    mean_churn = df[df[col] == val]['churn'].mean()
                    feature_churn_rates[f'{col}={val}'] = mean_churn

    # æ’åºä¸¦é¡¯ç¤º
    sorted_rates = sorted(feature_churn_rates.items(), key=lambda x: x[1], reverse=True)
    print("\næµå¤±ç‡æœ€é«˜çš„ç‰¹å¾µå€¼:")
    for feature, rate in sorted_rates[:10]:
        print(f"  {feature}: {rate:.1%}")


# ============================================================================
# 4. æ¨¡å‹è¨“ç·´
# ============================================================================
def train_churn_models(X_train, y_train, X_test, y_test):
    """
    è¨“ç·´å®¢æˆ¶æµå¤±é æ¸¬æ¨¡å‹
    """
    print("\n" + "=" * 80)
    print("2. æ¨¡å‹è¨“ç·´")
    print("=" * 80)

    # ç‰¹å¾µç¸®æ”¾
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    models = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
    }

    results = {}

    for name, model in models.items():
        print(f"\nè¨“ç·´ {name}...")
        model.fit(X_train_scaled, y_train)

        # é æ¸¬
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

        # è©•ä¼°
        acc = accuracy_score(y_test, y_pred)
        prec = precision_score(y_test, y_pred, zero_division=0)
        rec = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        auc = roc_auc_score(y_test, y_pred_proba)

        results[name] = {
            'model': model,
            'scaler': scaler,
            'accuracy': acc,
            'precision': prec,
            'recall': rec,
            'f1': f1,
            'auc': auc,
            'predictions': y_pred,
            'probabilities': y_pred_proba,
            'y_test': y_test
        }

        print(f"  Accuracy:  {acc:.4f}")
        print(f"  Precision: {prec:.4f}")
        print(f"  Recall:    {rec:.4f}")
        print(f"  F1-Score:  {f1:.4f}")
        print(f"  AUC-ROC:   {auc:.4f}")

    return results, scaler


# ============================================================================
# 5. æ··æ·†çŸ©é™£åˆ†æ
# ============================================================================
def analyze_confusion_matrix(results):
    """
    åˆ†ææ··æ·†çŸ©é™£
    """
    print("\n" + "=" * 80)
    print("3. æ··æ·†çŸ©é™£åˆ†æ")
    print("=" * 80)

    for name, result in results.items():
        print(f"\n{name}:")
        cm = confusion_matrix(result['y_test'], result['predictions'])
        print(f"  çœŸè² ä¾‹ (TN): {cm[0, 0]}")
        print(f"  å‡æ­£ä¾‹ (FP): {cm[0, 1]}")
        print(f"  å‡è² ä¾‹ (FN): {cm[1, 0]}")
        print(f"  çœŸæ­£ä¾‹ (TP): {cm[1, 1]}")

        print(f"\nè©³ç´°åˆ†é¡å ±å‘Š:")
        print(classification_report(result['y_test'], result['predictions'], zero_division=0))


# ============================================================================
# 6. ç‰¹å¾µé‡è¦æ€§åˆ†æ
# ============================================================================
def analyze_feature_importance(best_result, feature_names):
    """
    åˆ†æç‰¹å¾µé‡è¦æ€§
    """
    print("\n" + "=" * 80)
    print("4. ç‰¹å¾µé‡è¦æ€§åˆ†æ")
    print("=" * 80)

    model = best_result['model']

    if hasattr(model, 'feature_importances_'):
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)

        print("\nå½±éŸ¿å®¢æˆ¶æµå¤±çš„ä¸»è¦ç‰¹å¾µ (å‰15å€‹):")
        print(importance_df.head(15).to_string(index=False))

        return importance_df
    else:
        print("\næ­¤æ¨¡å‹ä¸æ”¯æ´ç‰¹å¾µé‡è¦æ€§åˆ†æ")
        return None


# ============================================================================
# 7. å–®å®¢æˆ¶æµå¤±é æ¸¬
# ============================================================================
def predict_customer_churn(model, scaler, customer_data, feature_names):
    """
    é æ¸¬å–®å€‹å®¢æˆ¶çš„æµå¤±é¢¨éšª
    """
    print("\n" + "=" * 80)
    print("5. å–®å®¢æˆ¶æµå¤±é¢¨éšªè©•ä¼°")
    print("=" * 80)

    # æº–å‚™ç‰¹å¾µ
    X = pd.DataFrame([customer_data], columns=feature_names)
    X_scaled = scaler.transform(X)

    # é æ¸¬
    churn_prob = model.predict_proba(X_scaled)[0, 1]
    churn_class = model.predict(X_scaled)[0]

    # é¢¨éšªç­‰ç´š
    if churn_prob < 0.3:
        risk_level = 'ä½é¢¨éšª'
        action = 'ç¶­æŒç¾æœ‰æœå‹™'
    elif churn_prob < 0.7:
        risk_level = 'ä¸­é¢¨éšª'
        action = 'ä¸»å‹•é—œæ‡·ï¼Œè©•ä¼°å®¢æˆ¶éœ€æ±‚'
    else:
        risk_level = 'é«˜é¢¨éšª'
        action = 'å„ªå…ˆæŒ½ç•™ï¼Œæä¾›å„ªæƒ æ–¹æ¡ˆ'

    print(f"\nå®¢æˆ¶æµå¤±é¢¨éšªè©•ä¼°:")
    print(f"  æµå¤±æ¦‚ç‡: {churn_prob:.1%}")
    print(f"  é¢¨éšªç­‰ç´š: {risk_level}")
    print(f"  æ¨è–¦è¡Œå‹•: {action}")

    return {
        'churn_probability': churn_prob,
        'risk_level': risk_level,
        'recommendation': action
    }


# ============================================================================
# 8. æ‰¹é‡æµå¤±é æ¸¬
# ============================================================================
def batch_churn_prediction(model, scaler, df_batch, feature_names):
    """
    æ‰¹é‡é æ¸¬å®¢æˆ¶æµå¤±
    """
    print("\n" + "=" * 80)
    print("6. æ‰¹é‡æµå¤±é¢¨éšªè©•ä¼°")
    print("=" * 80)

    # ç§»é™¤éç‰¹å¾µæ¬„ä½
    X_batch = df_batch[feature_names]
    X_scaled = scaler.transform(X_batch)

    # é æ¸¬
    churn_probs = model.predict_proba(X_scaled)[:, 1]

    # ç¢ºå®šé¢¨éšªç­‰ç´š
    risk_levels = []
    for prob in churn_probs:
        if prob < 0.3:
            risk_levels.append('ä½é¢¨éšª')
        elif prob < 0.7:
            risk_levels.append('ä¸­é¢¨éšª')
        else:
            risk_levels.append('é«˜é¢¨éšª')

    results_df = pd.DataFrame({
        'customer_id': df_batch['customer_id'].values if 'customer_id' in df_batch.columns else range(len(churn_probs)),
        'churn_probability': churn_probs,
        'risk_level': risk_levels
    })

    # çµ±è¨ˆ
    print(f"\næ‰¹é‡è©•ä¼°çµæœçµ±è¨ˆ:")
    print(f"  è©•ä¼°å®¢æˆ¶æ•¸: {len(results_df)}")
    print(f"\né¢¨éšªç­‰ç´šåˆ†ä½ˆ:")
    risk_dist = results_df['risk_level'].value_counts()
    for risk, count in risk_dist.items():
        print(f"  {risk}: {count} ({count/len(results_df)*100:.1f}%)")

    print(f"\næµå¤±æ¦‚ç‡çµ±è¨ˆ:")
    print(f"  å¹³å‡: {results_df['churn_probability'].mean():.2%}")
    print(f"  ä¸­ä½æ•¸: {results_df['churn_probability'].median():.2%}")
    print(f"  æœ€å°: {results_df['churn_probability'].min():.2%}")
    print(f"  æœ€å¤§: {results_df['churn_probability'].max():.2%}")

    # é«˜é¢¨éšªå®¢æˆ¶
    high_risk = results_df[results_df['risk_level'] == 'é«˜é¢¨éšª']
    print(f"\né«˜é¢¨éšªå®¢æˆ¶è©³æƒ… (å‰10å€‹):")
    print(high_risk.head(10).to_string(index=False))

    return results_df


# ============================================================================
# 9. å¯è¦–åŒ–
# ============================================================================
def visualize_results(df, results, importance_df=None):
    """
    å¯è¦–åŒ–åˆ†æçµæœ
    """
    print("\n" + "=" * 80)
    print("7. çµæœå¯è¦–åŒ–")
    print("=" * 80)

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # 1. æµå¤±ç‡åˆ†ä½ˆ
    churn_counts = df['churn'].value_counts()
    axes[0, 0].bar(['ä¿ç•™å®¢æˆ¶', 'æµå¤±å®¢æˆ¶'], churn_counts.values)
    axes[0, 0].set_title('å®¢æˆ¶æµå¤±åˆ†ä½ˆ')
    axes[0, 0].set_ylabel('è¨ˆæ•¸')

    # 2. åˆç´„é¡å‹vsæµå¤±
    contract_churn = df.groupby('contract_type')['churn'].agg(['sum', 'count'])
    contract_churn['rate'] = contract_churn['sum'] / contract_churn['count']
    axes[0, 1].bar(range(len(contract_churn)), contract_churn['rate'].values)
    axes[0, 1].set_title('åˆç´„é¡å‹èˆ‡æµå¤±ç‡')
    axes[0, 1].set_ylabel('æµå¤±ç‡')
    axes[0, 1].set_xticks(range(len(contract_churn)))
    axes[0, 1].set_xticklabels(contract_churn.index, rotation=45, ha='right')

    # 3. ä½¿ç”¨æœˆæ•¸vsæµå¤±
    tenure_bins = pd.cut(df['tenure'], bins=6)
    tenure_churn = df.groupby(tenure_bins)['churn'].agg(['sum', 'count'])
    tenure_churn['rate'] = tenure_churn['sum'] / tenure_churn['count']
    axes[1, 0].plot(range(len(tenure_churn)), tenure_churn['rate'].values, marker='o')
    axes[1, 0].set_title('ä½¿ç”¨æœˆæ•¸èˆ‡æµå¤±ç‡')
    axes[1, 0].set_ylabel('æµå¤±ç‡')
    axes[1, 0].set_xlabel('ä½¿ç”¨æœˆæ•¸å€é–“')

    # 4. ç‰¹å¾µé‡è¦æ€§
    if importance_df is not None:
        top_features = importance_df.head(10)
        axes[1, 1].barh(top_features['feature'], top_features['importance'])
        axes[1, 1].set_title('å‰10å€‹æœ€é‡è¦ç‰¹å¾µ')
        axes[1, 1].set_xlabel('é‡è¦æ€§')
    else:
        axes[1, 1].text(0.5, 0.5, 'ç‰¹å¾µé‡è¦æ€§åˆ†æä¸å¯ç”¨', ha='center', va='center')

    plt.tight_layout()
    plt.savefig('churn_analysis.png', dpi=300, bbox_inches='tight')
    print("\nâœ… åœ–è¡¨å·²ä¿å­˜ç‚º: churn_analysis.png")
    plt.show()


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================
def main():
    """
    å®Œæ•´çš„å®¢æˆ¶æµå¤±é æ¸¬ç¤ºä¾‹
    """
    print("\n" + "=" * 80)
    print("å®¢æˆ¶æµå¤±é æ¸¬ - å®Œæ•´ä½¿ç”¨ç¯„ä¾‹")
    print("=" * 80)

    # 1. ç”Ÿæˆæ•¸æ“š
    print("\næº–å‚™æ•¸æ“š...")
    df = generate_sample_churn_data(n_samples=1000)
    df = analyze_and_preprocess_data(df)
    analyze_features(df)

    # 2. åˆ†å‰²æ•¸æ“š
    feature_cols = [col for col in df.columns if col not in ['customer_id', 'churn']]
    X = df[feature_cols]
    y = df['churn']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"\næ•¸æ“šåˆ†å‰²:")
    print(f"  è¨“ç·´é›†: {len(X_train)}")
    print(f"  æ¸¬è©¦é›†: {len(X_test)}")

    # 3. è¨“ç·´æ¨¡å‹
    results, scaler = train_churn_models(X_train, y_train, X_test, y_test)

    # 4. æ··æ·†çŸ©é™£åˆ†æ
    analyze_confusion_matrix(results)

    # 5. é¸æ“‡æœ€ä½³æ¨¡å‹
    best_model_name = max(results.keys(), key=lambda x: results[x]['f1'])
    best_result = results[best_model_name]
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")

    # 6. ç‰¹å¾µé‡è¦æ€§
    importance_df = analyze_feature_importance(best_result, feature_cols)

    # 7. å–®å®¢æˆ¶é æ¸¬
    sample_customer = {
        'tenure': 12,
        'monthly_charges': 65.5,
        'total_charges': 786.0,
        'phone_service': 1,
        'streaming_tv': 1,
        'streaming_movies': 0,
        'tech_support': 0,
        'online_security': 0,
        'device_protection': 0,
        'paperless_billing': 1,
        'senior_citizen': 0,
        'partner': 1,
        'dependents': 0,
        'contract_type': 0,  # Month-to-month
        'internet_service': 0,  # DSL
        'payment_method': 0   # Electronic check
    }
    churn_prediction = predict_customer_churn(
        best_result['model'], scaler, sample_customer, feature_cols
    )

    # 8. æ‰¹é‡é æ¸¬
    batch_results = batch_churn_prediction(
        best_result['model'], scaler, X_test.iloc[:100].assign(customer_id=df['customer_id'].iloc[len(X_train):len(X_train)+100].values), feature_cols
    )

    # 9. å¯è¦–åŒ–
    visualize_results(df, results, importance_df)

    print("\n" + "=" * 80)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("=" * 80 + "\n")


if __name__ == '__main__':
    main()
