"""
AI-Assisted Data Analysis
æä¾›AIè¾…åŠ©çš„æ•°æ®åˆ†æã€æ´å¯Ÿç”Ÿæˆå’Œè‡ªåŠ¨åŒ–å»ºè®®
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
import warnings
warnings.filterwarnings('ignore')


class AIDataAssistant:
    """AIè¾…åŠ©æ•°æ®åˆ†æå™¨"""

    def __init__(self, df: pd.DataFrame):
        """
        åˆå§‹åŒ–AIæ•°æ®åŠ©æ‰‹

        Args:
            df: è¾“å…¥æ•°æ®æ¡†
        """
        self.df = df.copy()
        self.numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        self.categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
        self.insights = []

    def generate_data_quality_report(self) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š

        Returns:
            åŒ…å«æ•°æ®è´¨é‡ä¿¡æ¯çš„å­—å…¸
        """
        report = {
            'overview': {},
            'missing_values': {},
            'duplicates': {},
            'outliers': {},
            'data_types': {},
            'recommendations': []
        }

        # æ¦‚è§ˆ
        report['overview'] = {
            'total_rows': len(self.df),
            'total_columns': len(self.df.columns),
            'numeric_columns': len(self.numeric_cols),
            'categorical_columns': len(self.categorical_cols),
            'memory_usage_mb': self.df.memory_usage(deep=True).sum() / 1024**2
        }

        # ç¼ºå¤±å€¼åˆ†æ
        missing = self.df.isnull().sum()
        missing_pct = (missing / len(self.df)) * 100
        report['missing_values'] = {
            'columns_with_missing': missing[missing > 0].to_dict(),
            'missing_percentage': missing_pct[missing_pct > 0].to_dict(),
            'total_missing': missing.sum()
        }

        # é‡å¤å€¼
        duplicates = self.df.duplicated().sum()
        report['duplicates'] = {
            'count': int(duplicates),
            'percentage': float(duplicates / len(self.df) * 100)
        }

        # å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆæ•°å€¼åˆ—ï¼‰
        outliers_info = {}
        for col in self.numeric_cols:
            Q1 = self.df[col].quantile(0.25)
            Q3 = self.df[col].quantile(0.75)
            IQR = Q3 - Q1
            outliers = ((self.df[col] < Q1 - 1.5 * IQR) |
                       (self.df[col] > Q3 + 1.5 * IQR)).sum()
            if outliers > 0:
                outliers_info[col] = {
                    'count': int(outliers),
                    'percentage': float(outliers / len(self.df) * 100)
                }
        report['outliers'] = outliers_info

        # æ•°æ®ç±»å‹
        report['data_types'] = self.df.dtypes.astype(str).to_dict()

        # ç”Ÿæˆå»ºè®®
        report['recommendations'] = self._generate_quality_recommendations(report)

        return report

    def _generate_quality_recommendations(self, report: Dict) -> List[str]:
        """ç”Ÿæˆæ•°æ®è´¨é‡æ”¹è¿›å»ºè®®"""
        recommendations = []

        # ç¼ºå¤±å€¼å»ºè®®
        if report['missing_values']['total_missing'] > 0:
            high_missing = {k: v for k, v in report['missing_values']['missing_percentage'].items() if v > 50}
            if high_missing:
                recommendations.append(
                    f"âš ï¸ å‘ç° {len(high_missing)} åˆ—ç¼ºå¤±å€¼è¶…è¿‡50%ï¼Œå»ºè®®è€ƒè™‘åˆ é™¤è¿™äº›åˆ—: {list(high_missing.keys())}"
                )
            else:
                recommendations.append(
                    "ğŸ’¡ å»ºè®®ä½¿ç”¨åˆé€‚çš„æ–¹æ³•å¡«å……ç¼ºå¤±å€¼ï¼ˆå‡å€¼ã€ä¸­ä½æ•°ã€ä¼—æ•°æˆ–é¢„æµ‹å¡«å……ï¼‰"
                )

        # é‡å¤å€¼å»ºè®®
        if report['duplicates']['percentage'] > 5:
            recommendations.append(
                f"âš ï¸ å‘ç° {report['duplicates']['percentage']:.1f}% çš„é‡å¤æ•°æ®ï¼Œå»ºè®®æ£€æŸ¥å¹¶å¤„ç†"
            )

        # å¼‚å¸¸å€¼å»ºè®®
        if report['outliers']:
            recommendations.append(
                f"ğŸ’¡ åœ¨ {len(report['outliers'])} åˆ—ä¸­æ£€æµ‹åˆ°å¼‚å¸¸å€¼ï¼Œå»ºè®®è¿›ä¸€æ­¥åˆ†ææ˜¯å¦ä¸ºçœŸå®å¼‚å¸¸æˆ–æ•°æ®é”™è¯¯"
            )

        # æ•°æ®ç±»å‹å»ºè®®
        for col in self.df.columns:
            if self.df[col].dtype == 'object':
                unique_ratio = self.df[col].nunique() / len(self.df)
                if unique_ratio > 0.5:
                    recommendations.append(
                        f"ğŸ’¡ åˆ— '{col}' å¯èƒ½æ˜¯é«˜åŸºæ•°ç±»åˆ«å˜é‡ï¼ˆå”¯ä¸€å€¼æ¯”ä¾‹: {unique_ratio:.1%}ï¼‰ï¼Œè€ƒè™‘ç‰¹æ®Šç¼–ç æ–¹å¼"
                    )

        return recommendations

    def auto_detect_column_types(self) -> Dict[str, str]:
        """
        è‡ªåŠ¨æ£€æµ‹åˆ—çš„è¯­ä¹‰ç±»å‹

        Returns:
            åˆ—ååˆ°è¯­ä¹‰ç±»å‹çš„æ˜ å°„
        """
        column_types = {}

        for col in self.df.columns:
            col_type = 'unknown'

            # æ£€æŸ¥æ˜¯å¦ä¸ºIDåˆ—
            if 'id' in col.lower():
                col_type = 'identifier'

            # æ£€æŸ¥æ˜¯å¦ä¸ºæ—¥æœŸåˆ—
            elif 'date' in col.lower() or 'time' in col.lower():
                col_type = 'datetime'

            # æ£€æŸ¥æ˜¯å¦ä¸ºç±»åˆ«åˆ—
            elif self.df[col].dtype == 'object':
                unique_ratio = self.df[col].nunique() / len(self.df)
                if unique_ratio < 0.05:
                    col_type = 'categorical_low_cardinality'
                elif unique_ratio < 0.5:
                    col_type = 'categorical_medium_cardinality'
                else:
                    col_type = 'categorical_high_cardinality'

            # æ•°å€¼åˆ—
            elif self.df[col].dtype in ['int64', 'float64']:
                # æ£€æŸ¥æ˜¯å¦ä¸ºäºŒå…ƒå˜é‡
                if self.df[col].nunique() == 2:
                    col_type = 'binary'
                # æ£€æŸ¥æ˜¯å¦ä¸ºè®¡æ•°æ•°æ®
                elif (self.df[col] >= 0).all() and (self.df[col] % 1 == 0).all():
                    col_type = 'count'
                # æ£€æŸ¥æ˜¯å¦ä¸ºç™¾åˆ†æ¯”
                elif (self.df[col] >= 0).all() and (self.df[col] <= 1).all():
                    col_type = 'percentage'
                else:
                    col_type = 'continuous'

            column_types[col] = col_type

        return column_types

    def suggest_feature_engineering(self) -> List[Dict[str, Any]]:
        """
        æä¾›ç‰¹å¾å·¥ç¨‹å»ºè®®

        Returns:
            ç‰¹å¾å·¥ç¨‹å»ºè®®åˆ—è¡¨
        """
        suggestions = []

        column_types = self.auto_detect_column_types()

        # æ—¥æœŸç‰¹å¾å»ºè®®
        date_cols = [col for col, typ in column_types.items() if typ == 'datetime']
        if date_cols:
            suggestions.append({
                'type': 'datetime_features',
                'columns': date_cols,
                'suggestion': 'ä»æ—¥æœŸåˆ—æå–æ—¶é—´ç‰¹å¾ï¼ˆå¹´ã€æœˆã€æ—¥ã€æ˜ŸæœŸã€å­£åº¦ç­‰ï¼‰',
                'priority': 'high'
            })

        # ç±»åˆ«å˜é‡ç¼–ç å»ºè®®
        low_card_cols = [col for col, typ in column_types.items()
                        if typ == 'categorical_low_cardinality']
        if low_card_cols:
            suggestions.append({
                'type': 'onehot_encoding',
                'columns': low_card_cols,
                'suggestion': 'ä½¿ç”¨One-Hotç¼–ç å¤„ç†ä½åŸºæ•°ç±»åˆ«å˜é‡',
                'priority': 'high'
            })

        high_card_cols = [col for col, typ in column_types.items()
                         if typ == 'categorical_high_cardinality']
        if high_card_cols:
            suggestions.append({
                'type': 'target_encoding',
                'columns': high_card_cols,
                'suggestion': 'ä½¿ç”¨ç›®æ ‡ç¼–ç æˆ–é¢‘ç‡ç¼–ç å¤„ç†é«˜åŸºæ•°ç±»åˆ«å˜é‡',
                'priority': 'medium'
            })

        # æ•°å€¼ç‰¹å¾äº¤äº’å»ºè®®
        if len(self.numeric_cols) >= 2:
            suggestions.append({
                'type': 'interaction_features',
                'columns': self.numeric_cols[:5],  # é™åˆ¶æ•°é‡
                'suggestion': 'åˆ›å»ºæ•°å€¼ç‰¹å¾ä¹‹é—´çš„äº¤äº’é¡¹ï¼ˆä¹˜æ³•ã€é™¤æ³•ç­‰ï¼‰',
                'priority': 'medium'
            })

        # å¤šé¡¹å¼ç‰¹å¾å»ºè®®
        continuous_cols = [col for col, typ in column_types.items()
                          if typ == 'continuous']
        if continuous_cols:
            suggestions.append({
                'type': 'polynomial_features',
                'columns': continuous_cols[:3],
                'suggestion': 'åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾ä»¥æ•æ‰éçº¿æ€§å…³ç³»',
                'priority': 'low'
            })

        # ç‰¹å¾ç¼©æ”¾å»ºè®®
        if self.numeric_cols:
            ranges = {}
            for col in self.numeric_cols:
                ranges[col] = self.df[col].max() - self.df[col].min()

            if max(ranges.values()) / min(ranges.values()) > 100:
                suggestions.append({
                    'type': 'feature_scaling',
                    'columns': self.numeric_cols,
                    'suggestion': 'æ•°å€¼ç‰¹å¾èŒƒå›´å·®å¼‚è¾ƒå¤§ï¼Œå»ºè®®è¿›è¡Œæ ‡å‡†åŒ–æˆ–å½’ä¸€åŒ–',
                    'priority': 'high'
                })

        return suggestions

    def detect_correlations(self, threshold: float = 0.8) -> Dict[str, Any]:
        """
        æ£€æµ‹é«˜ç›¸å…³æ€§ç‰¹å¾

        Args:
            threshold: ç›¸å…³ç³»æ•°é˜ˆå€¼

        Returns:
            ç›¸å…³æ€§ä¿¡æ¯
        """
        if not self.numeric_cols:
            return {'high_correlations': [], 'correlation_matrix': None}

        corr_matrix = self.df[self.numeric_cols].corr()

        # æ‰¾å‡ºé«˜ç›¸å…³æ€§å¯¹
        high_corr_pairs = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i + 1, len(corr_matrix.columns)):
                corr_value = corr_matrix.iloc[i, j]
                if abs(corr_value) > threshold:
                    high_corr_pairs.append({
                        'feature_1': corr_matrix.columns[i],
                        'feature_2': corr_matrix.columns[j],
                        'correlation': float(corr_value),
                        'recommendation': f"è€ƒè™‘åˆ é™¤å…¶ä¸­ä¸€ä¸ªç‰¹å¾ä»¥å‡å°‘å¤šé‡å…±çº¿æ€§"
                    })

        return {
            'high_correlations': high_corr_pairs,
            'correlation_matrix': corr_matrix,
            'recommendation': f"å‘ç° {len(high_corr_pairs)} å¯¹é«˜ç›¸å…³æ€§ç‰¹å¾ï¼ˆ|r| > {threshold}ï¼‰"
        }

    def suggest_models(self, task_type: Optional[str] = None,
                      target_column: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        æ ¹æ®æ•°æ®ç‰¹å¾å»ºè®®åˆé€‚çš„æ¨¡å‹

        Args:
            task_type: ä»»åŠ¡ç±»å‹ ('classification', 'regression', 'clustering')
            target_column: ç›®æ ‡åˆ—å

        Returns:
            æ¨¡å‹å»ºè®®åˆ—è¡¨
        """
        suggestions = []

        # è‡ªåŠ¨æ£€æµ‹ä»»åŠ¡ç±»å‹
        if task_type is None and target_column:
            if target_column in self.df.columns:
                unique_values = self.df[target_column].nunique()
                if unique_values <= 20:
                    task_type = 'classification'
                else:
                    task_type = 'regression'

        # æ•°æ®é›†å¤§å°
        n_samples = len(self.df)
        n_features = len(self.df.columns)

        if task_type == 'classification':
            # åˆ†ç±»ä»»åŠ¡å»ºè®®
            if n_samples < 1000:
                suggestions.append({
                    'model': 'Logistic Regression',
                    'reason': 'æ ·æœ¬é‡è¾ƒå°ï¼Œç®€å•æ¨¡å‹æ›´åˆé€‚',
                    'priority': 'high',
                    'complexity': 'low'
                })

            suggestions.append({
                'model': 'Random Forest',
                'reason': 'å¤„ç†éçº¿æ€§å…³ç³»å’Œç‰¹å¾äº¤äº’æ•ˆæœå¥½ï¼Œä¸æ˜“è¿‡æ‹Ÿåˆ',
                'priority': 'high',
                'complexity': 'medium'
            })

            suggestions.append({
                'model': 'XGBoost/LightGBM',
                'reason': 'é€šå¸¸åœ¨ç»“æ„åŒ–æ•°æ®ä¸Šè¡¨ç°æœ€å¥½',
                'priority': 'high',
                'complexity': 'medium'
            })

            if n_samples > 10000:
                suggestions.append({
                    'model': 'Neural Network',
                    'reason': 'å¤§æ•°æ®é›†å¯ä»¥å……åˆ†åˆ©ç”¨æ·±åº¦å­¦ä¹ çš„ä¼˜åŠ¿',
                    'priority': 'medium',
                    'complexity': 'high'
                })

        elif task_type == 'regression':
            # å›å½’ä»»åŠ¡å»ºè®®
            suggestions.append({
                'model': 'Linear Regression',
                'reason': 'åŸºçº¿æ¨¡å‹ï¼Œå¿«é€ŸéªŒè¯çº¿æ€§å…³ç³»',
                'priority': 'high',
                'complexity': 'low'
            })

            suggestions.append({
                'model': 'Random Forest Regressor',
                'reason': 'æ•æ‰éçº¿æ€§å…³ç³»ï¼Œç‰¹å¾é‡è¦æ€§åˆ†æ',
                'priority': 'high',
                'complexity': 'medium'
            })

            suggestions.append({
                'model': 'XGBoost/LightGBM',
                'reason': 'é€šå¸¸åœ¨ç»“æ„åŒ–æ•°æ®ä¸Šè¡¨ç°æœ€å¥½',
                'priority': 'high',
                'complexity': 'medium'
            })

            if n_features > n_samples / 2:
                suggestions.append({
                    'model': 'Ridge/Lasso Regression',
                    'reason': 'ç‰¹å¾æ•°é‡å¤šï¼Œéœ€è¦æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ',
                    'priority': 'high',
                    'complexity': 'low'
                })

        elif task_type == 'clustering':
            # èšç±»ä»»åŠ¡å»ºè®®
            suggestions.append({
                'model': 'K-Means',
                'reason': 'ç®€å•é«˜æ•ˆï¼Œé€‚åˆçƒå½¢ç°‡',
                'priority': 'high',
                'complexity': 'low'
            })

            suggestions.append({
                'model': 'DBSCAN',
                'reason': 'å¯ä»¥å‘ç°ä»»æ„å½¢çŠ¶çš„ç°‡ï¼Œå¤„ç†å™ªå£°ç‚¹',
                'priority': 'medium',
                'complexity': 'medium'
            })

            if n_samples < 5000:
                suggestions.append({
                    'model': 'Hierarchical Clustering',
                    'reason': 'æ ·æœ¬é‡é€‚ä¸­ï¼Œå¯ä»¥ç”Ÿæˆèšç±»æ ‘çŠ¶å›¾',
                    'priority': 'medium',
                    'complexity': 'medium'
                })

        return suggestions

    def generate_insights(self) -> List[str]:
        """
        ç”Ÿæˆæ•°æ®æ´å¯Ÿ

        Returns:
            æ´å¯Ÿåˆ—è¡¨
        """
        insights = []

        # æ•°æ®è§„æ¨¡æ´å¯Ÿ
        n_rows, n_cols = self.df.shape
        insights.append(f"ğŸ“Š æ•°æ®é›†åŒ…å« {n_rows:,} è¡Œå’Œ {n_cols} åˆ—")

        # ç¼ºå¤±å€¼æ´å¯Ÿ
        missing_total = self.df.isnull().sum().sum()
        if missing_total > 0:
            missing_pct = missing_total / (n_rows * n_cols) * 100
            insights.append(f"âš ï¸ æ€»å…±æœ‰ {missing_total:,} ä¸ªç¼ºå¤±å€¼ ({missing_pct:.2f}%)")

        # æ•°å€¼åˆ—ç»Ÿè®¡æ´å¯Ÿ
        if self.numeric_cols:
            for col in self.numeric_cols[:3]:  # åªæ˜¾ç¤ºå‰3åˆ—
                mean_val = self.df[col].mean()
                std_val = self.df[col].std()
                cv = (std_val / mean_val) * 100 if mean_val != 0 else 0

                if cv > 100:
                    insights.append(f"ğŸ“ˆ åˆ— '{col}' å˜å¼‚ç³»æ•°è¾ƒé«˜ ({cv:.1f}%)ï¼Œæ•°æ®æ³¢åŠ¨å¤§")

                # æ£€æŸ¥åæ€
                skewness = self.df[col].skew()
                if abs(skewness) > 1:
                    direction = "å³å" if skewness > 0 else "å·¦å"
                    insights.append(f"ğŸ“Š åˆ— '{col}' å‘ˆç°{direction}åˆ†å¸ƒ (ååº¦: {skewness:.2f})")

        # ç±»åˆ«åˆ—æ´å¯Ÿ
        if self.categorical_cols:
            for col in self.categorical_cols[:3]:
                n_unique = self.df[col].nunique()
                insights.append(f"ğŸ·ï¸ ç±»åˆ«åˆ— '{col}' æœ‰ {n_unique} ä¸ªå”¯ä¸€å€¼")

                # ä¸å¹³è¡¡æ£€æµ‹
                if n_unique <= 10:
                    value_counts = self.df[col].value_counts()
                    max_pct = value_counts.iloc[0] / len(self.df) * 100
                    if max_pct > 80:
                        insights.append(
                            f"âš ï¸ åˆ— '{col}' ç±»åˆ«ä¸å¹³è¡¡ï¼Œæœ€å¸¸è§å€¼å  {max_pct:.1f}%"
                        )

        # ç›¸å…³æ€§æ´å¯Ÿ
        if len(self.numeric_cols) >= 2:
            corr_info = self.detect_correlations(threshold=0.7)
            if corr_info['high_correlations']:
                insights.append(
                    f"ğŸ”— å‘ç° {len(corr_info['high_correlations'])} å¯¹é«˜ç›¸å…³æ€§ç‰¹å¾"
                )

        return insights

    def auto_analyze(self) -> Dict[str, Any]:
        """
        è‡ªåŠ¨åˆ†æå¹¶ç”Ÿæˆå®Œæ•´æŠ¥å‘Š

        Returns:
            å®Œæ•´çš„åˆ†ææŠ¥å‘Š
        """
        print("=" * 80)
        print("AI è¾…åŠ©æ•°æ®åˆ†ææŠ¥å‘Š")
        print("=" * 80)

        # æ•°æ®è´¨é‡æŠ¥å‘Š
        print("\n1. æ•°æ®è´¨é‡åˆ†æ")
        print("-" * 80)
        quality_report = self.generate_data_quality_report()

        print(f"æ•°æ®ç»´åº¦: {quality_report['overview']['total_rows']} è¡Œ Ã— "
              f"{quality_report['overview']['total_columns']} åˆ—")
        print(f"æ•°å€¼åˆ—: {quality_report['overview']['numeric_columns']}")
        print(f"ç±»åˆ«åˆ—: {quality_report['overview']['categorical_columns']}")
        print(f"å†…å­˜ä½¿ç”¨: {quality_report['overview']['memory_usage_mb']:.2f} MB")

        if quality_report['missing_values']['total_missing'] > 0:
            print(f"\nç¼ºå¤±å€¼: {quality_report['missing_values']['total_missing']} ä¸ª")

        if quality_report['duplicates']['count'] > 0:
            print(f"é‡å¤è¡Œ: {quality_report['duplicates']['count']} è¡Œ "
                  f"({quality_report['duplicates']['percentage']:.1f}%)")

        # å»ºè®®
        print("\nğŸ“‹ æ•°æ®è´¨é‡å»ºè®®:")
        for i, rec in enumerate(quality_report['recommendations'], 1):
            print(f"  {i}. {rec}")

        # åˆ—ç±»å‹æ£€æµ‹
        print("\n2. åˆ—ç±»å‹è‡ªåŠ¨æ£€æµ‹")
        print("-" * 80)
        column_types = self.auto_detect_column_types()
        type_counts = {}
        for col_type in column_types.values():
            type_counts[col_type] = type_counts.get(col_type, 0) + 1

        for col_type, count in sorted(type_counts.items()):
            print(f"  {col_type}: {count} åˆ—")

        # ç‰¹å¾å·¥ç¨‹å»ºè®®
        print("\n3. ç‰¹å¾å·¥ç¨‹å»ºè®®")
        print("-" * 80)
        fe_suggestions = self.suggest_feature_engineering()
        for i, sug in enumerate(fe_suggestions, 1):
            print(f"  {i}. [{sug['priority'].upper()}] {sug['suggestion']}")
            print(f"     åˆ—: {', '.join(sug['columns'][:5])}"
                  f"{'...' if len(sug['columns']) > 5 else ''}")

        # ç›¸å…³æ€§åˆ†æ
        print("\n4. ç‰¹å¾ç›¸å…³æ€§åˆ†æ")
        print("-" * 80)
        corr_info = self.detect_correlations(threshold=0.7)
        if corr_info['high_correlations']:
            print(f"å‘ç° {len(corr_info['high_correlations'])} å¯¹é«˜ç›¸å…³æ€§ç‰¹å¾:")
            for pair in corr_info['high_correlations'][:5]:
                print(f"  â€¢ {pair['feature_1']} â†” {pair['feature_2']}: "
                      f"r = {pair['correlation']:.3f}")
        else:
            print("æœªå‘ç°é«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯¹")

        # æ•°æ®æ´å¯Ÿ
        print("\n5. æ•°æ®æ´å¯Ÿ")
        print("-" * 80)
        insights = self.generate_insights()
        for insight in insights:
            print(f"  {insight}")

        print("\n" + "=" * 80)

        return {
            'quality_report': quality_report,
            'column_types': column_types,
            'feature_engineering_suggestions': fe_suggestions,
            'correlation_info': corr_info,
            'insights': insights
        }


def main():
    """ç¤ºä¾‹ç”¨æ³•"""
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    df = pd.DataFrame({
        'id': range(1000),
        'age': np.random.randint(18, 80, 1000),
        'income': np.random.randint(20000, 150000, 1000),
        'credit_score': np.random.randint(300, 850, 1000),
        'category': np.random.choice(['A', 'B', 'C', 'D'], 1000),
        'region': np.random.choice(['North', 'South', 'East', 'West'], 1000),
        'purchase_amount': np.random.uniform(10, 1000, 1000),
        'date': pd.date_range('2023-01-01', periods=1000, freq='D')
    })

    # æ·»åŠ ä¸€äº›ç¼ºå¤±å€¼
    df.loc[np.random.choice(1000, 50, replace=False), 'income'] = np.nan
    df.loc[np.random.choice(1000, 30, replace=False), 'category'] = np.nan

    # æ·»åŠ ä¸€äº›é‡å¤è¡Œ
    df = pd.concat([df, df.iloc[:20]], ignore_index=True)

    # åˆå§‹åŒ–AIåŠ©æ‰‹
    assistant = AIDataAssistant(df)

    # è‡ªåŠ¨åˆ†æ
    report = assistant.auto_analyze()

    # è·å–æ¨¡å‹å»ºè®®
    print("\n6. æ¨¡å‹é€‰æ‹©å»ºè®®")
    print("-" * 80)
    model_suggestions = assistant.suggest_models(
        task_type='classification',
        target_column='category'
    )

    for i, sug in enumerate(model_suggestions, 1):
        print(f"  {i}. {sug['model']} (å¤æ‚åº¦: {sug['complexity']})")
        print(f"     ç†ç”±: {sug['reason']}")
        print()


if __name__ == '__main__':
    main()
