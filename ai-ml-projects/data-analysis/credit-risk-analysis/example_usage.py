"""
ä¿¡ç”¨é¢¨éšªåˆ†æ - å®Œæ•´ä½¿ç”¨ç¯„ä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ä¿¡ç”¨é¢¨éšªåˆ†æç³»çµ±é€²è¡Œé¢¨éšªè©•ä¼°
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

# è¨­ç½®ä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ============================================================================
# 1. ç”Ÿæˆç¤ºä¾‹ä¿¡ç”¨é¢¨éšªæ•¸æ“š
# ============================================================================
def generate_sample_credit_data(n_samples=1000, random_state=42):
    """
    ç”Ÿæˆç¤ºä¾‹ä¿¡ç”¨é¢¨éšªæ•¸æ“š
    """
    np.random.seed(random_state)

    data = {
        'age': np.random.randint(18, 75, n_samples),
        'annual_income': np.random.randint(20000, 200000, n_samples),
        'employment_length': np.random.randint(0, 50, n_samples),
        'loan_amount': np.random.randint(1000, 50000, n_samples),
        'debt_to_income_ratio': np.random.uniform(0, 1, n_samples),
        'credit_history_length': np.random.randint(0, 50, n_samples),
        'number_of_accounts': np.random.randint(0, 30, n_samples),
        'number_of_delinquencies': np.random.randint(0, 5, n_samples),
        'revolving_balance': np.random.randint(0, 50000, n_samples),
        'total_credit_limit': np.random.randint(5000, 500000, n_samples),
    }

    df = pd.DataFrame(data)

    # æ·»åŠ é¡åˆ¥è®Šæ•¸
    df['home_ownership'] = np.random.choice(['RENT', 'OWN', 'MORTGAGE'], n_samples)
    df['loan_purpose'] = np.random.choice(
        ['debt_consolidation', 'credit_card', 'home_improvement', 'personal', 'auto'],
        n_samples
    )

    # ç”Ÿæˆç›®æ¨™è®Šæ•¸ï¼ˆé•ç´„æ¦‚ç‡ç”±ç‰¹å¾µæ±ºå®šï¼‰
    # é«˜é½¡ã€é«˜æ”¶å…¥ã€ä½å‚µå‹™æ¯”ã€ä½é€¾æœŸæ¬¡æ•¸ -> ä½é•ç´„é¢¨éšª
    default_prob = (
        0.02 * (df['age'] < 30).astype(int) +
        0.03 * (df['annual_income'] < 30000).astype(int) +
        0.05 * (df['debt_to_income_ratio'] > 0.5).astype(int) +
        0.04 * (df['number_of_delinquencies'] > 0).astype(int) +
        0.02 * (df['employment_length'] < 2).astype(int) +
        0.01  # åŸºç¤é¢¨éšª
    )
    default_prob = np.clip(default_prob, 0, 1)
    df['default'] = (np.random.random(n_samples) < default_prob).astype(int)

    return df


# ============================================================================
# 2. æ•¸æ“šåˆ†æå’Œé è™•ç†
# ============================================================================
def analyze_and_preprocess_data(df):
    """
    åˆ†æå’Œé è™•ç†ä¿¡ç”¨é¢¨éšªæ•¸æ“š
    """
    print("=" * 80)
    print("1. æ•¸æ“šåˆ†æå’Œé è™•ç†")
    print("=" * 80)

    # åŸºæœ¬ä¿¡æ¯
    print("\næ•¸æ“šé›†æ¦‚æ³:")
    print(f"  ç¸½è¨˜éŒ„æ•¸: {len(df)}")
    print(f"  ç‰¹å¾µæ•¸: {df.shape[1]}")
    print(f"  æ•¸å€¼ç‰¹å¾µ: {df.select_dtypes(include=[np.number]).shape[1]}")
    print(f"  é¡åˆ¥ç‰¹å¾µ: {df.select_dtypes(include=['object']).shape[1]}")

    # ç¼ºå¤±å€¼æª¢æŸ¥
    missing = df.isnull().sum()
    if missing.sum() > 0:
        print("\nç¼ºå¤±å€¼:")
        print(missing[missing > 0])
    else:
        print("\nâœ… ç„¡ç¼ºå¤±å€¼")

    # ç›®æ¨™è®Šæ•¸åˆ†ä½ˆ
    print("\nç›®æ¨™è®Šæ•¸åˆ†ä½ˆ (default):")
    print(f"  æ²’æœ‰é•ç´„ (0): {(df['default'] == 0).sum()} ({(df['default'] == 0).mean()*100:.1f}%)")
    print(f"  é•ç´„ (1): {(df['default'] == 1).sum()} ({(df['default'] == 1).mean()*100:.1f}%)")

    # æè¿°çµ±è¨ˆ
    print("\næ•¸å€¼ç‰¹å¾µæè¿°çµ±è¨ˆ:")
    print(df.describe().round(2))

    # ç›¸é—œæ€§åˆ†æ
    print("\nèˆ‡é•ç´„æœ€ç›¸é—œçš„ç‰¹å¾µ:")
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_cols.remove('default')
    correlations = df[numeric_cols + ['default']].corr()['default'].drop('default').sort_values(ascending=False)
    print(correlations.head(10).round(3))

    # æ•¸æ“šé è™•ç†
    df_processed = df.copy()

    # ç·¨ç¢¼é¡åˆ¥è®Šæ•¸
    le_home = LabelEncoder()
    le_purpose = LabelEncoder()
    df_processed['home_ownership'] = le_home.fit_transform(df_processed['home_ownership'])
    df_processed['loan_purpose'] = le_purpose.fit_transform(df_processed['loan_purpose'])

    return df_processed


# ============================================================================
# 3. ç‰¹å¾µå·¥ç¨‹
# ============================================================================
def create_features(df):
    """
    å‰µå»ºæ–°çš„ç‰¹å¾µä»¥æ”¹é€²æ¨¡å‹æ€§èƒ½
    """
    print("\nå‰µå»ºæ–°ç‰¹å¾µ:")

    # ç‰¹å¾µå·¥ç¨‹
    df['income_per_account'] = df['annual_income'] / (df['number_of_accounts'] + 1)
    df['credit_utilization'] = df['revolving_balance'] / (df['total_credit_limit'] + 1)
    df['age_groups'] = pd.cut(df['age'], bins=[0, 25, 35, 50, 65, 100], labels=[0, 1, 2, 3, 4]).astype(int)
    df['high_risk_features'] = (df['number_of_delinquencies'] > 0).astype(int)
    df['credit_history_years'] = df['credit_history_length']

    print("  âœ… å‰µå»ºäº†5å€‹æ–°ç‰¹å¾µ:")
    print("    - income_per_account: äººå‡å¹´æ”¶å…¥")
    print("    - credit_utilization: ä¿¡ç”¨ä½¿ç”¨ç‡")
    print("    - age_groups: å¹´é½¡åˆ†çµ„")
    print("    - high_risk_features: é«˜é¢¨éšªç‰¹å¾µæ¨™è¨˜")
    print("    - credit_history_years: ä¿¡ç”¨æ­·å²å¹´æ•¸")

    return df


# ============================================================================
# 4. æ¨¡å‹è¨“ç·´
# ============================================================================
def train_credit_risk_models(X_train, y_train, X_test, y_test):
    """
    è¨“ç·´å¤šå€‹ä¿¡ç”¨é¢¨éšªé æ¸¬æ¨¡å‹
    """
    print("\n" + "=" * 80)
    print("2. æ¨¡å‹è¨“ç·´")
    print("=" * 80)

    # ç‰¹å¾µç¸®æ”¾
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    models = {
        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
    }

    results = {}

    for name, model in models.items():
        print(f"\nè¨“ç·´ {name}...")
        model.fit(X_train_scaled, y_train)

        # é æ¸¬
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

        # è©•ä¼°
        acc = accuracy_score(y_test, y_pred)
        prec = precision_score(y_test, y_pred, zero_division=0)
        rec = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        auc = roc_auc_score(y_test, y_pred_proba)

        results[name] = {
            'model': model,
            'scaler': scaler,
            'accuracy': acc,
            'precision': prec,
            'recall': rec,
            'f1': f1,
            'auc': auc,
            'predictions': y_pred,
            'probabilities': y_pred_proba
        }

        print(f"  Accuracy:  {acc:.4f}")
        print(f"  Precision: {prec:.4f}")
        print(f"  Recall:    {rec:.4f}")
        print(f"  F1-Score:  {f1:.4f}")
        print(f"  AUC-ROC:   {auc:.4f}")

    return results, scaler


# ============================================================================
# 5. æ¨¡å‹è©•ä¼°å’Œå¯è¦–åŒ–
# ============================================================================
def evaluate_models(results):
    """
    è©•ä¼°å’Œæ¯”è¼ƒæ¨¡å‹æ€§èƒ½
    """
    print("\n" + "=" * 80)
    print("3. æ¨¡å‹è©•ä¼°")
    print("=" * 80)

    # æ€§èƒ½æ¯”è¼ƒè¡¨
    print("\næ¨¡å‹æ€§èƒ½æ¯”è¼ƒ:")
    performance_df = pd.DataFrame({
        'Model': list(results.keys()),
        'Accuracy': [results[m]['accuracy'] for m in results.keys()],
        'Precision': [results[m]['precision'] for m in results.keys()],
        'Recall': [results[m]['recall'] for m in results.keys()],
        'F1-Score': [results[m]['f1'] for m in results.keys()],
        'AUC-ROC': [results[m]['auc'] for m in results.keys()]
    })
    print(performance_df.to_string(index=False))

    # æœ€ä½³æ¨¡å‹
    best_model_name = performance_df.loc[performance_df['F1-Score'].idxmax(), 'Model']
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")

    return best_model_name, results[best_model_name]


# ============================================================================
# 6. ç‰¹å¾µé‡è¦æ€§åˆ†æ
# ============================================================================
def analyze_feature_importance(best_result, feature_names):
    """
    åˆ†æç‰¹å¾µé‡è¦æ€§
    """
    print("\n" + "=" * 80)
    print("4. ç‰¹å¾µé‡è¦æ€§åˆ†æ")
    print("=" * 80)

    model = best_result['model']

    # Random Forest å’Œ Gradient Boosting æœ‰ç‰¹å¾µé‡è¦æ€§
    if hasattr(model, 'feature_importances_'):
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)

        print("\nå‰10å€‹æœ€é‡è¦çš„ç‰¹å¾µ:")
        print(importance_df.head(10).to_string(index=False))

        return importance_df
    else:
        print("\næ­¤æ¨¡å‹ä¸æ”¯æ´ç‰¹å¾µé‡è¦æ€§åˆ†æ")
        return None


# ============================================================================
# 7. ä¿¡ç”¨é¢¨éšªè©•ä¼°
# ============================================================================
def predict_credit_risk(model, scaler, applicant_data, feature_names):
    """
    è©•ä¼°å–®å€‹ç”³è«‹äººçš„ä¿¡ç”¨é¢¨éšª
    """
    print("\n" + "=" * 80)
    print("5. å–®äººä¿¡ç”¨é¢¨éšªè©•ä¼°")
    print("=" * 80)

    # æº–å‚™ç‰¹å¾µ
    X = pd.DataFrame([applicant_data], columns=feature_names)
    X_scaled = scaler.transform(X)

    # é æ¸¬
    risk_prob = model.predict_proba(X_scaled)[0, 1]
    risk_class = model.predict(X_scaled)[0]

    # é¢¨éšªç­‰ç´šï¼ˆåŸºæ–¼é•ç´„æ¦‚ç‡ï¼‰
    if risk_prob < 0.05:
        risk_grade = 'A'
        recommendation = 'æ‰¹å‡†ï¼Œå„ªæƒ åˆ©ç‡'
    elif risk_prob < 0.10:
        risk_grade = 'B'
        recommendation = 'æ‰¹å‡†ï¼Œæ¨™æº–åˆ©ç‡'
    elif risk_prob < 0.20:
        risk_grade = 'C'
        recommendation = 'æ‰¹å‡†ï¼Œè¼ƒé«˜åˆ©ç‡'
    elif risk_prob < 0.35:
        risk_grade = 'D'
        recommendation = 'è¬¹æ…è€ƒæ…®ï¼Œé«˜åˆ©ç‡'
    elif risk_prob < 0.50:
        risk_grade = 'E'
        recommendation = 'ä¸å»ºè­°æ‰¹å‡†'
    else:
        risk_grade = 'F'
        recommendation = 'æ‹’çµ•'

    # ä¿¡ç”¨è©•åˆ†ï¼ˆ300-850ï¼‰
    credit_score = int(850 - risk_prob * 550)

    print(f"\nç”³è«‹äººä¿¡æ¯:")
    for key, value in applicant_data.items():
        print(f"  {key}: {value}")

    print(f"\né¢¨éšªè©•ä¼°çµæœ:")
    print(f"  é•ç´„æ¦‚ç‡: {risk_prob:.2%}")
    print(f"  ä¿¡ç”¨è©•åˆ†: {credit_score}")
    print(f"  é¢¨éšªç­‰ç´š: {risk_grade}")
    print(f"  å»ºè­°: {recommendation}")

    return {
        'risk_probability': risk_prob,
        'credit_score': credit_score,
        'risk_grade': risk_grade,
        'recommendation': recommendation
    }


# ============================================================================
# 8. æ‰¹æ¬¡é¢¨éšªè©•ä¼°
# ============================================================================
def batch_credit_assessment(model, scaler, df_batch, feature_names):
    """
    æ‰¹é‡è©•ä¼°å¤šå€‹ç”³è«‹äºº
    """
    print("\n" + "=" * 80)
    print("6. æ‰¹é‡é¢¨éšªè©•ä¼°")
    print("=" * 80)

    X_batch = df_batch[feature_names]
    X_scaled = scaler.transform(X_batch)

    # é æ¸¬
    risk_probs = model.predict_proba(X_scaled)[:, 1]

    # ç¢ºå®šé¢¨éšªç­‰ç´š
    risk_grades = []
    for prob in risk_probs:
        if prob < 0.05:
            risk_grades.append('A')
        elif prob < 0.10:
            risk_grades.append('B')
        elif prob < 0.20:
            risk_grades.append('C')
        elif prob < 0.35:
            risk_grades.append('D')
        elif prob < 0.50:
            risk_grades.append('E')
        else:
            risk_grades.append('F')

    results_df = pd.DataFrame({
        'risk_probability': risk_probs,
        'credit_score': (850 - risk_probs * 550).astype(int),
        'risk_grade': risk_grades
    })

    # çµ±è¨ˆ
    print(f"\næ‰¹é‡è©•ä¼°çµæœçµ±è¨ˆ:")
    print(f"  è©•ä¼°äººæ•¸: {len(results_df)}")
    print(f"\né¢¨éšªç­‰ç´šåˆ†ä½ˆ:")
    grade_dist = results_df['risk_grade'].value_counts().sort_index()
    for grade, count in grade_dist.items():
        print(f"  {grade}: {count} ({count/len(results_df)*100:.1f}%)")

    print(f"\né•ç´„æ¦‚ç‡çµ±è¨ˆ:")
    print(f"  å¹³å‡: {results_df['risk_probability'].mean():.2%}")
    print(f"  ä¸­ä½æ•¸: {results_df['risk_probability'].median():.2%}")
    print(f"  æœ€å°: {results_df['risk_probability'].min():.2%}")
    print(f"  æœ€å¤§: {results_df['risk_probability'].max():.2%}")

    return results_df


# ============================================================================
# 9. å¯è¦–åŒ–
# ============================================================================
def visualize_results(df, results, importance_df=None):
    """
    å¯è¦–åŒ–åˆ†æçµæœ
    """
    print("\n" + "=" * 80)
    print("7. çµæœå¯è¦–åŒ–")
    print("=" * 80)

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # 1. é•ç´„åˆ†ä½ˆ
    default_counts = df['default'].value_counts()
    axes[0, 0].bar(['ç„¡é•ç´„', 'é•ç´„'], default_counts.values)
    axes[0, 0].set_title('ç›®æ¨™è®Šæ•¸åˆ†ä½ˆ')
    axes[0, 0].set_ylabel('è¨ˆæ•¸')

    # 2. å¹´é½¡vsé•ç´„
    age_bins = pd.cut(df['age'], bins=5)
    default_by_age = df.groupby(age_bins)['default'].agg(['sum', 'count'])
    default_by_age['rate'] = default_by_age['sum'] / default_by_age['count']
    axes[0, 1].plot(range(len(default_by_age)), default_by_age['rate'].values, marker='o')
    axes[0, 1].set_title('ä¸åŒå¹´é½¡æ®µçš„é•ç´„ç‡')
    axes[0, 1].set_ylabel('é•ç´„ç‡')
    axes[0, 1].set_xlabel('å¹´é½¡æ®µ')

    # 3. æ”¶å…¥vsé•ç´„
    income_bins = pd.cut(df['annual_income'], bins=5)
    default_by_income = df.groupby(income_bins)['default'].agg(['sum', 'count'])
    default_by_income['rate'] = default_by_income['sum'] / default_by_income['count']
    axes[1, 0].plot(range(len(default_by_income)), default_by_income['rate'].values, marker='o')
    axes[1, 0].set_title('ä¸åŒæ”¶å…¥æ°´æº–çš„é•ç´„ç‡')
    axes[1, 0].set_ylabel('é•ç´„ç‡')
    axes[1, 0].set_xlabel('æ”¶å…¥æ°´æº–')

    # 4. ç‰¹å¾µé‡è¦æ€§
    if importance_df is not None:
        top_features = importance_df.head(10)
        axes[1, 1].barh(top_features['feature'], top_features['importance'])
        axes[1, 1].set_title('å‰10å€‹æœ€é‡è¦ç‰¹å¾µ')
        axes[1, 1].set_xlabel('é‡è¦æ€§')
    else:
        axes[1, 1].text(0.5, 0.5, 'ç‰¹å¾µé‡è¦æ€§åˆ†æä¸å¯ç”¨', ha='center', va='center')

    plt.tight_layout()
    plt.savefig('credit_risk_analysis.png', dpi=300, bbox_inches='tight')
    print("\nâœ… åœ–è¡¨å·²ä¿å­˜ç‚º: credit_risk_analysis.png")
    plt.show()


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================
def main():
    """
    å®Œæ•´çš„ä¿¡ç”¨é¢¨éšªåˆ†æç¤ºä¾‹
    """
    print("\n" + "=" * 80)
    print("ä¿¡ç”¨é¢¨éšªåˆ†æ - å®Œæ•´ä½¿ç”¨ç¯„ä¾‹")
    print("=" * 80)

    # 1. ç”Ÿæˆæ•¸æ“š
    print("\næº–å‚™æ•¸æ“š...")
    df = generate_sample_credit_data(n_samples=1000)
    df = analyze_and_preprocess_data(df)
    df = create_features(df)

    # 2. åˆ†å‰²æ•¸æ“š
    feature_cols = [col for col in df.columns if col != 'default']
    X = df[feature_cols]
    y = df['default']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"\næ•¸æ“šåˆ†å‰²:")
    print(f"  è¨“ç·´é›†: {len(X_train)}")
    print(f"  æ¸¬è©¦é›†: {len(X_test)}")

    # 3. è¨“ç·´æ¨¡å‹
    results, scaler = train_credit_risk_models(X_train, y_train, X_test, y_test)

    # 4. è©•ä¼°æ¨¡å‹
    best_model_name, best_result = evaluate_models(results)

    # 5. ç‰¹å¾µé‡è¦æ€§
    importance_df = analyze_feature_importance(best_result, feature_cols)

    # 6. å–®äººè©•ä¼°
    sample_applicant = {
        'age': 35,
        'annual_income': 75000,
        'employment_length': 5,
        'loan_amount': 15000,
        'debt_to_income_ratio': 0.35,
        'credit_history_length': 10,
        'number_of_accounts': 8,
        'number_of_delinquencies': 0,
        'revolving_balance': 5000,
        'total_credit_limit': 50000,
        'home_ownership': 1,  # MORTGAGE (encoded)
        'loan_purpose': 0,    # debt_consolidation (encoded)
        'income_per_account': 75000 / 8,
        'credit_utilization': 5000 / 50000,
        'age_groups': 2,
        'high_risk_features': 0,
        'credit_history_years': 10
    }
    risk_assessment = predict_credit_risk(
        best_result['model'], scaler, sample_applicant, feature_cols
    )

    # 7. æ‰¹é‡è©•ä¼°
    batch_results = batch_credit_assessment(
        best_result['model'], scaler, X_test.iloc[:50], feature_cols
    )

    # 8. å¯è¦–åŒ–
    visualize_results(df, results, importance_df)

    print("\n" + "=" * 80)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("=" * 80 + "\n")


if __name__ == '__main__':
    main()
