#!/usr/bin/env python3
"""
Data Merger - è³‡æ–™åˆä½µå·¥å…·

åŠŸèƒ½ï¼š
- å¤šæ ¼å¼æ”¯æ´ï¼ˆCSVã€JSONã€Excelï¼‰
- æ™ºèƒ½æ¬„ä½æ˜ å°„
- è³‡æ–™å»é‡
- é—œè¯åˆä½µï¼ˆé¡ä¼¼ SQL JOINï¼‰
- è¡çªè§£æ±ºç­–ç•¥
- åˆä½µå ±å‘Š
"""

import argparse
import sys
import json
from pathlib import Path
from typing import List, Dict, Any, Union
import pandas as pd

def load_data(file_path: str) -> pd.DataFrame:
    """è¼‰å…¥è³‡æ–™æª”æ¡ˆï¼ˆè‡ªå‹•åµæ¸¬æ ¼å¼ï¼‰"""
    try:
        file_ext = Path(file_path).suffix.lower()

        if file_ext == '.csv':
            return pd.read_csv(file_path)
        elif file_ext == '.json':
            return pd.read_json(file_path)
        elif file_ext in ['.xlsx', '.xls']:
            return pd.read_excel(file_path)
        else:
            print(f"âŒ ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼: {file_ext}")
            sys.exit(1)
    except FileNotFoundError:
        print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ è®€å–éŒ¯èª¤: {e}")
        sys.exit(1)

def save_data(df: pd.DataFrame, output_file: str):
    """å„²å­˜è³‡æ–™ï¼ˆæ ¹æ“šå‰¯æª”åè‡ªå‹•é¸æ“‡æ ¼å¼ï¼‰"""
    file_ext = Path(output_file).suffix.lower()

    if file_ext == '.csv':
        df.to_csv(output_file, index=False, encoding='utf-8')
    elif file_ext == '.json':
        df.to_json(output_file, orient='records', force_ascii=False, indent=2)
    elif file_ext in ['.xlsx', '.xls']:
        df.to_excel(output_file, index=False)
    else:
        print(f"âš ï¸  æœªçŸ¥çš„è¼¸å‡ºæ ¼å¼ï¼Œä½¿ç”¨ CSV")
        df.to_csv(output_file, index=False, encoding='utf-8')

    print(f"âœ… å·²å„²å­˜: {output_file}")

def simple_concat(files: List[str]) -> pd.DataFrame:
    """ç°¡å–®å‚ç›´åˆä½µï¼ˆå †ç–Šï¼‰"""
    dfs = []
    for file_path in files:
        df = load_data(file_path)
        df['ä¾†æºæª”æ¡ˆ'] = Path(file_path).name
        dfs.append(df)
        print(f"  ğŸ“„ {Path(file_path).name}: {len(df)} ç­†")

    result = pd.concat(dfs, ignore_index=True)
    print(f"\nâœ… åˆä½µå®Œæˆï¼Œå…± {len(result)} ç­†è³‡æ–™")
    return result

def merge_with_key(files: List[str], key: str, how: str = 'inner') -> pd.DataFrame:
    """ä½¿ç”¨éµå€¼åˆä½µï¼ˆé¡ä¼¼ SQL JOINï¼‰"""
    if len(files) < 2:
        print("âŒ éœ€è¦è‡³å°‘ 2 å€‹æª”æ¡ˆé€²è¡Œéµå€¼åˆä½µ")
        sys.exit(1)

    result = load_data(files[0])
    print(f"  ğŸ“„ åŸºç¤è³‡æ–™: {Path(files[0]).name} ({len(result)} ç­†)")

    for file_path in files[1:]:
        df = load_data(file_path)
        print(f"  ğŸ“„ åˆä½µ: {Path(file_path).name} ({len(df)} ç­†)")

        # æª¢æŸ¥éµæ˜¯å¦å­˜åœ¨
        if key not in result.columns:
            print(f"âŒ éµ '{key}' ä¸å­˜åœ¨æ–¼ {Path(files[0]).name}")
            sys.exit(1)
        if key not in df.columns:
            print(f"âŒ éµ '{key}' ä¸å­˜åœ¨æ–¼ {Path(file_path).name}")
            sys.exit(1)

        # åŸ·è¡Œåˆä½µ
        result = pd.merge(result, df, on=key, how=how, suffixes=('', f'_{Path(file_path).stem}'))
        print(f"     â†’ åˆä½µå¾Œ: {len(result)} ç­†")

    print(f"\nâœ… åˆä½µå®Œæˆï¼Œå…± {len(result)} ç­†è³‡æ–™")
    return result

def merge_with_mapping(files: List[str], mappings: Dict[str, str]) -> pd.DataFrame:
    """ä½¿ç”¨æ¬„ä½æ˜ å°„åˆä½µ"""
    if len(files) < 2:
        print("âŒ éœ€è¦è‡³å°‘ 2 å€‹æª”æ¡ˆé€²è¡Œåˆä½µ")
        sys.exit(1)

    # è¼‰å…¥ç¬¬ä¸€å€‹æª”æ¡ˆ
    result = load_data(files[0])
    print(f"  ğŸ“„ åŸºç¤è³‡æ–™: {Path(files[0]).name} ({len(result)} ç­†)")

    # åˆä½µå…¶ä»–æª”æ¡ˆ
    for file_path in files[1:]:
        df = load_data(file_path)

        # é‡å‘½åæ¬„ä½
        rename_dict = {}
        for old_name, new_name in mappings.items():
            if old_name in df.columns:
                rename_dict[old_name] = new_name

        if rename_dict:
            df = df.rename(columns=rename_dict)
            print(f"  ğŸ“„ {Path(file_path).name}: é‡å‘½å {len(rename_dict)} å€‹æ¬„ä½")

        # å‚ç›´åˆä½µ
        result = pd.concat([result, df], ignore_index=True)

    print(f"\nâœ… åˆä½µå®Œæˆï¼Œå…± {len(result)} ç­†è³‡æ–™")
    return result

def smart_merge(files: List[str], threshold: float = 0.7) -> pd.DataFrame:
    """æ™ºèƒ½åˆä½µï¼ˆè‡ªå‹•åµæ¸¬ç›¸ä¼¼æ¬„ä½ï¼‰"""
    from difflib import SequenceMatcher

    def similarity(a: str, b: str) -> float:
        return SequenceMatcher(None, a.lower(), b.lower()).ratio()

    # è¼‰å…¥æ‰€æœ‰æª”æ¡ˆ
    dfs = [load_data(f) for f in files]

    # æ‰¾å‡ºæ‰€æœ‰æ¬„ä½
    all_columns = set()
    for df in dfs:
        all_columns.update(df.columns)

    print(f"ğŸ” åµæ¸¬åˆ° {len(all_columns)} å€‹ä¸åŒçš„æ¬„ä½åç¨±")

    # å»ºç«‹æ¬„ä½æ˜ å°„
    column_mapping = {}
    for i, df in enumerate(dfs):
        mapping = {}
        for col in df.columns:
            best_match = col
            best_score = 1.0

            # æ‰¾æœ€ç›¸ä¼¼çš„æ¨™æº–æ¬„ä½
            for standard_col in all_columns:
                score = similarity(col, standard_col)
                if score > threshold and score < best_score:
                    best_match = standard_col
                    best_score = score

            mapping[col] = best_match

        if any(k != v for k, v in mapping.items()):
            print(f"  ğŸ“„ {Path(files[i]).name}: æ˜ å°„ {sum(1 for k, v in mapping.items() if k != v)} å€‹æ¬„ä½")

        dfs[i] = df.rename(columns=mapping)

    # åˆä½µæ‰€æœ‰è³‡æ–™
    result = pd.concat(dfs, ignore_index=True)
    print(f"\nâœ… æ™ºèƒ½åˆä½µå®Œæˆï¼Œå…± {len(result)} ç­†è³‡æ–™")
    return result

def deduplicate_data(df: pd.DataFrame, subset: List[str] = None, strategy: str = 'first') -> pd.DataFrame:
    """å»é™¤é‡è¤‡è³‡æ–™"""
    before_count = len(df)

    if strategy == 'first':
        df = df.drop_duplicates(subset=subset, keep='first')
    elif strategy == 'last':
        df = df.drop_duplicates(subset=subset, keep='last')
    elif strategy == 'all':
        df = df.drop_duplicates(subset=subset, keep=False)

    removed = before_count - len(df)
    print(f"ğŸ—‘ï¸  ç§»é™¤ {removed} ç­†é‡è¤‡è³‡æ–™ï¼ˆç­–ç•¥: {strategy}ï¼‰")
    return df

def generate_merge_report(df: pd.DataFrame, files: List[str]):
    """ç”Ÿæˆåˆä½µå ±å‘Š"""
    print("\n" + "=" * 60)
    print("è³‡æ–™åˆä½µå ±å‘Š")
    print("=" * 60)

    print(f"\nğŸ“Š è¼¸å…¥æª”æ¡ˆ:")
    for i, file_path in enumerate(files, 1):
        print(f"  {i}. {Path(file_path).name}")

    print(f"\nğŸ“ˆ åˆä½µçµæœ:")
    print(f"  ç¸½è³‡æ–™ç­†æ•¸: {len(df)}")
    print(f"  ç¸½æ¬„ä½æ•¸: {len(df.columns)}")
    print(f"  è¨˜æ†¶é«”ä½¿ç”¨: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")

    print(f"\nğŸ“‹ æ¬„ä½è³‡è¨Š:")
    for col in df.columns:
        null_count = df[col].isnull().sum()
        null_percent = null_count / len(df) * 100
        dtype = df[col].dtype
        print(f"  {col}:")
        print(f"    é¡å‹: {dtype}, ç¼ºå¤±å€¼: {null_count} ({null_percent:.1f}%)")

    print(f"\nğŸ” è³‡æ–™å“è³ª:")
    print(f"  ç¼ºå¤±å€¼ç¸½æ•¸: {df.isnull().sum().sum()}")
    print(f"  é‡è¤‡è³‡æ–™: {df.duplicated().sum()}")

    # æ•¸å€¼çµ±è¨ˆ
    numeric_cols = df.select_dtypes(include=['number']).columns
    if len(numeric_cols) > 0:
        print(f"\nğŸ“Š æ•¸å€¼æ¬„ä½çµ±è¨ˆ:")
        for col in numeric_cols[:5]:  # åªé¡¯ç¤ºå‰ 5 å€‹
            print(f"  {col}:")
            print(f"    å¹³å‡å€¼: {df[col].mean():.2f}, ä¸­ä½æ•¸: {df[col].median():.2f}")

    print("\n" + "=" * 60)

def main():
    parser = argparse.ArgumentParser(
        description='Data Merger - è³‡æ–™åˆä½µå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument('files', nargs='+', help='è¦åˆä½µçš„æª”æ¡ˆ')
    parser.add_argument('--key', type=str, help='åˆä½µéµå€¼ï¼ˆç”¨æ–¼ JOIN æ“ä½œï¼‰')
    parser.add_argument('--join', choices=['inner', 'outer', 'left', 'right'], default='inner',
                        help='JOIN é¡å‹')
    parser.add_argument('--map', type=str, help='æ¬„ä½æ˜ å°„ï¼ˆæ ¼å¼: old1:new1,old2:new2ï¼‰')
    parser.add_argument('--smart', action='store_true', help='æ™ºèƒ½åˆä½µï¼ˆè‡ªå‹•åµæ¸¬ç›¸ä¼¼æ¬„ä½ï¼‰')
    parser.add_argument('--deduplicate', action='store_true', help='å»é™¤é‡è¤‡è³‡æ–™')
    parser.add_argument('--dedup-strategy', choices=['first', 'last', 'all'], default='first',
                        help='å»é‡ç­–ç•¥')
    parser.add_argument('--dedup-subset', type=str, help='å»é‡åƒè€ƒæ¬„ä½ï¼ˆé€—è™Ÿåˆ†éš”ï¼‰')
    parser.add_argument('--report', action='store_true', help='é¡¯ç¤ºåˆä½µå ±å‘Š')
    parser.add_argument('-o', '--output', type=str, required=True, help='è¼¸å‡ºæª”æ¡ˆè·¯å¾‘')

    args = parser.parse_args()

    if len(args.files) < 2:
        print("âŒ éœ€è¦è‡³å°‘ 2 å€‹æª”æ¡ˆé€²è¡Œåˆä½µ")
        sys.exit(1)

    print(f"ğŸ”„ åˆä½µ {len(args.files)} å€‹æª”æ¡ˆ...\n")

    # åŸ·è¡Œåˆä½µ
    if args.key:
        # éµå€¼åˆä½µ
        print(f"ğŸ”‘ ä½¿ç”¨éµå€¼åˆä½µ: {args.key} ({args.join} join)")
        result = merge_with_key(args.files, args.key, args.join)
    elif args.map:
        # æ¬„ä½æ˜ å°„åˆä½µ
        print("ğŸ“‹ ä½¿ç”¨æ¬„ä½æ˜ å°„åˆä½µ")
        mappings = {}
        for pair in args.map.split(','):
            old, new = pair.split(':')
            mappings[old.strip()] = new.strip()
        result = merge_with_mapping(args.files, mappings)
    elif args.smart:
        # æ™ºèƒ½åˆä½µ
        print("ğŸ§  æ™ºèƒ½åˆä½µæ¨¡å¼")
        result = smart_merge(args.files)
    else:
        # ç°¡å–®åˆä½µ
        print("ğŸ“š ç°¡å–®å †ç–Šåˆä½µ")
        result = simple_concat(args.files)

    # å»é‡
    if args.deduplicate:
        subset = None
        if args.dedup_subset:
            subset = [s.strip() for s in args.dedup_subset.split(',')]
        result = deduplicate_data(result, subset, args.dedup_strategy)

    # é¡¯ç¤ºå ±å‘Š
    if args.report:
        generate_merge_report(result, args.files)

    # å„²å­˜çµæœ
    save_data(result, args.output)
    print(f"\nâœ… åˆä½µå®Œæˆï¼è³‡æ–™ç­†æ•¸: {len(result)}, æ¬„ä½æ•¸: {len(result.columns)}")

if __name__ == '__main__':
    main()
