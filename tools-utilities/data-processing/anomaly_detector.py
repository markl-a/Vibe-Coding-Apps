#!/usr/bin/env python3
"""
Anomaly Detector - è³‡æ–™ç•°å¸¸åµæ¸¬å·¥å…·

åŠŸèƒ½:
- çµ±è¨ˆæ–¹æ³•ç•°å¸¸æª¢æ¸¬
- æ©Ÿå™¨å­¸ç¿’ç•°å¸¸æª¢æ¸¬
- æ™‚é–“åºåˆ—ç•°å¸¸æª¢æ¸¬
- å¤šç¶­åº¦ç•°å¸¸åˆ†æ
- è‡ªå‹•ç•°å¸¸æ¨™è¨˜
- è¦–è¦ºåŒ–ç•°å¸¸åˆ†å¸ƒ
- ç•°å¸¸è§£é‡‹å’Œå»ºè­°
"""

import argparse
import sys
from pathlib import Path
from typing import Dict, Any, List, Tuple
import pandas as pd
import numpy as np
from scipy import stats
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')


class AnomalyDetector:
    """ç•°å¸¸æª¢æ¸¬å™¨"""

    def __init__(self, file_path: str):
        self.file_path = Path(file_path)
        self.df = None
        self.anomalies = {}
        self._load_data()

    def _load_data(self):
        """è¼‰å…¥è³‡æ–™"""
        try:
            file_ext = self.file_path.suffix.lower()

            if file_ext == '.csv':
                self.df = pd.read_csv(self.file_path)
            elif file_ext == '.json':
                self.df = pd.read_json(self.file_path)
            elif file_ext in ['.xlsx', '.xls']:
                self.df = pd.read_excel(self.file_path)
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼: {file_ext}")

            print(f"âœ… æˆåŠŸè¼‰å…¥è³‡æ–™: {len(self.df)} ç­†, {len(self.df.columns)} æ¬„")
        except Exception as e:
            print(f"âŒ è¼‰å…¥è³‡æ–™å¤±æ•—: {e}")
            sys.exit(1)

    def detect_statistical_anomalies(self, method: str = 'iqr', threshold: float = 1.5) -> Dict[str, Any]:
        """ä½¿ç”¨çµ±è¨ˆæ–¹æ³•æª¢æ¸¬ç•°å¸¸"""
        print(f"\nğŸ” ä½¿ç”¨çµ±è¨ˆæ–¹æ³•æª¢æ¸¬ç•°å¸¸ (method={method}, threshold={threshold})...")

        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        results = {}

        for col in numeric_cols:
            data = self.df[col].dropna()

            if len(data) == 0:
                continue

            if method == 'iqr':
                # IQR æ–¹æ³•
                Q1 = data.quantile(0.25)
                Q3 = data.quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - threshold * IQR
                upper_bound = Q3 + threshold * IQR

                anomaly_mask = (self.df[col] < lower_bound) | (self.df[col] > upper_bound)

            elif method == 'zscore':
                # Z-score æ–¹æ³•
                z_scores = np.abs(stats.zscore(data))
                anomaly_mask = pd.Series(False, index=self.df.index)
                anomaly_mask[data.index] = z_scores > threshold

            elif method == 'modified_zscore':
                # Modified Z-score æ–¹æ³• (æ›´ç©©å¥)
                median = data.median()
                mad = np.median(np.abs(data - median))
                modified_z_scores = 0.6745 * (data - median) / mad
                anomaly_mask = pd.Series(False, index=self.df.index)
                anomaly_mask[data.index] = np.abs(modified_z_scores) > threshold

            else:
                print(f"âš ï¸  æœªçŸ¥çš„æ–¹æ³•: {method}")
                continue

            anomaly_indices = self.df.index[anomaly_mask].tolist()
            anomaly_count = len(anomaly_indices)

            results[col] = {
                'method': method,
                'anomaly_count': anomaly_count,
                'anomaly_percentage': (anomaly_count / len(self.df)) * 100,
                'anomaly_indices': anomaly_indices[:10],  # åªä¿ç•™å‰10å€‹
                'anomaly_values': self.df.loc[anomaly_indices[:10], col].tolist() if anomaly_count > 0 else []
            }

            if method == 'iqr':
                results[col]['bounds'] = {
                    'lower': float(lower_bound),
                    'upper': float(upper_bound)
                }

            print(f"  â€¢ {col}: ç™¼ç¾ {anomaly_count} å€‹ç•°å¸¸å€¼ ({results[col]['anomaly_percentage']:.1f}%)")

        self.anomalies['statistical'] = results
        return results

    def detect_ml_anomalies(self, contamination: float = 0.1) -> Dict[str, Any]:
        """ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’æª¢æ¸¬ç•°å¸¸ (Isolation Forest)"""
        print(f"\nğŸ¤– ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’æª¢æ¸¬ç•°å¸¸ (contamination={contamination})...")

        numeric_cols = self.df.select_dtypes(include=[np.number]).columns

        if len(numeric_cols) < 2:
            print("âš ï¸  æ•¸å€¼æ¬„ä½ä¸è¶³,ç„¡æ³•é€²è¡Œæ©Ÿå™¨å­¸ç¿’ç•°å¸¸æª¢æ¸¬")
            return {}

        # æº–å‚™è³‡æ–™
        X = self.df[numeric_cols].copy()

        # è™•ç†ç¼ºå¤±å€¼
        X = X.fillna(X.mean())

        # æ¨™æº–åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # ä½¿ç”¨ Isolation Forest
        iso_forest = IsolationForest(
            contamination=contamination,
            random_state=42,
            n_estimators=100
        )

        # é æ¸¬ç•°å¸¸ (-1 ç‚ºç•°å¸¸, 1 ç‚ºæ­£å¸¸)
        predictions = iso_forest.fit_predict(X_scaled)
        anomaly_scores = iso_forest.score_samples(X_scaled)

        # æ‰¾å‡ºç•°å¸¸é»
        anomaly_mask = predictions == -1
        anomaly_indices = self.df.index[anomaly_mask].tolist()
        anomaly_count = len(anomaly_indices)

        results = {
            'method': 'isolation_forest',
            'anomaly_count': anomaly_count,
            'anomaly_percentage': (anomaly_count / len(self.df)) * 100,
            'anomaly_indices': anomaly_indices[:20],  # å‰20å€‹
            'features_used': list(numeric_cols),
            'contamination': contamination
        }

        # ç‚ºæ¯å€‹ç•°å¸¸é»è¨ˆç®—ç•°å¸¸åˆ†æ•¸
        if anomaly_count > 0:
            anomaly_samples = []
            for idx in anomaly_indices[:10]:
                row_data = self.df.loc[idx, numeric_cols].to_dict()
                anomaly_samples.append({
                    'index': int(idx),
                    'score': float(anomaly_scores[idx]),
                    'values': {k: float(v) for k, v in row_data.items()}
                })

            results['top_anomalies'] = anomaly_samples

        print(f"  â€¢ ç™¼ç¾ {anomaly_count} å€‹å¤šç¶­ç•°å¸¸é» ({results['anomaly_percentage']:.1f}%)")

        self.anomalies['machine_learning'] = results
        return results

    def detect_univariate_patterns(self) -> Dict[str, Any]:
        """æª¢æ¸¬å–®è®Šé‡æ¨¡å¼ç•°å¸¸"""
        print(f"\nğŸ“Š æª¢æ¸¬å–®è®Šé‡æ¨¡å¼ç•°å¸¸...")

        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        results = {}

        for col in numeric_cols:
            data = self.df[col].dropna()

            if len(data) < 10:
                continue

            patterns = {}

            # 1. æª¢æ¸¬å¸¸æ•¸å€¼
            if data.nunique() == 1:
                patterns['constant_value'] = True

            # 2. æª¢æ¸¬éåº¦é‡è¤‡å€¼
            value_counts = data.value_counts()
            if len(value_counts) > 0:
                most_common_pct = value_counts.iloc[0] / len(data) * 100
                if most_common_pct > 80:
                    patterns['excessive_repetition'] = {
                        'value': float(value_counts.index[0]),
                        'percentage': float(most_common_pct)
                    }

            # 3. æª¢æ¸¬çªç„¶è·³è®Š
            if len(data) > 1:
                diff = data.diff().abs()
                if diff.max() > 10 * diff.median():
                    patterns['sudden_jump'] = True

            # 4. æª¢æ¸¬è¶¨å‹¢ç•°å¸¸ (å¦‚æœæ˜¯æ™‚é–“åºåˆ—)
            if len(data) >= 30:
                # ç°¡å–®çš„è¶¨å‹¢æª¢æ¸¬
                x = np.arange(len(data))
                slope, _, r_value, _, _ = stats.linregress(x, data.values)

                if abs(r_value) > 0.8:  # å¼·è¶¨å‹¢
                    patterns['strong_trend'] = {
                        'direction': 'increasing' if slope > 0 else 'decreasing',
                        'r_squared': float(r_value ** 2)
                    }

            if patterns:
                results[col] = patterns
                pattern_names = ', '.join(patterns.keys())
                print(f"  â€¢ {col}: ç™¼ç¾æ¨¡å¼ç•°å¸¸ ({pattern_names})")

        self.anomalies['patterns'] = results
        return results

    def detect_correlation_anomalies(self, threshold: float = 0.7) -> Dict[str, Any]:
        """æª¢æ¸¬ç›¸é—œæ€§ç•°å¸¸"""
        print(f"\nğŸ”— æª¢æ¸¬ç›¸é—œæ€§ç•°å¸¸ (threshold={threshold})...")

        numeric_cols = self.df.select_dtypes(include=[np.number]).columns

        if len(numeric_cols) < 2:
            print("âš ï¸  æ•¸å€¼æ¬„ä½ä¸è¶³")
            return {}

        # è¨ˆç®—ç›¸é—œä¿‚æ•¸
        corr_matrix = self.df[numeric_cols].corr()

        # æ‰¾å‡ºé«˜ç›¸é—œæ€§çš„æ¬„ä½å°
        high_correlations = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_value = corr_matrix.iloc[i, j]
                if abs(corr_value) > threshold:
                    high_correlations.append({
                        'column1': corr_matrix.columns[i],
                        'column2': corr_matrix.columns[j],
                        'correlation': float(corr_value),
                        'type': 'positive' if corr_value > 0 else 'negative'
                    })

        results = {
            'high_correlation_pairs': high_correlations,
            'threshold': threshold
        }

        if high_correlations:
            print(f"  â€¢ ç™¼ç¾ {len(high_correlations)} çµ„é«˜ç›¸é—œæ€§æ¬„ä½å°")
            for pair in high_correlations[:5]:
                print(f"    - {pair['column1']} â†” {pair['column2']}: {pair['correlation']:.3f}")

        self.anomalies['correlations'] = results
        return results

    def comprehensive_detection(self,
                                statistical_method: str = 'iqr',
                                ml_contamination: float = 0.1) -> Dict[str, Any]:
        """åŸ·è¡Œå…¨é¢ç•°å¸¸æª¢æ¸¬"""
        print("ğŸš€ é–‹å§‹å…¨é¢ç•°å¸¸æª¢æ¸¬...\n")

        self.detect_statistical_anomalies(method=statistical_method)
        self.detect_ml_anomalies(contamination=ml_contamination)
        self.detect_univariate_patterns()
        self.detect_correlation_anomalies()

        return self.anomalies

    def print_summary(self):
        """åˆ—å°ç•°å¸¸æª¢æ¸¬æ‘˜è¦"""
        print("\n" + "="*70)
        print("ğŸ“‹ ç•°å¸¸æª¢æ¸¬æ‘˜è¦å ±å‘Š")
        print("="*70)

        # çµ±è¨ˆæ–¹æ³•æª¢æ¸¬çµæœ
        if 'statistical' in self.anomalies:
            print(f"\nğŸ“Š çµ±è¨ˆæ–¹æ³•æª¢æ¸¬:")
            total_anomalies = sum(
                info['anomaly_count']
                for info in self.anomalies['statistical'].values()
            )
            print(f"  â€¢ æª¢æ¸¬åˆ° {total_anomalies} å€‹çµ±è¨ˆç•°å¸¸")

            # åˆ—å‡ºç•°å¸¸æœ€å¤šçš„æ¬„ä½
            sorted_cols = sorted(
                self.anomalies['statistical'].items(),
                key=lambda x: x[1]['anomaly_count'],
                reverse=True
            )
            if sorted_cols:
                print(f"  â€¢ ç•°å¸¸æœ€å¤šçš„æ¬„ä½:")
                for col, info in sorted_cols[:3]:
                    print(f"    - {col}: {info['anomaly_count']} å€‹ ({info['anomaly_percentage']:.1f}%)")

        # æ©Ÿå™¨å­¸ç¿’æª¢æ¸¬çµæœ
        if 'machine_learning' in self.anomalies:
            ml_results = self.anomalies['machine_learning']
            print(f"\nğŸ¤– æ©Ÿå™¨å­¸ç¿’æª¢æ¸¬:")
            print(f"  â€¢ å¤šç¶­ç•°å¸¸é»: {ml_results['anomaly_count']} å€‹ ({ml_results['anomaly_percentage']:.1f}%)")

        # æ¨¡å¼ç•°å¸¸
        if 'patterns' in self.anomalies:
            patterns = self.anomalies['patterns']
            if patterns:
                print(f"\nğŸ“ˆ æ¨¡å¼ç•°å¸¸:")
                print(f"  â€¢ {len(patterns)} å€‹æ¬„ä½ç™¼ç¾æ¨¡å¼ç•°å¸¸")

        # ç›¸é—œæ€§ç•°å¸¸
        if 'correlations' in self.anomalies:
            corr = self.anomalies['correlations']
            if corr['high_correlation_pairs']:
                print(f"\nğŸ”— ç›¸é—œæ€§ç•°å¸¸:")
                print(f"  â€¢ {len(corr['high_correlation_pairs'])} çµ„é«˜ç›¸é—œæ€§æ¬„ä½å°")

        print("\n" + "="*70)

    def save_report(self, output_file: str):
        """å„²å­˜ç•°å¸¸æª¢æ¸¬å ±å‘Š"""
        import json

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.anomalies, f, ensure_ascii=False, indent=2, default=str)

        print(f"\nâœ… å ±å‘Šå·²å„²å­˜: {output_file}")

    def mark_anomalies(self, output_file: str):
        """æ¨™è¨˜ç•°å¸¸ä¸¦å„²å­˜"""
        marked_df = self.df.copy()

        # æ·»åŠ ç•°å¸¸æ¨™è¨˜æ¬„ä½
        marked_df['anomaly_statistical'] = False
        marked_df['anomaly_ml'] = False

        # æ¨™è¨˜çµ±è¨ˆç•°å¸¸
        if 'statistical' in self.anomalies:
            for col, info in self.anomalies['statistical'].items():
                if col in marked_df.columns:
                    marked_df.loc[info['anomaly_indices'], 'anomaly_statistical'] = True

        # æ¨™è¨˜æ©Ÿå™¨å­¸ç¿’ç•°å¸¸
        if 'machine_learning' in self.anomalies:
            ml_indices = self.anomalies['machine_learning']['anomaly_indices']
            marked_df.loc[ml_indices, 'anomaly_ml'] = True

        # å„²å­˜
        file_ext = Path(output_file).suffix.lower()
        if file_ext == '.csv':
            marked_df.to_csv(output_file, index=False, encoding='utf-8')
        elif file_ext == '.json':
            marked_df.to_json(output_file, orient='records', force_ascii=False, indent=2)
        elif file_ext in ['.xlsx', '.xls']:
            marked_df.to_excel(output_file, index=False)

        print(f"âœ… å·²æ¨™è¨˜ç•°å¸¸ä¸¦å„²å­˜: {output_file}")


def main():
    parser = argparse.ArgumentParser(
        description='Anomaly Detector - è³‡æ–™ç•°å¸¸åµæ¸¬å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument('file', help='è¦æª¢æ¸¬çš„è³‡æ–™æª”æ¡ˆ')
    parser.add_argument('--method', choices=['iqr', 'zscore', 'modified_zscore'],
                       default='iqr', help='çµ±è¨ˆæª¢æ¸¬æ–¹æ³•')
    parser.add_argument('--threshold', type=float, default=1.5,
                       help='ç•°å¸¸æª¢æ¸¬é–¾å€¼')
    parser.add_argument('--contamination', type=float, default=0.1,
                       help='æ©Ÿå™¨å­¸ç¿’ç•°å¸¸æ¯”ä¾‹é æœŸ')
    parser.add_argument('--statistical-only', action='store_true',
                       help='åªä½¿ç”¨çµ±è¨ˆæ–¹æ³•')
    parser.add_argument('--ml-only', action='store_true',
                       help='åªä½¿ç”¨æ©Ÿå™¨å­¸ç¿’æ–¹æ³•')
    parser.add_argument('--report', type=str,
                       help='å„²å­˜ç•°å¸¸å ±å‘Š (JSON)')
    parser.add_argument('--mark', type=str,
                       help='æ¨™è¨˜ç•°å¸¸ä¸¦å„²å­˜è³‡æ–™')

    args = parser.parse_args()

    # å‰µå»ºç•°å¸¸æª¢æ¸¬å™¨
    detector = AnomalyDetector(args.file)

    # åŸ·è¡Œæª¢æ¸¬
    if args.statistical_only:
        detector.detect_statistical_anomalies(method=args.method, threshold=args.threshold)
    elif args.ml_only:
        detector.detect_ml_anomalies(contamination=args.contamination)
    else:
        detector.comprehensive_detection(
            statistical_method=args.method,
            ml_contamination=args.contamination
        )

    # åˆ—å°æ‘˜è¦
    detector.print_summary()

    # å„²å­˜å ±å‘Š
    if args.report:
        detector.save_report(args.report)

    # æ¨™è¨˜ç•°å¸¸
    if args.mark:
        detector.mark_anomalies(args.mark)


if __name__ == '__main__':
    main()
