#!/usr/bin/env python3
"""
AI-Powered Data Analyzer - AI é©…å‹•çš„è³‡æ–™åˆ†æå·¥å…·

åŠŸèƒ½:
- è‡ªå‹•åŒ–è³‡æ–™åˆ†æå’Œè¦‹è§£ç”Ÿæˆ
- çµ±è¨ˆåˆ†æå’Œè¶¨å‹¢è­˜åˆ¥
- è³‡æ–™åˆ†å¸ƒè¦–è¦ºåŒ–
- ç›¸é—œæ€§åˆ†æ
- ç•°å¸¸å€¼è­˜åˆ¥
- æ™ºèƒ½å ±å‘Šç”Ÿæˆ
- AI è¼”åŠ©çš„è³‡æ–™è§£é‡‹
"""

import argparse
import sys
import json
from pathlib import Path
from typing import Dict, Any, List, Tuple
from datetime import datetime
import pandas as pd
import numpy as np
from scipy import stats
from collections import Counter
import warnings
warnings.filterwarnings('ignore')


class DataAnalyzer:
    """AI é©…å‹•çš„è³‡æ–™åˆ†æå™¨"""

    def __init__(self, file_path: str):
        self.file_path = Path(file_path)
        self.df = None
        self.analysis_results = {}
        self._load_data()

    def _load_data(self):
        """è¼‰å…¥è³‡æ–™æª”æ¡ˆ"""
        try:
            file_ext = self.file_path.suffix.lower()

            if file_ext == '.csv':
                self.df = pd.read_csv(self.file_path)
            elif file_ext == '.json':
                self.df = pd.read_json(self.file_path)
            elif file_ext in ['.xlsx', '.xls']:
                self.df = pd.read_excel(self.file_path)
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼: {file_ext}")

            print(f"âœ… æˆåŠŸè¼‰å…¥è³‡æ–™: {len(self.df)} ç­†, {len(self.df.columns)} æ¬„")
        except Exception as e:
            print(f"âŒ è¼‰å…¥è³‡æ–™å¤±æ•—: {e}")
            sys.exit(1)

    def basic_statistics(self) -> Dict[str, Any]:
        """åŸºæœ¬çµ±è¨ˆåˆ†æ"""
        print("\nğŸ“Š åŸ·è¡ŒåŸºæœ¬çµ±è¨ˆåˆ†æ...")

        stats_result = {
            'overview': {
                'total_rows': len(self.df),
                'total_columns': len(self.df.columns),
                'memory_usage_mb': self.df.memory_usage(deep=True).sum() / 1024 / 1024,
                'duplicate_rows': int(self.df.duplicated().sum()),
                'missing_cells': int(self.df.isnull().sum().sum()),
            },
            'columns': {}
        }

        # åˆ†ææ¯å€‹æ¬„ä½
        for col in self.df.columns:
            col_stats = {
                'dtype': str(self.df[col].dtype),
                'non_null_count': int(self.df[col].count()),
                'null_count': int(self.df[col].isnull().sum()),
                'null_percentage': float(self.df[col].isnull().sum() / len(self.df) * 100),
                'unique_count': int(self.df[col].nunique()),
            }

            # æ•¸å€¼å‹æ¬„ä½çš„çµ±è¨ˆ
            if pd.api.types.is_numeric_dtype(self.df[col]):
                col_stats.update({
                    'mean': float(self.df[col].mean()) if not self.df[col].isnull().all() else None,
                    'median': float(self.df[col].median()) if not self.df[col].isnull().all() else None,
                    'std': float(self.df[col].std()) if not self.df[col].isnull().all() else None,
                    'min': float(self.df[col].min()) if not self.df[col].isnull().all() else None,
                    'max': float(self.df[col].max()) if not self.df[col].isnull().all() else None,
                    'q25': float(self.df[col].quantile(0.25)) if not self.df[col].isnull().all() else None,
                    'q75': float(self.df[col].quantile(0.75)) if not self.df[col].isnull().all() else None,
                })

            # é¡åˆ¥å‹æ¬„ä½çš„çµ±è¨ˆ
            else:
                top_values = self.df[col].value_counts().head(5)
                col_stats['top_values'] = {
                    str(k): int(v) for k, v in top_values.items()
                }

            stats_result['columns'][col] = col_stats

        self.analysis_results['basic_statistics'] = stats_result
        return stats_result

    def correlation_analysis(self) -> Dict[str, Any]:
        """ç›¸é—œæ€§åˆ†æ"""
        print("\nğŸ”— åŸ·è¡Œç›¸é—œæ€§åˆ†æ...")

        numeric_cols = self.df.select_dtypes(include=[np.number]).columns

        if len(numeric_cols) < 2:
            print("âš ï¸  æ•¸å€¼æ¬„ä½ä¸è¶³,ç„¡æ³•é€²è¡Œç›¸é—œæ€§åˆ†æ")
            return {}

        # è¨ˆç®—ç›¸é—œä¿‚æ•¸çŸ©é™£
        corr_matrix = self.df[numeric_cols].corr()

        # æ‰¾å‡ºé«˜ç›¸é—œæ€§çš„æ¬„ä½å°
        high_correlations = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_value = corr_matrix.iloc[i, j]
                if abs(corr_value) > 0.7:  # é«˜ç›¸é—œæ€§é–¾å€¼
                    high_correlations.append({
                        'column1': corr_matrix.columns[i],
                        'column2': corr_matrix.columns[j],
                        'correlation': float(corr_value),
                        'strength': 'strong positive' if corr_value > 0.7 else 'strong negative'
                    })

        result = {
            'correlation_matrix': corr_matrix.to_dict(),
            'high_correlations': high_correlations,
            'numeric_columns': list(numeric_cols)
        }

        self.analysis_results['correlation'] = result
        return result

    def distribution_analysis(self) -> Dict[str, Any]:
        """è³‡æ–™åˆ†å¸ƒåˆ†æ"""
        print("\nğŸ“ˆ åŸ·è¡Œè³‡æ–™åˆ†å¸ƒåˆ†æ...")

        distribution_result = {}
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns

        for col in numeric_cols:
            data = self.df[col].dropna()

            if len(data) == 0:
                continue

            # è¨ˆç®—ååº¦å’Œå³°åº¦
            skewness = float(stats.skew(data))
            kurtosis = float(stats.kurtosis(data))

            # æ­£æ…‹æ€§æª¢é©— (Shapiro-Wilk test)
            if len(data) > 3 and len(data) < 5000:
                _, p_value = stats.shapiro(data)
                is_normal = p_value > 0.05
            else:
                is_normal = None
                p_value = None

            distribution_result[col] = {
                'skewness': skewness,
                'skewness_interpretation': self._interpret_skewness(skewness),
                'kurtosis': kurtosis,
                'kurtosis_interpretation': self._interpret_kurtosis(kurtosis),
                'is_normal_distribution': is_normal,
                'normality_p_value': float(p_value) if p_value else None,
            }

        self.analysis_results['distribution'] = distribution_result
        return distribution_result

    def _interpret_skewness(self, skewness: float) -> str:
        """è§£é‡‹ååº¦"""
        if abs(skewness) < 0.5:
            return "è¿‘ä¼¼å°ç¨±"
        elif skewness > 0:
            return "å³å(æ­£å)"
        else:
            return "å·¦å(è² å)"

    def _interpret_kurtosis(self, kurtosis: float) -> str:
        """è§£é‡‹å³°åº¦"""
        if abs(kurtosis) < 0.5:
            return "æ­£å¸¸å³°åº¦"
        elif kurtosis > 0:
            return "é«˜å³°æ…‹(å°–å³°)"
        else:
            return "ä½å³°æ…‹(å¹³å³°)"

    def outlier_detection(self) -> Dict[str, Any]:
        """ç•°å¸¸å€¼æª¢æ¸¬"""
        print("\nğŸ” åŸ·è¡Œç•°å¸¸å€¼æª¢æ¸¬...")

        outliers_result = {}
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns

        for col in numeric_cols:
            data = self.df[col].dropna()

            if len(data) == 0:
                continue

            # IQR æ–¹æ³•
            Q1 = data.quantile(0.25)
            Q3 = data.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            outliers_mask = (self.df[col] < lower_bound) | (self.df[col] > upper_bound)
            outliers_count = int(outliers_mask.sum())
            outliers_percentage = float(outliers_count / len(self.df) * 100)

            # Z-score æ–¹æ³•
            z_scores = np.abs(stats.zscore(data))
            z_outliers_count = int((z_scores > 3).sum())

            outliers_result[col] = {
                'iqr_method': {
                    'lower_bound': float(lower_bound),
                    'upper_bound': float(upper_bound),
                    'outliers_count': outliers_count,
                    'outliers_percentage': outliers_percentage,
                },
                'zscore_method': {
                    'outliers_count': z_outliers_count,
                    'outliers_percentage': float(z_outliers_count / len(data) * 100),
                }
            }

        self.analysis_results['outliers'] = outliers_result
        return outliers_result

    def trend_analysis(self) -> Dict[str, Any]:
        """è¶¨å‹¢åˆ†æ(å¦‚æœæœ‰æ™‚é–“åºåˆ—)"""
        print("\nğŸ“‰ æª¢æŸ¥è¶¨å‹¢åˆ†æå¯èƒ½æ€§...")

        # å°‹æ‰¾å¯èƒ½çš„æ™‚é–“æ¬„ä½
        date_columns = []
        for col in self.df.columns:
            if pd.api.types.is_datetime64_any_dtype(self.df[col]):
                date_columns.append(col)
            elif self.df[col].dtype == 'object':
                try:
                    pd.to_datetime(self.df[col].head(10))
                    date_columns.append(col)
                except:
                    pass

        if not date_columns:
            print("âš ï¸  æœªç™¼ç¾æ™‚é–“åºåˆ—æ¬„ä½")
            return {}

        trend_result = {
            'detected_date_columns': date_columns,
            'message': 'ç™¼ç¾å¯èƒ½çš„æ™‚é–“åºåˆ—æ¬„ä½,å¯é€²è¡Œè¶¨å‹¢åˆ†æ'
        }

        self.analysis_results['trend'] = trend_result
        return trend_result

    def generate_insights(self) -> List[str]:
        """ç”Ÿæˆ AI é©…å‹•çš„æ•¸æ“šè¦‹è§£"""
        print("\nğŸ¤– ç”Ÿæˆæ™ºèƒ½è¦‹è§£...")

        insights = []

        # åŸºæ–¼çµ±è¨ˆåˆ†æç”Ÿæˆè¦‹è§£
        if 'basic_statistics' in self.analysis_results:
            stats = self.analysis_results['basic_statistics']
            overview = stats['overview']

            # è³‡æ–™å“è³ªè¦‹è§£
            missing_pct = (overview['missing_cells'] /
                          (overview['total_rows'] * overview['total_columns']) * 100)
            if missing_pct > 10:
                insights.append(
                    f"âš ï¸  è³‡æ–™å“è³ªè­¦å‘Š: æœ‰ {missing_pct:.1f}% çš„è³‡æ–™éºå¤±,å»ºè­°é€²è¡Œæ¸…ç†"
                )
            elif missing_pct > 0:
                insights.append(
                    f"â„¹ï¸  è³‡æ–™å“è³ª: æœ‰ {missing_pct:.1f}% çš„è³‡æ–™éºå¤±,å±¬æ–¼å¯æ¥å—ç¯„åœ"
                )
            else:
                insights.append("âœ… è³‡æ–™å“è³ªå„ªç§€: ç„¡éºå¤±å€¼")

            # é‡è¤‡è³‡æ–™è¦‹è§£
            dup_pct = overview['duplicate_rows'] / overview['total_rows'] * 100
            if dup_pct > 5:
                insights.append(
                    f"âš ï¸  ç™¼ç¾ {overview['duplicate_rows']} ç­†é‡è¤‡è³‡æ–™ ({dup_pct:.1f}%),å»ºè­°å»é‡"
                )

        # åŸºæ–¼ç›¸é—œæ€§åˆ†æç”Ÿæˆè¦‹è§£
        if 'correlation' in self.analysis_results:
            corr = self.analysis_results['correlation']
            if corr.get('high_correlations'):
                insights.append(
                    f"ğŸ”— ç™¼ç¾ {len(corr['high_correlations'])} çµ„é«˜ç›¸é—œæ€§æ¬„ä½,å¯èƒ½å­˜åœ¨å†—é¤˜æˆ–å› æœé—œä¿‚"
                )
                for hc in corr['high_correlations'][:3]:  # åªé¡¯ç¤ºå‰3çµ„
                    insights.append(
                        f"   â€¢ {hc['column1']} èˆ‡ {hc['column2']} çš„ç›¸é—œæ€§: {hc['correlation']:.3f}"
                    )

        # åŸºæ–¼ç•°å¸¸å€¼æª¢æ¸¬ç”Ÿæˆè¦‹è§£
        if 'outliers' in self.analysis_results:
            outliers = self.analysis_results['outliers']
            high_outlier_cols = [
                col for col, data in outliers.items()
                if data['iqr_method']['outliers_percentage'] > 5
            ]
            if high_outlier_cols:
                insights.append(
                    f"ğŸ” ä»¥ä¸‹æ¬„ä½æœ‰è¼ƒå¤šç•°å¸¸å€¼ (>5%): {', '.join(high_outlier_cols)}"
                )

        # åŸºæ–¼åˆ†å¸ƒåˆ†æç”Ÿæˆè¦‹è§£
        if 'distribution' in self.analysis_results:
            dist = self.analysis_results['distribution']
            skewed_cols = [
                col for col, data in dist.items()
                if abs(data['skewness']) > 1
            ]
            if skewed_cols:
                insights.append(
                    f"ğŸ“Š ä»¥ä¸‹æ¬„ä½å‘ˆç¾æ˜é¡¯åæ…‹åˆ†å¸ƒ: {', '.join(skewed_cols)}"
                )

        if not insights:
            insights.append("âœ… è³‡æ–™æ•´é«”å“è³ªè‰¯å¥½,æœªç™¼ç¾æ˜é¡¯å•é¡Œ")

        self.analysis_results['insights'] = insights
        return insights

    def comprehensive_analysis(self) -> Dict[str, Any]:
        """åŸ·è¡Œå…¨é¢åˆ†æ"""
        print("ğŸš€ é–‹å§‹å…¨é¢è³‡æ–™åˆ†æ...\n")

        # åŸ·è¡Œæ‰€æœ‰åˆ†æ
        self.basic_statistics()
        self.correlation_analysis()
        self.distribution_analysis()
        self.outlier_detection()
        self.trend_analysis()
        self.generate_insights()

        return self.analysis_results

    def print_summary(self):
        """åˆ—å°åˆ†ææ‘˜è¦"""
        print("\n" + "="*70)
        print("ğŸ“‹ è³‡æ–™åˆ†ææ‘˜è¦å ±å‘Š")
        print("="*70)

        if 'basic_statistics' in self.analysis_results:
            stats = self.analysis_results['basic_statistics']['overview']
            print(f"\nğŸ“Š è³‡æ–™æ¦‚è¦½:")
            print(f"  â€¢ ç¸½ç­†æ•¸: {stats['total_rows']:,}")
            print(f"  â€¢ æ¬„ä½æ•¸: {stats['total_columns']}")
            print(f"  â€¢ è¨˜æ†¶é«”ä½¿ç”¨: {stats['memory_usage_mb']:.2f} MB")
            print(f"  â€¢ é‡è¤‡ç­†æ•¸: {stats['duplicate_rows']:,}")
            print(f"  â€¢ éºå¤±å€¼: {stats['missing_cells']:,}")

        if 'insights' in self.analysis_results:
            print(f"\nğŸ’¡ æ™ºèƒ½è¦‹è§£:")
            for insight in self.analysis_results['insights']:
                print(f"  {insight}")

        print("\n" + "="*70)

    def save_report(self, output_file: str, format: str = 'json'):
        """å„²å­˜åˆ†æå ±å‘Š"""
        output_path = Path(output_file)

        if format == 'json':
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(self.analysis_results, f, ensure_ascii=False, indent=2, default=str)
        elif format == 'html':
            self._generate_html_report(output_path)

        print(f"\nâœ… å ±å‘Šå·²å„²å­˜: {output_path}")

    def _generate_html_report(self, output_path: Path):
        """ç”Ÿæˆ HTML æ ¼å¼å ±å‘Š"""
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>è³‡æ–™åˆ†æå ±å‘Š - {self.file_path.name}</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f5f5;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 10px;
            margin-bottom: 20px;
        }}
        .section {{
            background: white;
            padding: 20px;
            margin-bottom: 20px;
            border-radius: 10px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        .metric {{
            display: inline-block;
            margin: 10px 20px 10px 0;
        }}
        .metric-label {{
            color: #666;
            font-size: 14px;
        }}
        .metric-value {{
            font-size: 24px;
            font-weight: bold;
            color: #333;
        }}
        .insight {{
            padding: 10px;
            margin: 5px 0;
            background: #f8f9fa;
            border-left: 4px solid #667eea;
        }}
        table {{
            width: 100%;
            border-collapse: collapse;
        }}
        th, td {{
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }}
        th {{
            background-color: #667eea;
            color: white;
        }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ“Š è³‡æ–™åˆ†æå ±å‘Š</h1>
        <p>æª”æ¡ˆ: {self.file_path.name}</p>
        <p>åˆ†ææ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>
"""

        # åŸºæœ¬çµ±è¨ˆ
        if 'basic_statistics' in self.analysis_results:
            stats = self.analysis_results['basic_statistics']['overview']
            html_content += f"""
    <div class="section">
        <h2>ğŸ“ˆ è³‡æ–™æ¦‚è¦½</h2>
        <div class="metric">
            <div class="metric-label">ç¸½ç­†æ•¸</div>
            <div class="metric-value">{stats['total_rows']:,}</div>
        </div>
        <div class="metric">
            <div class="metric-label">æ¬„ä½æ•¸</div>
            <div class="metric-value">{stats['total_columns']}</div>
        </div>
        <div class="metric">
            <div class="metric-label">è¨˜æ†¶é«”ä½¿ç”¨</div>
            <div class="metric-value">{stats['memory_usage_mb']:.2f} MB</div>
        </div>
        <div class="metric">
            <div class="metric-label">é‡è¤‡ç­†æ•¸</div>
            <div class="metric-value">{stats['duplicate_rows']:,}</div>
        </div>
    </div>
"""

        # æ™ºèƒ½è¦‹è§£
        if 'insights' in self.analysis_results:
            html_content += """
    <div class="section">
        <h2>ğŸ’¡ æ™ºèƒ½è¦‹è§£</h2>
"""
            for insight in self.analysis_results['insights']:
                html_content += f'        <div class="insight">{insight}</div>\n'
            html_content += "    </div>\n"

        html_content += """
</body>
</html>
"""

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)


def main():
    parser = argparse.ArgumentParser(
        description='AI-Powered Data Analyzer - AI é©…å‹•çš„è³‡æ–™åˆ†æå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument('file', help='è¦åˆ†æçš„è³‡æ–™æª”æ¡ˆ')
    parser.add_argument('--basic', action='store_true', help='åªåŸ·è¡ŒåŸºæœ¬çµ±è¨ˆåˆ†æ')
    parser.add_argument('--correlation', action='store_true', help='åªåŸ·è¡Œç›¸é—œæ€§åˆ†æ')
    parser.add_argument('--distribution', action='store_true', help='åªåŸ·è¡Œåˆ†å¸ƒåˆ†æ')
    parser.add_argument('--outliers', action='store_true', help='åªåŸ·è¡Œç•°å¸¸å€¼æª¢æ¸¬')
    parser.add_argument('--full', action='store_true', help='åŸ·è¡Œå®Œæ•´åˆ†æ(é è¨­)')
    parser.add_argument('--report', type=str, help='å„²å­˜åˆ†æå ±å‘Š')
    parser.add_argument('--format', choices=['json', 'html'], default='json',
                       help='å ±å‘Šæ ¼å¼')

    args = parser.parse_args()

    # å‰µå»ºåˆ†æå™¨
    analyzer = DataAnalyzer(args.file)

    # åŸ·è¡ŒæŒ‡å®šçš„åˆ†æ
    if args.basic:
        analyzer.basic_statistics()
    elif args.correlation:
        analyzer.correlation_analysis()
    elif args.distribution:
        analyzer.distribution_analysis()
    elif args.outliers:
        analyzer.outlier_detection()
    else:
        # é è¨­åŸ·è¡Œå®Œæ•´åˆ†æ
        analyzer.comprehensive_analysis()

    # åˆ—å°æ‘˜è¦
    analyzer.print_summary()

    # å„²å­˜å ±å‘Š
    if args.report:
        analyzer.save_report(args.report, args.format)


if __name__ == '__main__':
    main()
