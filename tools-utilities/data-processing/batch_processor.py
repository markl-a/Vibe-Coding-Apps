#!/usr/bin/env python3
"""
Batch Processor - æ‰¹æ¬¡è™•ç†å·¥å…·

åŠŸèƒ½ï¼š
- å¹³è¡Œè™•ç†
- é€²åº¦é¡¯ç¤º
- éŒ¯èª¤è™•ç†èˆ‡é‡è©¦
- è™•ç†è¨˜éŒ„
- è‡ªè¨‚è™•ç†å‡½æ•¸
- çµæœå½™ç¸½
"""

import argparse
import sys
import json
from pathlib import Path
from typing import List, Dict, Any, Callable
from glob import glob
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
import pandas as pd
from tqdm import tqdm

class BatchProcessor:
    """æ‰¹æ¬¡è™•ç†å™¨"""

    def __init__(self, workers: int = 4, verbose: bool = True):
        self.workers = workers
        self.verbose = verbose
        self.results = []
        self.errors = []
        self.start_time = None
        self.end_time = None

    def log(self, message: str):
        """è¨˜éŒ„è¨Šæ¯"""
        if self.verbose:
            timestamp = datetime.now().strftime('%H:%M:%S')
            print(f"[{timestamp}] {message}")

    def process_files(self, files: List[str], processor: Callable, **kwargs) -> Dict[str, Any]:
        """æ‰¹æ¬¡è™•ç†æª”æ¡ˆ"""
        self.start_time = datetime.now()
        self.log(f"ğŸš€ é–‹å§‹æ‰¹æ¬¡è™•ç† {len(files)} å€‹æª”æ¡ˆ (ä½¿ç”¨ {self.workers} å€‹å·¥ä½œè€…)")

        # ä½¿ç”¨é€²åº¦æ¢
        with tqdm(total=len(files), desc="è™•ç†é€²åº¦") as pbar:
            with ThreadPoolExecutor(max_workers=self.workers) as executor:
                # æäº¤æ‰€æœ‰ä»»å‹™
                future_to_file = {
                    executor.submit(processor, file_path, **kwargs): file_path
                    for file_path in files
                }

                # æ”¶é›†çµæœ
                for future in as_completed(future_to_file):
                    file_path = future_to_file[future]
                    try:
                        result = future.result()
                        self.results.append({
                            'file': file_path,
                            'status': 'success',
                            'result': result
                        })
                    except Exception as e:
                        self.errors.append({
                            'file': file_path,
                            'error': str(e)
                        })
                        self.log(f"âŒ è™•ç†å¤±æ•—: {Path(file_path).name} - {e}")

                    pbar.update(1)

        self.end_time = datetime.now()
        return self.generate_summary()

    def generate_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆè™•ç†æ‘˜è¦"""
        total = len(self.results) + len(self.errors)
        success = len(self.results)
        failed = len(self.errors)
        duration = (self.end_time - self.start_time).total_seconds()

        summary = {
            'total': total,
            'success': success,
            'failed': failed,
            'success_rate': success / total * 100 if total > 0 else 0,
            'duration': duration,
            'results': self.results,
            'errors': self.errors
        }

        return summary

    def print_summary(self):
        """é¡¯ç¤ºè™•ç†æ‘˜è¦"""
        summary = self.generate_summary()

        print("\n" + "=" * 60)
        print("æ‰¹æ¬¡è™•ç†æ‘˜è¦")
        print("=" * 60)
        print(f"ç¸½æª”æ¡ˆæ•¸: {summary['total']}")
        print(f"æˆåŠŸ: {summary['success']} âœ…")
        print(f"å¤±æ•—: {summary['failed']} âŒ")
        print(f"æˆåŠŸç‡: {summary['success_rate']:.2f}%")
        print(f"è™•ç†æ™‚é–“: {summary['duration']:.2f} ç§’")

        if summary['failed'] > 0:
            print(f"\nå¤±æ•—çš„æª”æ¡ˆ:")
            for error in self.errors:
                print(f"  âŒ {Path(error['file']).name}: {error['error']}")

        print("=" * 60)


def convert_file(file_path: str, output_dir: str, target_format: str) -> str:
    """è½‰æ›æª”æ¡ˆæ ¼å¼"""
    file_ext = Path(file_path).suffix.lower()
    file_name = Path(file_path).stem

    # è¼‰å…¥è³‡æ–™
    if file_ext == '.csv':
        df = pd.read_csv(file_path)
    elif file_ext == '.json':
        df = pd.read_json(file_path)
    elif file_ext in ['.xlsx', '.xls']:
        df = pd.read_excel(file_path)
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„è¼¸å…¥æ ¼å¼: {file_ext}")

    # è¼¸å‡ºè·¯å¾‘
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # å„²å­˜è³‡æ–™
    if target_format == 'csv':
        output_file = output_path / f"{file_name}.csv"
        df.to_csv(output_file, index=False, encoding='utf-8')
    elif target_format == 'json':
        output_file = output_path / f"{file_name}.json"
        df.to_json(output_file, orient='records', force_ascii=False, indent=2)
    elif target_format == 'excel':
        output_file = output_path / f"{file_name}.xlsx"
        df.to_excel(output_file, index=False)
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„è¼¸å‡ºæ ¼å¼: {target_format}")

    return str(output_file)


def clean_file(file_path: str, output_dir: str) -> str:
    """æ¸…ç†è³‡æ–™æª”æ¡ˆ"""
    df = pd.read_csv(file_path)

    # ç§»é™¤ç©ºç™½
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = df[col].str.strip()

    # ç§»é™¤é‡è¤‡
    df = df.drop_duplicates()

    # è™•ç†ç¼ºå¤±å€¼
    df = df.dropna()

    # å„²å­˜
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    output_file = output_path / f"cleaned_{Path(file_path).name}"
    df.to_csv(output_file, index=False, encoding='utf-8')

    return str(output_file)


def validate_file(file_path: str, schema: Dict[str, Any] = None) -> Dict[str, Any]:
    """é©—è­‰æª”æ¡ˆ"""
    df = pd.read_csv(file_path)

    validation_result = {
        'file': file_path,
        'rows': len(df),
        'columns': len(df.columns),
        'missing_values': df.isnull().sum().sum(),
        'duplicates': df.duplicated().sum(),
        'valid': True,
        'errors': []
    }

    # åŸºæœ¬é©—è­‰
    if len(df) == 0:
        validation_result['valid'] = False
        validation_result['errors'].append('æª”æ¡ˆç‚ºç©º')

    if df.isnull().sum().sum() > len(df) * 0.5:
        validation_result['valid'] = False
        validation_result['errors'].append('ç¼ºå¤±å€¼éå¤šï¼ˆ>50%ï¼‰')

    return validation_result


def merge_files(file_pattern: str, output_file: str) -> str:
    """åˆä½µå¤šå€‹æª”æ¡ˆ"""
    files = glob(file_pattern)

    if not files:
        raise ValueError(f"æ‰¾ä¸åˆ°ç¬¦åˆçš„æª”æ¡ˆ: {file_pattern}")

    dfs = []
    for file_path in files:
        df = pd.read_csv(file_path)
        df['ä¾†æºæª”æ¡ˆ'] = Path(file_path).name
        dfs.append(df)

    result = pd.concat(dfs, ignore_index=True)
    result.to_csv(output_file, index=False, encoding='utf-8')

    return output_file


def analyze_file(file_path: str) -> Dict[str, Any]:
    """åˆ†ææª”æ¡ˆ"""
    df = pd.read_csv(file_path)

    analysis = {
        'file': file_path,
        'rows': len(df),
        'columns': len(df.columns),
        'memory_mb': df.memory_usage(deep=True).sum() / 1024 / 1024,
        'dtypes': df.dtypes.value_counts().to_dict(),
        'missing_values': df.isnull().sum().to_dict(),
        'numeric_stats': {}
    }

    # æ•¸å€¼çµ±è¨ˆ
    numeric_cols = df.select_dtypes(include=['number']).columns
    for col in numeric_cols:
        analysis['numeric_stats'][col] = {
            'mean': float(df[col].mean()),
            'median': float(df[col].median()),
            'std': float(df[col].std()),
            'min': float(df[col].min()),
            'max': float(df[col].max())
        }

    return analysis


def main():
    parser = argparse.ArgumentParser(
        description='Batch Processor - æ‰¹æ¬¡è™•ç†å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument('--input', type=str, required=True, help='è¼¸å…¥æª”æ¡ˆæ¨¡å¼ï¼ˆä¾‹å¦‚: *.csvï¼‰')
    parser.add_argument('--convert', choices=['csv', 'json', 'excel'], help='è½‰æ›æ ¼å¼')
    parser.add_argument('--clean', action='store_true', help='æ¸…ç†è³‡æ–™')
    parser.add_argument('--validate', action='store_true', help='é©—è­‰æª”æ¡ˆ')
    parser.add_argument('--analyze', action='store_true', help='åˆ†ææª”æ¡ˆ')
    parser.add_argument('--merge', action='store_true', help='åˆä½µæª”æ¡ˆ')
    parser.add_argument('--workers', type=int, default=4, help='å¹³è¡Œå·¥ä½œè€…æ•¸é‡')
    parser.add_argument('--output', type=str, default='output', help='è¼¸å‡ºç›®éŒ„æˆ–æª”æ¡ˆ')
    parser.add_argument('--report', type=str, help='å„²å­˜è™•ç†å ±å‘Šï¼ˆJSONï¼‰')
    parser.add_argument('--quiet', action='store_true', help='éœéŸ³æ¨¡å¼')

    args = parser.parse_args()

    # æ‰¾å‡ºç¬¦åˆçš„æª”æ¡ˆ
    files = glob(args.input)

    if not files:
        print(f"âŒ æ‰¾ä¸åˆ°ç¬¦åˆçš„æª”æ¡ˆ: {args.input}")
        sys.exit(1)

    print(f"ğŸ“ æ‰¾åˆ° {len(files)} å€‹æª”æ¡ˆ\n")

    # å‰µå»ºæ‰¹æ¬¡è™•ç†å™¨
    processor = BatchProcessor(workers=args.workers, verbose=not args.quiet)

    # åŸ·è¡Œæ“ä½œ
    if args.merge:
        # åˆä½µæ¨¡å¼ï¼ˆä¸ä½¿ç”¨å¹³è¡Œè™•ç†ï¼‰
        print(f"ğŸ”„ åˆä½µæª”æ¡ˆ...")
        try:
            result = merge_files(args.input, args.output)
            print(f"âœ… å·²åˆä½µè‡³: {result}")
        except Exception as e:
            print(f"âŒ åˆä½µå¤±æ•—: {e}")
            sys.exit(1)

    elif args.convert:
        # è½‰æ›æ¨¡å¼
        summary = processor.process_files(
            files,
            convert_file,
            output_dir=args.output,
            target_format=args.convert
        )

    elif args.clean:
        # æ¸…ç†æ¨¡å¼
        summary = processor.process_files(
            files,
            clean_file,
            output_dir=args.output
        )

    elif args.validate:
        # é©—è­‰æ¨¡å¼
        summary = processor.process_files(
            files,
            validate_file
        )

        # é¡¯ç¤ºé©—è­‰çµæœ
        print("\né©—è­‰çµæœ:")
        for result in processor.results:
            validation = result['result']
            status = "âœ…" if validation['valid'] else "âŒ"
            print(f"{status} {Path(validation['file']).name}")
            if not validation['valid']:
                for error in validation['errors']:
                    print(f"    - {error}")

    elif args.analyze:
        # åˆ†ææ¨¡å¼
        summary = processor.process_files(
            files,
            analyze_file
        )

        # é¡¯ç¤ºåˆ†æçµæœ
        print("\nåˆ†æçµæœ:")
        for result in processor.results:
            analysis = result['result']
            print(f"\nğŸ“„ {Path(analysis['file']).name}:")
            print(f"  è³‡æ–™ç­†æ•¸: {analysis['rows']}")
            print(f"  æ¬„ä½æ•¸: {analysis['columns']}")
            print(f"  è¨˜æ†¶é«”: {analysis['memory_mb']:.2f} MB")
            print(f"  ç¼ºå¤±å€¼: {sum(analysis['missing_values'].values())}")

    else:
        print("âŒ è«‹æŒ‡å®šæ“ä½œ: --convert, --clean, --validate, --analyze, --merge")
        sys.exit(1)

    # é¡¯ç¤ºæ‘˜è¦
    if not args.merge:
        processor.print_summary()

        # å„²å­˜å ±å‘Š
        if args.report:
            summary = processor.generate_summary()
            with open(args.report, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2, default=str)
            print(f"\nâœ… å ±å‘Šå·²å„²å­˜: {args.report}")


if __name__ == '__main__':
    main()
