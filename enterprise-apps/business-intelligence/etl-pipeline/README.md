# ETL æ•¸æ“šç®¡é“ ETL Data Pipeline

ğŸ”„ ä¼æ¥­ç´š ETL (Extract, Transform, Load) æ•¸æ“šç®¡é“ï¼Œæ”¯æŒå¤šæºæ•¸æ“šæ•´åˆã€è½‰æ›å’ŒåŠ è¼‰

## å°ˆæ¡ˆç°¡ä»‹

æœ¬å°ˆæ¡ˆæä¾›å®Œæ•´çš„ ETL è§£æ±ºæ–¹æ¡ˆï¼Œæ”¯æŒå¾å¤šå€‹æ•¸æ“šæºæå–æ•¸æ“šã€åŸ·è¡Œè¤‡é›œè½‰æ›ã€è¼‰å…¥åˆ°ç›®æ¨™ç³»çµ±ã€‚å…§å»ºæ•¸æ“šè³ªé‡æª¢æŸ¥ã€éŒ¯èª¤è™•ç†ã€ç›£æ§å’Œæ—¥èªŒåŠŸèƒ½ã€‚

## åŠŸèƒ½ç‰¹é»

- âœ… å¤šæ•¸æ“šæºæ”¯æŒï¼ˆæ•¸æ“šåº«ã€APIã€æ–‡ä»¶ã€æµå¼æ•¸æ“šï¼‰
- âœ… å¯è¦–åŒ–æµç¨‹è¨­è¨ˆ
- âœ… æ•¸æ“šè½‰æ›ï¼ˆæ¸…æ´—ã€èšåˆã€é—œè¯ã€è¨ˆç®—ï¼‰
- âœ… å¢é‡æ›´æ–°å’Œå…¨é‡åŒæ­¥
- âœ… æ•¸æ“šè³ªé‡æª¢æŸ¥
- âœ… éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶
- âœ… ä»»å‹™èª¿åº¦ï¼ˆCronã€äº‹ä»¶è§¸ç™¼ï¼‰
- âœ… ç›£æ§å’Œæ—¥èªŒ
- âœ… ä¸¦è¡Œè™•ç†

## å¿«é€Ÿé–‹å§‹

### å®‰è£ä¾è³´

```bash
cd etl-pipeline
pip install -r requirements.txt
```

### 1. é…ç½®æ•¸æ“šæº

ç·¨è¼¯ `config/sources.yaml`ï¼š

```yaml
sources:
  # PostgreSQL æ•¸æ“šåº«
  sales_db:
    type: postgresql
    host: localhost
    port: 5432
    database: sales
    username: ${DB_USER}
    password: ${DB_PASSWORD}

  # REST API
  crm_api:
    type: api
    url: https://api.example.com/customers
    auth:
      type: bearer
      token: ${API_TOKEN}

  # CSV æ–‡ä»¶
  product_csv:
    type: csv
    path: data/products.csv
```

### 2. å®šç¾© ETL æµç¨‹

å‰µå»º `pipelines/sales_etl.py`ï¼š

```python
from etl import Pipeline, Extract, Transform, Load

# å‰µå»ºç®¡é“
pipeline = Pipeline(name='sales_etl')

# Extract: å¾æ•¸æ“šåº«æå–
extract = Extract(
    source='sales_db',
    query='SELECT * FROM orders WHERE updated_at > :last_run'
)

# Transform: æ•¸æ“šè½‰æ›
transform = Transform([
    # æ¸…æ´—æ•¸æ“š
    {'type': 'remove_nulls', 'columns': ['customer_id', 'amount']},
    {'type': 'convert_type', 'column': 'amount', 'to': 'float'},

    # é—œè¯ç¶­åº¦è¡¨
    {'type': 'join',
     'right_source': 'product_csv',
     'left_on': 'product_id',
     'right_on': 'id',
     'how': 'left'},

    # èšåˆè¨ˆç®—
    {'type': 'aggregate',
     'group_by': ['customer_id', 'date'],
     'agg': {'amount': 'sum', 'order_id': 'count'}}
])

# Load: è¼‰å…¥åˆ°æ•¸æ“šå€‰å„²
load = Load(
    destination='data_warehouse',
    table='fact_sales',
    mode='upsert',  # æˆ– 'append', 'replace'
    key_columns=['customer_id', 'date']
)

# çµ„è£ç®¡é“
pipeline.add_step(extract)
pipeline.add_step(transform)
pipeline.add_step(load)
```

### 3. é‹è¡Œç®¡é“

```bash
# æ‰‹å‹•é‹è¡Œ
python run_pipeline.py --pipeline sales_etl

# ä½¿ç”¨èª¿åº¦å™¨
python scheduler.py --config config/schedules.yaml
```

### 4. ç›£æ§å„€è¡¨æ¿

```bash
streamlit run monitor.py
```

è¨ªå• `http://localhost:8501` æŸ¥çœ‹ ETL é‹è¡Œç‹€æ…‹ã€‚

## ä½¿ç”¨ç¯„ä¾‹

### åŸºç¤ ETL æµç¨‹

```python
from etl import ETLPipeline

# å‰µå»ºç®¡é“
pipeline = ETLPipeline(name='customer_etl')

# Extract: å¾ API æå–æ•¸æ“š
data = pipeline.extract(
    source_type='api',
    url='https://api.example.com/customers',
    headers={'Authorization': 'Bearer TOKEN'}
)

# Transform: è½‰æ›æ•¸æ“š
data = pipeline.transform(data, [
    # é¸æ“‡æ¬„ä½
    pipeline.select_columns(['id', 'name', 'email', 'created_at']),

    # é‡å‘½åæ¬„ä½
    pipeline.rename_columns({'created_at': 'registration_date'}),

    # æ·»åŠ è¨ˆç®—æ¬„ä½
    pipeline.add_column('full_name', lambda row: f"{row['first_name']} {row['last_name']}"),

    # éæ¿¾æ•¸æ“š
    pipeline.filter_rows(lambda row: row['status'] == 'active')
])

# Load: è¼‰å…¥åˆ°æ•¸æ“šåº«
pipeline.load(
    data,
    destination='postgresql://localhost/warehouse',
    table='dim_customers',
    mode='replace'
)

# åŸ·è¡Œä¸¦ç²å–çµ±è¨ˆ
result = pipeline.run()
print(f"è™•ç† {result.rows_processed} è¡Œï¼Œè€—æ™‚ {result.duration:.2f} ç§’")
```

### å¢é‡æ›´æ–°

```python
from etl import IncrementalETL

# å¢é‡ ETL
etl = IncrementalETL(
    name='orders_incremental',
    checkpoint_column='updated_at',  # ç”¨æ–¼è¿½è¹¤å¢é‡çš„æ¬„ä½
    checkpoint_storage='redis://localhost'  # å­˜å„²æª¢æŸ¥é»
)

# æå–å¢é‡æ•¸æ“š
last_checkpoint = etl.get_last_checkpoint()
data = etl.extract(
    query=f"SELECT * FROM orders WHERE updated_at > '{last_checkpoint}'"
)

# è™•ç†æ•¸æ“š
etl.transform(data)
etl.load(data)

# æ›´æ–°æª¢æŸ¥é»
etl.update_checkpoint(data['updated_at'].max())
```

### æ•¸æ“šè³ªé‡æª¢æŸ¥

```python
from etl import DataQuality

# å®šç¾©è³ªé‡è¦å‰‡
quality_checks = DataQuality([
    # éç©ºæª¢æŸ¥
    {'type': 'not_null', 'columns': ['customer_id', 'order_id', 'amount']},

    # å”¯ä¸€æ€§æª¢æŸ¥
    {'type': 'unique', 'columns': ['order_id']},

    # ç¯„åœæª¢æŸ¥
    {'type': 'range', 'column': 'amount', 'min': 0, 'max': 1000000},

    # æ ¼å¼æª¢æŸ¥
    {'type': 'regex', 'column': 'email', 'pattern': r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'},

    # åƒç…§å®Œæ•´æ€§
    {'type': 'foreign_key',
     'column': 'customer_id',
     'reference_table': 'customers',
     'reference_column': 'id'}
])

# åŸ·è¡Œæª¢æŸ¥
results = quality_checks.validate(data)

if not results.passed:
    print(f"è³ªé‡æª¢æŸ¥å¤±æ•—: {results.failed_checks}")
    # è™•ç†å¤±æ•—è¨˜éŒ„
    failed_rows = results.get_failed_rows()
    failed_rows.to_csv('data/failed_quality_check.csv')
```

### è¤‡é›œè½‰æ›

```python
from etl import Transform

transform = Transform()

# æ•¸æ“šæ¸…æ´—
data = transform.clean(data, [
    # å»é™¤é‡è¤‡
    transform.deduplicate(subset=['customer_id', 'date']),

    # å¡«å……ç¼ºå¤±å€¼
    transform.fill_missing({'region': 'Unknown', 'category': 'Other'}),

    # æ¨™æº–åŒ–æ–‡æœ¬
    transform.standardize_text(['name', 'address'], lowercase=True, remove_special_chars=True),

    # è™•ç†ç•°å¸¸å€¼
    transform.handle_outliers('amount', method='clip', lower_percentile=1, upper_percentile=99)
])

# æ•¸æ“šè±å¯Œ
data = transform.enrich(data, [
    # åœ°ç†ç·¨ç¢¼
    transform.geocode('address', output_columns=['latitude', 'longitude']),

    # æŸ¥æ‰¾ç¶­åº¦
    transform.lookup(
        lookup_table='products',
        on='product_id',
        select=['category', 'brand', 'price']
    ),

    # è¨ˆç®—è¡ç”Ÿæ¬„ä½
    transform.calculate([
        {'name': 'profit', 'expression': 'revenue - cost'},
        {'name': 'profit_margin', 'expression': '(revenue - cost) / revenue * 100'}
    ])
])

# æ•¸æ“šèšåˆ
data = transform.aggregate(
    group_by=['customer_id', 'month'],
    aggregations={
        'amount': ['sum', 'mean', 'count'],
        'order_id': 'nunique'
    }
)
```

### ä¸¦è¡Œè™•ç†

```python
from etl import ParallelETL

# ä¸¦è¡Œè™•ç†å¤§æ•¸æ“šé›†
parallel_etl = ParallelETL(
    name='large_dataset_etl',
    num_workers=4  # ä½¿ç”¨ 4 å€‹ä¸¦è¡Œé€²ç¨‹
)

# åˆ†ç‰‡è™•ç†
parallel_etl.extract_parallel(
    source='postgresql://localhost/bigdata',
    query='SELECT * FROM large_table',
    partition_by='date',  # æŒ‰æ—¥æœŸåˆ†ç‰‡
    num_partitions=10
)

# ä¸¦è¡Œè½‰æ›
parallel_etl.transform_parallel(transform_function)

# æ‰¹é‡è¼‰å…¥
parallel_etl.load_parallel(
    destination='warehouse',
    batch_size=10000
)
```

### éŒ¯èª¤è™•ç†

```python
from etl import ETLPipeline, RetryPolicy

pipeline = ETLPipeline(
    name='resilient_etl',
    retry_policy=RetryPolicy(
        max_retries=3,
        backoff_factor=2,  # æŒ‡æ•¸é€€é¿
        retry_on=['ConnectionError', 'TimeoutError']
    )
)

# éŒ¯èª¤è¨˜éŒ„
pipeline.on_error(lambda error, row: {
    'timestamp': datetime.now(),
    'error_type': type(error).__name__,
    'error_message': str(error),
    'failed_row': row
})

# æ­»ä¿¡éšŠåˆ—
pipeline.set_dead_letter_queue('failed_records.csv')
```

## å°ˆæ¡ˆçµæ§‹

```
etl-pipeline/
â”œâ”€â”€ README.md                  # å°ˆæ¡ˆèªªæ˜
â”œâ”€â”€ requirements.txt           # ä¾è³´å¥—ä»¶
â”œâ”€â”€ run_pipeline.py           # ç®¡é“é‹è¡Œå™¨
â”œâ”€â”€ scheduler.py              # ä»»å‹™èª¿åº¦å™¨
â”œâ”€â”€ monitor.py                # ç›£æ§å„€è¡¨æ¿
â”œâ”€â”€ etl/                      # ETL æ ¸å¿ƒæ¨¡çµ„
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pipeline.py          # ç®¡é“åŸºé¡
â”‚   â”œâ”€â”€ extractors/          # æ•¸æ“šæå–å™¨
â”‚   â”‚   â”œâ”€â”€ database.py
â”‚   â”‚   â”œâ”€â”€ api.py
â”‚   â”‚   â”œâ”€â”€ file.py
â”‚   â”‚   â””â”€â”€ stream.py
â”‚   â”œâ”€â”€ transformers/        # æ•¸æ“šè½‰æ›å™¨
â”‚   â”‚   â”œâ”€â”€ cleaning.py
â”‚   â”‚   â”œâ”€â”€ aggregation.py
â”‚   â”‚   â”œâ”€â”€ join.py
â”‚   â”‚   â””â”€â”€ calculation.py
â”‚   â”œâ”€â”€ loaders/             # æ•¸æ“šåŠ è¼‰å™¨
â”‚   â”‚   â”œâ”€â”€ database.py
â”‚   â”‚   â”œâ”€â”€ file.py
â”‚   â”‚   â””â”€â”€ api.py
â”‚   â”œâ”€â”€ quality.py           # æ•¸æ“šè³ªé‡
â”‚   â””â”€â”€ monitoring.py        # ç›£æ§å’Œæ—¥èªŒ
â”œâ”€â”€ pipelines/                # ETL ç®¡é“å®šç¾©
â”‚   â”œâ”€â”€ sales_etl.py
â”‚   â”œâ”€â”€ customer_etl.py
â”‚   â””â”€â”€ product_etl.py
â”œâ”€â”€ config/                   # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ sources.yaml         # æ•¸æ“šæºé…ç½®
â”‚   â”œâ”€â”€ destinations.yaml    # ç›®æ¨™é…ç½®
â”‚   â””â”€â”€ schedules.yaml       # èª¿åº¦é…ç½®
â”œâ”€â”€ data/                     # æ•¸æ“šç›®éŒ„
â”‚   â”œâ”€â”€ input/
â”‚   â”œâ”€â”€ output/
â”‚   â””â”€â”€ failed/
â””â”€â”€ logs/                     # æ—¥èªŒ
    â””â”€â”€ etl.log
```

## æ”¯æŒçš„æ•¸æ“šæº

### æ•¸æ“šåº«

- PostgreSQL
- MySQL
- SQL Server
- Oracle
- MongoDB
- Cassandra

### æ–‡ä»¶

- CSV
- JSON
- Excel
- Parquet
- Avro

### API

- REST API
- GraphQL
- SOAP

### æµå¼æ•¸æ“š

- Kafka
- RabbitMQ
- AWS Kinesis

### é›²å­˜å„²

- AWS S3
- Google Cloud Storage
- Azure Blob Storage

## èª¿åº¦é…ç½®

```yaml
schedules:
  # æ¯æ—¥é‹è¡Œ
  daily_sales_etl:
    pipeline: sales_etl
    schedule: '0 2 * * *'  # æ¯å¤©å‡Œæ™¨ 2 é»
    timezone: Asia/Taipei

  # æ¯å°æ™‚é‹è¡Œ
  hourly_events_etl:
    pipeline: events_etl
    schedule: '0 * * * *'  # æ¯å°æ™‚æ•´é»

  # äº‹ä»¶è§¸ç™¼
  order_received:
    pipeline: order_processing_etl
    trigger: webhook
    endpoint: /webhooks/order
```

## ç›£æ§å’Œæ—¥èªŒ

### ç›£æ§æŒ‡æ¨™

- é‹è¡Œç‹€æ…‹ï¼ˆæˆåŠŸ/å¤±æ•—/é‹è¡Œä¸­ï¼‰
- è™•ç†è¡Œæ•¸
- åŸ·è¡Œæ™‚é–“
- éŒ¯èª¤ç‡
- æ•¸æ“šè³ªé‡åˆ†æ•¸

### æ—¥èªŒé…ç½®

```python
import logging

# é…ç½®æ—¥èªŒ
logging.config.dictConfig({
    'version': 1,
    'handlers': {
        'file': {
            'class': 'logging.FileHandler',
            'filename': 'logs/etl.log',
            'level': 'INFO'
        },
        'console': {
            'class': 'logging.StreamHandler',
            'level': 'DEBUG'
        }
    },
    'loggers': {
        'etl': {
            'handlers': ['file', 'console'],
            'level': 'INFO'
        }
    }
})
```

## æœ€ä½³å¯¦è¸

1. **è¨­è¨ˆåŸå‰‡**
   - å–®ä¸€è·è²¬ï¼šæ¯å€‹ç®¡é“å°ˆæ³¨ä¸€å€‹æ•¸æ“šæµ
   - å†ªç­‰æ€§ï¼šé‡è¤‡é‹è¡Œç”¢ç”Ÿç›¸åŒçµæœ
   - å¯æ¢å¾©ï¼šæ”¯æŒå¾å¤±æ•—é»æ¢å¾©

2. **æ€§èƒ½å„ªåŒ–**
   - ä½¿ç”¨å¢é‡æ›´æ–°è€Œéå…¨é‡
   - ä¸¦è¡Œè™•ç†å¤§æ•¸æ“šé›†
   - æ‰¹é‡æ’å…¥è€Œéé€è¡Œ
   - é©ç•¶ä½¿ç”¨ç·©å­˜

3. **æ•¸æ“šè³ªé‡**
   - æºé ­é©—è­‰æ•¸æ“š
   - è½‰æ›éç¨‹ä¸­æª¢æŸ¥
   - è¼‰å…¥å‰æœ€çµ‚é©—è­‰

4. **éŒ¯èª¤è™•ç†**
   - è¨˜éŒ„æ‰€æœ‰éŒ¯èª¤
   - å¤±æ•—æ•¸æ“šéš”é›¢
   - è¨­ç½®é‡è©¦æ©Ÿåˆ¶
   - ç™¼é€é è­¦é€šçŸ¥

5. **ç›£æ§é‹ç¶­**
   - ç›£æ§é—œéµæŒ‡æ¨™
   - è¨­ç½®é è­¦é–¾å€¼
   - å®šæœŸå¯©æŸ¥æ—¥èªŒ
   - å„ªåŒ–æ…¢æŸ¥è©¢

## æŠ€è¡“æ£§

- **Python 3.8+**
- **Pandas** - æ•¸æ“šè™•ç†
- **SQLAlchemy** - æ•¸æ“šåº«é€£æ¥
- **Apache Airflow** - å·¥ä½œæµèª¿åº¦ï¼ˆå¯é¸ï¼‰
- **Prefect** - ç¾ä»£å·¥ä½œæµå¼•æ“ï¼ˆå¯é¸ï¼‰
- **Great Expectations** - æ•¸æ“šè³ªé‡
- **Streamlit** - ç›£æ§ç•Œé¢

## å¸¸è¦‹å•é¡Œ

**Q: å¦‚ä½•è™•ç†å¤§æ•¸æ“šé›†ï¼Ÿ**

A:
- ä½¿ç”¨åˆ†ç‰‡/åˆ†å€è™•ç†
- å•Ÿç”¨ä¸¦è¡Œè™•ç†
- ä½¿ç”¨æµå¼è™•ç†
- è€ƒæ…®ä½¿ç”¨ Spark è™•ç† TB ç´šæ•¸æ“š

**Q: å¦‚ä½•ç¢ºä¿æ•¸æ“šä¸€è‡´æ€§ï¼Ÿ**

A:
- ä½¿ç”¨äº‹å‹™
- å¯¦ç¾å†ªç­‰æ€§
- è¨˜éŒ„æª¢æŸ¥é»
- å¯¦æ–½æ•¸æ“šè³ªé‡æª¢æŸ¥

**Q: å¦‚ä½•å„ªåŒ–æ€§èƒ½ï¼Ÿ**

A:
- åªæå–éœ€è¦çš„æ¬„ä½
- åœ¨æ•¸æ“šåº«ç«¯åšèšåˆ
- ä½¿ç”¨æ‰¹é‡æ“ä½œ
- é©ç•¶å»ºç«‹ç´¢å¼•

## æˆæ¬Š

MIT License
