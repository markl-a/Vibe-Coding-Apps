"""
ETL Pipeline - å®Œæ•´çš„è³‡æ–™æŠ½å–ã€è½‰æ›ã€è¼‰å…¥æµç¨‹
æ”¯æŒå¾å¤šç¨®æ•¸æ“šæºæå–æ•¸æ“šï¼Œé€²è¡Œæ¸…æ´—è½‰æ›ï¼Œä¸¦è¼‰å…¥åˆ°ç›®æ¨™ç³»çµ±
"""

import pandas as pd
import numpy as np
import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Any
import time


class DataExtractor:
    """æ•¸æ“šæå–å™¨"""

    def extract_from_csv(self, file_path: str) -> pd.DataFrame:
        """å¾ CSV æ–‡ä»¶æå–æ•¸æ“š"""
        print(f"ğŸ“¥ å¾ CSV æå–æ•¸æ“š: {file_path}")
        try:
            df = pd.read_csv(file_path)
            print(f"   âœ“ æå– {len(df)} ç­†è¨˜éŒ„")
            return df
        except Exception as e:
            print(f"   âœ— éŒ¯èª¤: {e}")
            return pd.DataFrame()

    def extract_from_json(self, file_path: str) -> pd.DataFrame:
        """å¾ JSON æ–‡ä»¶æå–æ•¸æ“š"""
        print(f"ğŸ“¥ å¾ JSON æå–æ•¸æ“š: {file_path}")
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            df = pd.DataFrame(data)
            print(f"   âœ“ æå– {len(df)} ç­†è¨˜éŒ„")
            return df
        except Exception as e:
            print(f"   âœ— éŒ¯èª¤: {e}")
            return pd.DataFrame()

    def extract_from_api_mock(self) -> pd.DataFrame:
        """æ¨¡æ“¬å¾ API æå–æ•¸æ“š"""
        print(f"ğŸ“¥ å¾ API æå–æ•¸æ“š (æ¨¡æ“¬)")
        # æ¨¡æ“¬ API å»¶é²
        time.sleep(0.5)

        # ç”Ÿæˆæ¨¡æ“¬æ•¸æ“š
        data = {
            'api_id': range(1, 101),
            'status': np.random.choice(['active', 'inactive', 'pending'], 100),
            'value': np.random.randint(1, 1000, 100),
            'timestamp': pd.date_range(end=datetime.now(), periods=100, freq='H')
        }
        df = pd.DataFrame(data)
        print(f"   âœ“ æå– {len(df)} ç­†è¨˜éŒ„")
        return df


class DataTransformer:
    """æ•¸æ“šè½‰æ›å™¨"""

    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•¸æ“šæ¸…æ´—"""
        print("ğŸ§¹ åŸ·è¡Œæ•¸æ“šæ¸…æ´—...")
        original_count = len(df)

        # ç§»é™¤å®Œå…¨é‡è¤‡çš„è¡Œ
        df = df.drop_duplicates()

        # ç§»é™¤å…¨éƒ¨ç‚ºç©ºçš„è¡Œ
        df = df.dropna(how='all')

        # å¡«è£œç¼ºå¤±å€¼ï¼ˆæ•¸å€¼å‹ç”¨ä¸­ä½æ•¸ï¼Œé¡åˆ¥å‹ç”¨çœ¾æ•¸ï¼‰
        for col in df.columns:
            if df[col].dtype in ['float64', 'int64']:
                df[col].fillna(df[col].median(), inplace=True)
            else:
                df[col].fillna(df[col].mode()[0] if len(df[col].mode()) > 0 else 'Unknown', inplace=True)

        cleaned_count = len(df)
        print(f"   âœ“ æ¸…æ´—å®Œæˆ: {original_count} â†’ {cleaned_count} ç­† (ç§»é™¤ {original_count - cleaned_count} ç­†)")
        return df

    def normalize_data(self, df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:
        """æ•¸æ“šæ¨™æº–åŒ–"""
        print(f"ğŸ“Š æ¨™æº–åŒ–æ¬„ä½: {', '.join(columns)}")
        for col in columns:
            if col in df.columns and df[col].dtype in ['float64', 'int64']:
                min_val = df[col].min()
                max_val = df[col].max()
                if max_val > min_val:
                    df[f'{col}_normalized'] = (df[col] - min_val) / (max_val - min_val)
                    print(f"   âœ“ {col}: [{min_val:.2f}, {max_val:.2f}] â†’ [0.00, 1.00]")
        return df

    def aggregate_data(self, df: pd.DataFrame, group_by: str, agg_dict: Dict) -> pd.DataFrame:
        """æ•¸æ“šèšåˆ"""
        print(f"ğŸ“Š æŒ‰ {group_by} èšåˆæ•¸æ“š...")
        if group_by not in df.columns:
            print(f"   âœ— æ‰¾ä¸åˆ°æ¬„ä½: {group_by}")
            return df

        result = df.groupby(group_by).agg(agg_dict).reset_index()
        print(f"   âœ“ èšåˆå®Œæˆ: {len(df)} â†’ {len(result)} ç­†")
        return result

    def enrich_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•¸æ“šå¢å¼·"""
        print("âœ¨ åŸ·è¡Œæ•¸æ“šå¢å¼·...")

        # æ·»åŠ è™•ç†æ™‚é–“æˆ³
        df['processed_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        # æ·»åŠ æ•¸æ“šå“è³ªåˆ†æ•¸ï¼ˆç¤ºä¾‹ï¼‰
        df['data_quality_score'] = np.random.uniform(0.8, 1.0, len(df))

        print(f"   âœ“ æ·»åŠ  {2} å€‹å¢å¼·æ¬„ä½")
        return df


class DataLoader:
    """æ•¸æ“šè¼‰å…¥å™¨"""

    def load_to_csv(self, df: pd.DataFrame, file_path: str):
        """è¼‰å…¥åˆ° CSV æ–‡ä»¶"""
        print(f"ğŸ’¾ è¼‰å…¥æ•¸æ“šåˆ° CSV: {file_path}")
        try:
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            df.to_csv(file_path, index=False)
            print(f"   âœ“ æˆåŠŸè¼‰å…¥ {len(df)} ç­†è¨˜éŒ„")
        except Exception as e:
            print(f"   âœ— éŒ¯èª¤: {e}")

    def load_to_json(self, df: pd.DataFrame, file_path: str):
        """è¼‰å…¥åˆ° JSON æ–‡ä»¶"""
        print(f"ğŸ’¾ è¼‰å…¥æ•¸æ“šåˆ° JSON: {file_path}")
        try:
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            df.to_json(file_path, orient='records', indent=2, force_ascii=False)
            print(f"   âœ“ æˆåŠŸè¼‰å…¥ {len(df)} ç­†è¨˜éŒ„")
        except Exception as e:
            print(f"   âœ— éŒ¯èª¤: {e}")

    def load_to_database_mock(self, df: pd.DataFrame, table_name: str):
        """æ¨¡æ“¬è¼‰å…¥åˆ°æ•¸æ“šåº«"""
        print(f"ğŸ’¾ è¼‰å…¥æ•¸æ“šåˆ°æ•¸æ“šåº« (æ¨¡æ“¬): {table_name}")
        # æ¨¡æ“¬æ•¸æ“šåº«å¯«å…¥å»¶é²
        time.sleep(0.3)
        print(f"   âœ“ æˆåŠŸè¼‰å…¥ {len(df)} ç­†è¨˜éŒ„åˆ°è¡¨ {table_name}")


class ETLPipeline:
    """ETL Pipeline ä¸»æµç¨‹"""

    def __init__(self):
        self.extractor = DataExtractor()
        self.transformer = DataTransformer()
        self.loader = DataLoader()
        self.metrics = {
            'start_time': None,
            'end_time': None,
            'records_extracted': 0,
            'records_transformed': 0,
            'records_loaded': 0,
            'errors': []
        }

    def run(self, config: Dict[str, Any]):
        """åŸ·è¡Œ ETL Pipeline"""
        print("=" * 80)
        print("ğŸš€ ETL Pipeline é–‹å§‹åŸ·è¡Œ")
        print("=" * 80)
        self.metrics['start_time'] = datetime.now()

        try:
            # 1. Extract (æå–)
            print("\nã€éšæ®µ 1/3ã€‘æ•¸æ“šæå–")
            print("-" * 80)
            df = self._extract(config.get('extract', {}))
            if df.empty:
                raise Exception("æå–éšæ®µå¤±æ•—ï¼šæ²’æœ‰æ•¸æ“š")
            self.metrics['records_extracted'] = len(df)

            # 2. Transform (è½‰æ›)
            print("\nã€éšæ®µ 2/3ã€‘æ•¸æ“šè½‰æ›")
            print("-" * 80)
            df = self._transform(df, config.get('transform', {}))
            self.metrics['records_transformed'] = len(df)

            # 3. Load (è¼‰å…¥)
            print("\nã€éšæ®µ 3/3ã€‘æ•¸æ“šè¼‰å…¥")
            print("-" * 80)
            self._load(df, config.get('load', {}))
            self.metrics['records_loaded'] = len(df)

            self.metrics['end_time'] = datetime.now()
            self._print_summary()

        except Exception as e:
            self.metrics['errors'].append(str(e))
            print(f"\nâŒ Pipeline åŸ·è¡Œå¤±æ•—: {e}")
            raise

    def _extract(self, extract_config: Dict) -> pd.DataFrame:
        """æå–æ•¸æ“š"""
        source_type = extract_config.get('type', 'csv')
        source_path = extract_config.get('path', '')

        if source_type == 'csv':
            return self.extractor.extract_from_csv(source_path)
        elif source_type == 'json':
            return self.extractor.extract_from_json(source_path)
        elif source_type == 'api':
            return self.extractor.extract_from_api_mock()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•¸æ“šæºé¡å‹: {source_type}")

    def _transform(self, df: pd.DataFrame, transform_config: Dict) -> pd.DataFrame:
        """è½‰æ›æ•¸æ“š"""
        # æ•¸æ“šæ¸…æ´—
        if transform_config.get('clean', True):
            df = self.transformer.clean_data(df)

        # æ•¸æ“šæ¨™æº–åŒ–
        if 'normalize' in transform_config:
            df = self.transformer.normalize_data(df, transform_config['normalize'])

        # æ•¸æ“šèšåˆ
        if 'aggregate' in transform_config:
            agg_config = transform_config['aggregate']
            df = self.transformer.aggregate_data(
                df,
                agg_config['group_by'],
                agg_config['functions']
            )

        # æ•¸æ“šå¢å¼·
        if transform_config.get('enrich', True):
            df = self.transformer.enrich_data(df)

        return df

    def _load(self, df: pd.DataFrame, load_config: Dict):
        """è¼‰å…¥æ•¸æ“š"""
        target_type = load_config.get('type', 'csv')
        target_path = load_config.get('path', 'output/result.csv')

        if target_type == 'csv':
            self.loader.load_to_csv(df, target_path)
        elif target_type == 'json':
            self.loader.load_to_json(df, target_path)
        elif target_type == 'database':
            table_name = load_config.get('table', 'etl_result')
            self.loader.load_to_database_mock(df, table_name)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç›®æ¨™é¡å‹: {target_type}")

    def _print_summary(self):
        """æ‰“å°åŸ·è¡Œæ‘˜è¦"""
        duration = (self.metrics['end_time'] - self.metrics['start_time']).total_seconds()

        print("\n" + "=" * 80)
        print("âœ… ETL Pipeline åŸ·è¡Œå®Œæˆ")
        print("=" * 80)
        print(f"â±ï¸  åŸ·è¡Œæ™‚é–“: {duration:.2f} ç§’")
        print(f"ğŸ“¥ æå–è¨˜éŒ„: {self.metrics['records_extracted']:,} ç­†")
        print(f"ğŸ”„ è½‰æ›è¨˜éŒ„: {self.metrics['records_transformed']:,} ç­†")
        print(f"ğŸ’¾ è¼‰å…¥è¨˜éŒ„: {self.metrics['records_loaded']:,} ç­†")

        if self.metrics['errors']:
            print(f"\nâš ï¸  ç™¼ç”Ÿ {len(self.metrics['errors'])} å€‹éŒ¯èª¤:")
            for error in self.metrics['errors']:
                print(f"   - {error}")
        else:
            print("\nğŸ‰ æ²’æœ‰éŒ¯èª¤ç™¼ç”Ÿ")
        print("=" * 80)


def example_1_basic_pipeline():
    """ç¯„ä¾‹ 1ï¼šåŸºæœ¬çš„ CSV è™•ç†"""
    print("\n" + "ğŸ”µ" * 40)
    print("ç¯„ä¾‹ 1ï¼šåŸºæœ¬ ETL Pipeline - CSV æ•¸æ“šè™•ç†")
    print("ğŸ”µ" * 40)

    # å…ˆç”Ÿæˆç¤ºä¾‹æ•¸æ“š
    from data_generator import generate_sales_data
    os.makedirs('data', exist_ok=True)
    sales_df = generate_sales_data()
    sales_df.to_csv('data/sales_input.csv', index=False)

    config = {
        'extract': {
            'type': 'csv',
            'path': 'data/sales_input.csv'
        },
        'transform': {
            'clean': True,
            'enrich': True
        },
        'load': {
            'type': 'csv',
            'path': 'output/sales_cleaned.csv'
        }
    }

    pipeline = ETLPipeline()
    pipeline.run(config)


def example_2_aggregation_pipeline():
    """ç¯„ä¾‹ 2ï¼šå¸¶èšåˆçš„ ETL"""
    print("\n" + "ğŸŸ¢" * 40)
    print("ç¯„ä¾‹ 2ï¼šèšåˆåˆ†æ Pipeline - æŒ‰é¡åˆ¥çµ±è¨ˆ")
    print("ğŸŸ¢" * 40)

    # ç”Ÿæˆç¤ºä¾‹æ•¸æ“š
    from data_generator import generate_sales_data
    os.makedirs('data', exist_ok=True)
    sales_df = generate_sales_data()
    sales_df.to_csv('data/sales_input.csv', index=False)

    config = {
        'extract': {
            'type': 'csv',
            'path': 'data/sales_input.csv'
        },
        'transform': {
            'clean': True,
            'aggregate': {
                'group_by': 'category',
                'functions': {
                    'amount': ['sum', 'mean', 'count'],
                    'quantity': 'sum'
                }
            },
            'enrich': True
        },
        'load': {
            'type': 'json',
            'path': 'output/category_summary.json'
        }
    }

    pipeline = ETLPipeline()
    pipeline.run(config)


def example_3_api_to_database():
    """ç¯„ä¾‹ 3ï¼šAPI åˆ°æ•¸æ“šåº«"""
    print("\n" + "ğŸŸ¡" * 40)
    print("ç¯„ä¾‹ 3ï¼šAPI Pipeline - å¾ API æå–ä¸¦è¼‰å…¥æ•¸æ“šåº«")
    print("ğŸŸ¡" * 40)

    config = {
        'extract': {
            'type': 'api'
        },
        'transform': {
            'clean': True,
            'normalize': ['value'],
            'enrich': True
        },
        'load': {
            'type': 'database',
            'table': 'api_data'
        }
    }

    pipeline = ETLPipeline()
    pipeline.run(config)


def main():
    """ä¸»å‡½æ•¸ï¼šé‹è¡Œæ‰€æœ‰ç¯„ä¾‹"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                         ETL Pipeline ç¤ºç¯„ç³»çµ±                                 â•‘
â•‘                                                                              â•‘
â•‘  å±•ç¤ºå®Œæ•´çš„ Extract (æå–)ã€Transform (è½‰æ›)ã€Load (è¼‰å…¥) æµç¨‹               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # é‹è¡Œæ‰€æœ‰ç¯„ä¾‹
    try:
        example_1_basic_pipeline()
        example_2_aggregation_pipeline()
        example_3_api_to_database()

        print("\n" + "ğŸ‰" * 40)
        print("æ‰€æœ‰ ETL Pipeline ç¯„ä¾‹åŸ·è¡Œå®Œæˆï¼")
        print("ğŸ‰" * 40)
        print("\nğŸ“ è¼¸å‡ºæ–‡ä»¶ä½ç½®:")
        print("   - output/sales_cleaned.csv")
        print("   - output/category_summary.json")
        print("\nğŸ’¡ æç¤ºï¼šå¯ä»¥æŸ¥çœ‹è¼¸å‡ºæ–‡ä»¶ä¾†ç¢ºèª ETL è™•ç†çµæœ")

    except Exception as e:
        print(f"\nâŒ åŸ·è¡Œå¤±æ•—: {e}")
        raise


if __name__ == '__main__':
    main()
