"""
AI æ´å¯Ÿå¼•æ“æ ¸å¿ƒæ¨¡å—
æä¾›è‡ªåŠ¨å¼‚å¸¸æ£€æµ‹ã€è¶‹åŠ¿åˆ†æã€ç›¸å…³æ€§å‘ç°å’Œæ™ºèƒ½æ´å¯Ÿç”Ÿæˆ
"""

import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
from scipy import stats
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.cluster import DBSCAN, KMeans
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')


class InsightsEngine:
    """AI æ´å¯Ÿå¼•æ“"""

    def __init__(self, config: Optional[Dict] = None):
        """åˆå§‹åŒ–æ´å¯Ÿå¼•æ“"""
        self.config = config or self._default_config()
        self.scaler = StandardScaler()

    def _default_config(self) -> Dict:
        """é»˜è®¤é…ç½®"""
        return {
            'anomaly_detection': {
                'zscore_threshold': 3.0,
                'iqr_multiplier': 1.5,
                'isolation_forest_contamination': 0.1,
                'lof_neighbors': 20
            },
            'trend_detection': {
                'min_r_squared': 0.5,
                'min_p_value': 0.05,
                'min_duration': 7
            },
            'correlation': {
                'min_correlation': 0.5,
                'method': 'pearson'
            },
            'clustering': {
                'n_clusters': 3,
                'min_cluster_size': 5
            }
        }

    # ==================== å¼‚å¸¸æ£€æµ‹ ====================

    def detect_anomalies(
        self,
        data: pd.DataFrame,
        column: str,
        method: str = 'zscore',
        **kwargs
    ) -> List[Dict[str, Any]]:
        """
        æ£€æµ‹å¼‚å¸¸å€¼

        Args:
            data: è¾“å…¥æ•°æ®
            column: è¦æ£€æµ‹çš„åˆ—å
            method: æ£€æµ‹æ–¹æ³• ('zscore', 'iqr', 'isolation_forest', 'lof')
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            å¼‚å¸¸ç‚¹åˆ—è¡¨
        """
        if column not in data.columns:
            raise ValueError(f"åˆ— '{column}' ä¸å­˜åœ¨")

        if method == 'zscore':
            return self._detect_anomalies_zscore(data, column, **kwargs)
        elif method == 'iqr':
            return self._detect_anomalies_iqr(data, column, **kwargs)
        elif method == 'isolation_forest':
            return self._detect_anomalies_isolation_forest(data, column, **kwargs)
        elif method == 'lof':
            return self._detect_anomalies_lof(data, column, **kwargs)
        else:
            raise ValueError(f"æœªçŸ¥çš„æ£€æµ‹æ–¹æ³•: {method}")

    def _detect_anomalies_zscore(
        self,
        data: pd.DataFrame,
        column: str,
        threshold: Optional[float] = None
    ) -> List[Dict[str, Any]]:
        """Z-Score å¼‚å¸¸æ£€æµ‹"""
        threshold = threshold or self.config['anomaly_detection']['zscore_threshold']

        values = data[column].values
        mean = np.mean(values)
        std = np.std(values)

        if std == 0:
            return []

        z_scores = np.abs((values - mean) / std)
        anomaly_mask = z_scores > threshold

        anomalies = []
        for idx in np.where(anomaly_mask)[0]:
            anomalies.append({
                'index': int(idx),
                'value': float(values[idx]),
                'z_score': float(z_scores[idx]),
                'deviation': float((values[idx] - mean) / mean * 100),
                'method': 'zscore',
                'severity': self._calculate_severity(z_scores[idx], threshold)
            })

        return anomalies

    def _detect_anomalies_iqr(
        self,
        data: pd.DataFrame,
        column: str,
        multiplier: Optional[float] = None
    ) -> List[Dict[str, Any]]:
        """IQR (å››åˆ†ä½è·) å¼‚å¸¸æ£€æµ‹"""
        multiplier = multiplier or self.config['anomaly_detection']['iqr_multiplier']

        values = data[column].values
        q1 = np.percentile(values, 25)
        q3 = np.percentile(values, 75)
        iqr = q3 - q1

        lower_bound = q1 - multiplier * iqr
        upper_bound = q3 + multiplier * iqr

        anomaly_mask = (values < lower_bound) | (values > upper_bound)

        anomalies = []
        for idx in np.where(anomaly_mask)[0]:
            value = values[idx]
            if value < lower_bound:
                distance = (lower_bound - value) / iqr
            else:
                distance = (value - upper_bound) / iqr

            anomalies.append({
                'index': int(idx),
                'value': float(value),
                'iqr_distance': float(distance),
                'lower_bound': float(lower_bound),
                'upper_bound': float(upper_bound),
                'method': 'iqr',
                'severity': self._calculate_severity(distance, multiplier)
            })

        return anomalies

    def _detect_anomalies_isolation_forest(
        self,
        data: pd.DataFrame,
        column: str,
        contamination: Optional[float] = None
    ) -> List[Dict[str, Any]]:
        """Isolation Forest å¼‚å¸¸æ£€æµ‹"""
        contamination = contamination or self.config['anomaly_detection']['isolation_forest_contamination']

        values = data[[column]].values

        # è®­ç»ƒ Isolation Forest
        iso_forest = IsolationForest(
            contamination=contamination,
            random_state=42,
            n_estimators=100
        )
        predictions = iso_forest.fit_predict(values)
        scores = iso_forest.score_samples(values)

        anomalies = []
        for idx, (pred, score) in enumerate(zip(predictions, scores)):
            if pred == -1:  # å¼‚å¸¸ç‚¹
                anomalies.append({
                    'index': int(idx),
                    'value': float(values[idx][0]),
                    'anomaly_score': float(-score),  # è´Ÿåˆ†æ•°ï¼Œè¶Šå¤§è¶Šå¼‚å¸¸
                    'method': 'isolation_forest',
                    'severity': self._calculate_severity(-score, 0.5)
                })

        return anomalies

    def _detect_anomalies_lof(
        self,
        data: pd.DataFrame,
        column: str,
        n_neighbors: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """Local Outlier Factor å¼‚å¸¸æ£€æµ‹"""
        n_neighbors = n_neighbors or self.config['anomaly_detection']['lof_neighbors']

        values = data[[column]].values

        # è®­ç»ƒ LOF
        lof = LocalOutlierFactor(
            n_neighbors=min(n_neighbors, len(values) - 1),
            contamination='auto'
        )
        predictions = lof.fit_predict(values)
        scores = -lof.negative_outlier_factor_

        anomalies = []
        for idx, (pred, score) in enumerate(zip(predictions, scores)):
            if pred == -1:  # å¼‚å¸¸ç‚¹
                anomalies.append({
                    'index': int(idx),
                    'value': float(values[idx][0]),
                    'lof_score': float(score),
                    'method': 'lof',
                    'severity': self._calculate_severity(score, 1.5)
                })

        return anomalies

    def _calculate_severity(self, score: float, threshold: float) -> str:
        """è®¡ç®—å¼‚å¸¸ä¸¥é‡ç¨‹åº¦"""
        ratio = score / threshold
        if ratio > 2:
            return 'critical'
        elif ratio > 1.5:
            return 'high'
        elif ratio > 1:
            return 'medium'
        else:
            return 'low'

    # ==================== è¶‹åŠ¿æ£€æµ‹ ====================

    def detect_trends(
        self,
        data: pd.DataFrame,
        column: str,
        min_r_squared: Optional[float] = None,
        min_p_value: Optional[float] = None
    ) -> List[Dict[str, Any]]:
        """
        æ£€æµ‹æ•°æ®è¶‹åŠ¿

        Args:
            data: è¾“å…¥æ•°æ®
            column: è¦åˆ†æçš„åˆ—å
            min_r_squared: æœ€å° RÂ² å€¼
            min_p_value: æœ€å¤§ p å€¼

        Returns:
            è¶‹åŠ¿ä¿¡æ¯åˆ—è¡¨
        """
        min_r_squared = min_r_squared or self.config['trend_detection']['min_r_squared']
        min_p_value = min_p_value or self.config['trend_detection']['min_p_value']

        if column not in data.columns:
            raise ValueError(f"åˆ— '{column}' ä¸å­˜åœ¨")

        values = data[column].values
        x = np.arange(len(values))

        # çº¿æ€§å›å½’
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, values)
        r_squared = r_value ** 2

        trends = []

        # æ£€æŸ¥æ˜¯å¦æ˜¾è‘—
        if r_squared >= min_r_squared and p_value <= min_p_value:
            # è®¡ç®—å˜åŒ–ç‡
            start_value = values[0]
            end_value = values[-1]
            change_percent = ((end_value - start_value) / start_value * 100) if start_value != 0 else 0

            # ç¡®å®šæ–¹å‘
            if slope > 0:
                direction = 'increasing'
                emoji = 'ğŸ“ˆ'
            elif slope < 0:
                direction = 'decreasing'
                emoji = 'ğŸ“‰'
            else:
                direction = 'stable'
                emoji = 'â¡ï¸'

            trends.append({
                'column': column,
                'direction': direction,
                'emoji': emoji,
                'slope': float(slope),
                'intercept': float(intercept),
                'r_squared': float(r_squared),
                'p_value': float(p_value),
                'change_percent': float(change_percent),
                'strength': 'strong' if r_squared > 0.8 else 'moderate' if r_squared > 0.6 else 'weak',
                'confidence': float(1 - p_value)
            })

        return trends

    def detect_seasonality(
        self,
        data: pd.DataFrame,
        column: str,
        period: int = 7
    ) -> Dict[str, Any]:
        """
        æ£€æµ‹å­£èŠ‚æ€§æ¨¡å¼

        Args:
            data: è¾“å…¥æ•°æ®
            column: è¦åˆ†æçš„åˆ—å
            period: å‘¨æœŸé•¿åº¦ï¼ˆé»˜è®¤7å¤©ï¼‰

        Returns:
            å­£èŠ‚æ€§ä¿¡æ¯
        """
        values = data[column].values

        if len(values) < period * 2:
            return {'has_seasonality': False, 'reason': 'æ•°æ®é•¿åº¦ä¸è¶³'}

        # ç®€å•çš„å­£èŠ‚æ€§æ£€æµ‹ï¼šè®¡ç®—å‘¨æœŸæ€§ç›¸å…³
        n_periods = len(values) // period
        reshaped = values[:n_periods * period].reshape(n_periods, period)

        # è®¡ç®—æ¯ä¸ªå‘¨æœŸä½ç½®çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®
        period_means = np.mean(reshaped, axis=0)
        period_stds = np.std(reshaped, axis=0)
        overall_std = np.std(values)

        # å¦‚æœå‘¨æœŸå†…çš„å˜å¼‚æ˜¾è‘—å¤§äºå‘¨æœŸé—´çš„å˜å¼‚ï¼Œåˆ™å­˜åœ¨å­£èŠ‚æ€§
        seasonality_strength = np.std(period_means) / overall_std if overall_std > 0 else 0

        has_seasonality = seasonality_strength > 0.3

        return {
            'has_seasonality': bool(has_seasonality),
            'period': int(period),
            'strength': float(seasonality_strength),
            'pattern': period_means.tolist(),
            'interpretation': self._interpret_seasonality(period, period_means)
        }

    def _interpret_seasonality(self, period: int, pattern: np.ndarray) -> str:
        """è§£é‡Šå­£èŠ‚æ€§æ¨¡å¼"""
        max_idx = np.argmax(pattern)
        min_idx = np.argmin(pattern)

        if period == 7:
            days = ['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥']
            return f"æœ€é«˜: {days[max_idx]}, æœ€ä½: {days[min_idx]}"
        elif period == 30 or period == 31:
            return f"æœ€é«˜: ç¬¬{max_idx+1}å¤©, æœ€ä½: ç¬¬{min_idx+1}å¤©"
        else:
            return f"å‘¨æœŸ{period}: æœ€é«˜ä½ç½®{max_idx}, æœ€ä½ä½ç½®{min_idx}"

    # ==================== ç›¸å…³æ€§åˆ†æ ====================

    def find_correlations(
        self,
        data: pd.DataFrame,
        threshold: Optional[float] = None,
        method: str = 'pearson'
    ) -> List[Dict[str, Any]]:
        """
        å‘ç°å˜é‡é—´çš„ç›¸å…³æ€§

        Args:
            data: è¾“å…¥æ•°æ®
            threshold: æœ€å°ç›¸å…³ç³»æ•°é˜ˆå€¼
            method: ç›¸å…³æ€§æ–¹æ³• ('pearson', 'spearman', 'kendall')

        Returns:
            ç›¸å…³æ€§åˆ—è¡¨
        """
        threshold = threshold or self.config['correlation']['min_correlation']

        # åªé€‰æ‹©æ•°å€¼åˆ—
        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()

        if len(numeric_cols) < 2:
            return []

        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
        corr_matrix = data[numeric_cols].corr(method=method)

        correlations = []

        # æå–æ˜¾è‘—ç›¸å…³æ€§
        for i, col1 in enumerate(numeric_cols):
            for j, col2 in enumerate(numeric_cols):
                if i < j:  # é¿å…é‡å¤å’Œè‡ªç›¸å…³
                    corr = corr_matrix.loc[col1, col2]
                    if abs(corr) >= threshold:
                        correlations.append({
                            'variable1': col1,
                            'variable2': col2,
                            'correlation': float(corr),
                            'strength': self._correlation_strength(abs(corr)),
                            'direction': 'positive' if corr > 0 else 'negative',
                            'interpretation': self._interpret_correlation(col1, col2, corr)
                        })

        # æŒ‰ç›¸å…³æ€§å¼ºåº¦æ’åº
        correlations.sort(key=lambda x: abs(x['correlation']), reverse=True)

        return correlations

    def _correlation_strength(self, corr: float) -> str:
        """ç›¸å…³æ€§å¼ºåº¦åˆ†ç±»"""
        if corr >= 0.8:
            return 'very_strong'
        elif corr >= 0.6:
            return 'strong'
        elif corr >= 0.4:
            return 'moderate'
        else:
            return 'weak'

    def _interpret_correlation(self, var1: str, var2: str, corr: float) -> str:
        """è§£é‡Šç›¸å…³æ€§"""
        direction = "æ­£ç›¸å…³" if corr > 0 else "è´Ÿç›¸å…³"
        strength = self._correlation_strength(abs(corr))

        strength_cn = {
            'very_strong': 'éå¸¸å¼º',
            'strong': 'å¼º',
            'moderate': 'ä¸­ç­‰',
            'weak': 'å¼±'
        }

        return f"{var1} ä¸ {var2} å‘ˆç°{strength_cn[strength]}{direction} (r={corr:.2f})"

    # ==================== æ¨¡å¼å‘ç° ====================

    def discover_patterns(
        self,
        data: pd.DataFrame,
        method: str = 'kmeans',
        **kwargs
    ) -> Dict[str, Any]:
        """
        å‘ç°æ•°æ®æ¨¡å¼

        Args:
            data: è¾“å…¥æ•°æ®
            method: èšç±»æ–¹æ³• ('kmeans', 'dbscan')
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            æ¨¡å¼ä¿¡æ¯
        """
        # åªä½¿ç”¨æ•°å€¼åˆ—
        numeric_data = data.select_dtypes(include=[np.number])

        if numeric_data.empty:
            return {'patterns': [], 'error': 'æ²¡æœ‰æ•°å€¼åˆ—å¯ä¾›åˆ†æ'}

        # æ ‡å‡†åŒ–æ•°æ®
        scaled_data = self.scaler.fit_transform(numeric_data)

        if method == 'kmeans':
            return self._kmeans_clustering(scaled_data, numeric_data.columns, **kwargs)
        elif method == 'dbscan':
            return self._dbscan_clustering(scaled_data, numeric_data.columns, **kwargs)
        else:
            raise ValueError(f"æœªçŸ¥çš„èšç±»æ–¹æ³•: {method}")

    def _kmeans_clustering(
        self,
        scaled_data: np.ndarray,
        columns: List[str],
        n_clusters: Optional[int] = None
    ) -> Dict[str, Any]:
        """K-Means èšç±»"""
        n_clusters = n_clusters or self.config['clustering']['n_clusters']
        n_clusters = min(n_clusters, len(scaled_data))  # ç¡®ä¿ä¸è¶…è¿‡æ•°æ®ç‚¹æ•°

        if n_clusters < 2:
            return {'patterns': [], 'error': 'æ•°æ®ç‚¹å¤ªå°‘ï¼Œæ— æ³•èšç±»'}

        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        labels = kmeans.fit_predict(scaled_data)

        # åˆ†ææ¯ä¸ªç°‡
        patterns = []
        for cluster_id in range(n_clusters):
            mask = labels == cluster_id
            cluster_size = np.sum(mask)

            if cluster_size > 0:
                # è®¡ç®—ç°‡ä¸­å¿ƒåœ¨åŸå§‹å°ºåº¦çš„å€¼
                cluster_center = kmeans.cluster_centers_[cluster_id]

                patterns.append({
                    'cluster_id': int(cluster_id),
                    'size': int(cluster_size),
                    'percentage': float(cluster_size / len(labels) * 100),
                    'center': cluster_center.tolist(),
                    'description': self._describe_cluster(cluster_id, cluster_center, columns)
                })

        return {
            'method': 'kmeans',
            'n_clusters': n_clusters,
            'patterns': patterns,
            'labels': labels.tolist()
        }

    def _dbscan_clustering(
        self,
        scaled_data: np.ndarray,
        columns: List[str],
        eps: float = 0.5,
        min_samples: int = 5
    ) -> Dict[str, Any]:
        """DBSCAN èšç±»"""
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(scaled_data)

        unique_labels = set(labels)
        patterns = []

        for cluster_id in unique_labels:
            if cluster_id == -1:  # å™ªå£°ç‚¹
                continue

            mask = labels == cluster_id
            cluster_size = np.sum(mask)
            cluster_data = scaled_data[mask]
            cluster_center = np.mean(cluster_data, axis=0)

            patterns.append({
                'cluster_id': int(cluster_id),
                'size': int(cluster_size),
                'percentage': float(cluster_size / len(labels) * 100),
                'center': cluster_center.tolist(),
                'description': self._describe_cluster(cluster_id, cluster_center, columns)
            })

        noise_count = np.sum(labels == -1)

        return {
            'method': 'dbscan',
            'n_clusters': len(patterns),
            'patterns': patterns,
            'noise_points': int(noise_count),
            'labels': labels.tolist()
        }

    def _describe_cluster(
        self,
        cluster_id: int,
        center: np.ndarray,
        columns: List[str]
    ) -> str:
        """æè¿°ç°‡çš„ç‰¹å¾"""
        # æ‰¾å‡ºæœ€æ˜¾è‘—çš„ç‰¹å¾ï¼ˆè·ç¦»0æœ€è¿œçš„ï¼‰
        abs_center = np.abs(center)
        top_features_idx = np.argsort(abs_center)[-2:]  # å–å‰2ä¸ªæœ€æ˜¾è‘—ç‰¹å¾

        features = []
        for idx in top_features_idx:
            value = center[idx]
            col = columns[idx]
            level = "é«˜" if value > 0.5 else "ä½" if value < -0.5 else "ä¸­"
            features.append(f"{level}{col}")

        return f"ç°‡ {cluster_id}: {', '.join(features)}"

    # ==================== ç»¼åˆæ´å¯Ÿç”Ÿæˆ ====================

    def generate_insights(
        self,
        data: pd.DataFrame,
        min_importance: int = 1,
        max_insights: int = 20
    ) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆç»¼åˆæ´å¯Ÿ

        Args:
            data: è¾“å…¥æ•°æ®
            min_importance: æœ€å°é‡è¦æ€§ï¼ˆ1-5ï¼‰
            max_insights: æœ€å¤§æ´å¯Ÿæ•°é‡

        Returns:
            æ´å¯Ÿåˆ—è¡¨
        """
        insights = []

        # è·å–æ‰€æœ‰æ•°å€¼åˆ—
        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()

        if not numeric_cols:
            return insights

        # 1. å¼‚å¸¸æ´å¯Ÿ
        for col in numeric_cols:
            try:
                anomalies = self.detect_anomalies(data, col, method='isolation_forest')
                if anomalies:
                    # åªæŠ¥å‘Šæœ€ä¸¥é‡çš„å¼‚å¸¸
                    critical_anomalies = [a for a in anomalies if a['severity'] in ['critical', 'high']]
                    if critical_anomalies:
                        importance = 5 if len(critical_anomalies) > 3 else 4
                        insights.append({
                            'type': 'anomaly',
                            'title': f'{col} å‘ç°å¼‚å¸¸å€¼',
                            'description': f'åœ¨ {col} ä¸­æ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¼‚å¸¸æ•°æ®ç‚¹ï¼Œå…¶ä¸­ {len(critical_anomalies)} ä¸ªä¸ºé«˜ä¸¥é‡åº¦å¼‚å¸¸',
                            'importance': importance,
                            'details': critical_anomalies[:3],  # åªæ˜¾ç¤ºå‰3ä¸ª
                            'recommendation': f'å»ºè®®æ£€æŸ¥ {col} çš„æ•°æ®æ¥æºå’Œå¤„ç†æµç¨‹ï¼Œè°ƒæŸ¥å¼‚å¸¸åŸå› '
                        })
            except Exception:
                pass

        # 2. è¶‹åŠ¿æ´å¯Ÿ
        for col in numeric_cols:
            try:
                trends = self.detect_trends(data, col)
                if trends:
                    trend = trends[0]
                    importance = 4 if trend['strength'] == 'strong' else 3

                    insights.append({
                        'type': 'trend',
                        'title': f'{col} å‘ˆç°{trend["direction"]}è¶‹åŠ¿',
                        'description': f'{col} æ˜¾ç¤ºå‡º{trend["strength"]}çš„{trend["direction"]}è¶‹åŠ¿ï¼Œå˜åŒ–å¹…åº¦ä¸º {trend["change_percent"]:.1f}%',
                        'importance': importance,
                        'details': trend,
                        'recommendation': self._trend_recommendation(col, trend)
                    })
            except Exception:
                pass

        # 3. ç›¸å…³æ€§æ´å¯Ÿ
        try:
            correlations = self.find_correlations(data, threshold=0.6)
            for corr in correlations[:3]:  # åªå–å‰3ä¸ªæœ€å¼ºç›¸å…³æ€§
                importance = 4 if corr['strength'] == 'very_strong' else 3

                insights.append({
                    'type': 'correlation',
                    'title': f'{corr["variable1"]} ä¸ {corr["variable2"]} é«˜åº¦ç›¸å…³',
                    'description': corr['interpretation'],
                    'importance': importance,
                    'details': corr,
                    'recommendation': f'è€ƒè™‘åˆ©ç”¨ {corr["variable1"]} å’Œ {corr["variable2"]} çš„å…³ç³»è¿›è¡Œé¢„æµ‹æˆ–ä¼˜åŒ–'
                })
        except Exception:
            pass

        # 4. æ¨¡å¼æ´å¯Ÿ
        try:
            patterns = self.discover_patterns(data, method='kmeans')
            if patterns.get('patterns'):
                insights.append({
                    'type': 'pattern',
                    'title': f'è¯†åˆ«å‡º {len(patterns["patterns"])} ä¸ªæ•°æ®ç¾¤ç»„',
                    'description': f'æ•°æ®å¯ä»¥åˆ†ä¸º {len(patterns["patterns"])} ä¸ªä¸åŒçš„ç¾¤ç»„ï¼Œæ¯ä¸ªç¾¤ç»„å…·æœ‰ç‹¬ç‰¹çš„ç‰¹å¾',
                    'importance': 3,
                    'details': patterns,
                    'recommendation': 'é’ˆå¯¹ä¸åŒç¾¤ç»„åˆ¶å®šå·®å¼‚åŒ–ç­–ç•¥'
                })
        except Exception:
            pass

        # æŒ‰é‡è¦æ€§æ’åºå¹¶è¿‡æ»¤
        insights = [i for i in insights if i['importance'] >= min_importance]
        insights.sort(key=lambda x: x['importance'], reverse=True)

        return insights[:max_insights]

    def _trend_recommendation(self, column: str, trend: Dict) -> str:
        """ç”Ÿæˆè¶‹åŠ¿å»ºè®®"""
        if trend['direction'] == 'increasing':
            if 'revenue' in column.lower() or 'sales' in column.lower():
                return f'ç»§ç»­ä¿æŒå½“å‰ç­–ç•¥ï¼Œ{column} å¢é•¿æ€åŠ¿è‰¯å¥½'
            elif 'cost' in column.lower() or 'expense' in column.lower():
                return f'å…³æ³¨ {column} çš„ä¸Šå‡è¶‹åŠ¿ï¼Œè€ƒè™‘æˆæœ¬æ§åˆ¶æªæ–½'
            else:
                return f'ç›‘æ§ {column} çš„å¢é•¿ï¼Œç¡®ä¿å¯æŒç»­æ€§'
        elif trend['direction'] == 'decreasing':
            if 'revenue' in column.lower() or 'sales' in column.lower():
                return f'è­¦å‘Šï¼š{column} å‘ˆä¸‹é™è¶‹åŠ¿ï¼Œéœ€è¦åŠæ—¶é‡‡å–æªæ–½'
            elif 'cost' in column.lower() or 'expense' in column.lower():
                return f'{column} ä¸‹é™æ˜¯ç§¯æä¿¡å·ï¼Œç»§ç»­ä¼˜åŒ–'
            else:
                return f'åˆ†æ {column} ä¸‹é™çš„åŸå› ï¼Œè¯„ä¼°å½±å“'
        else:
            return f'{column} ä¿æŒç¨³å®š'
