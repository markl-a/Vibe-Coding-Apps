import { Injectable } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';

interface TranscriptionSegment {
  speaker: string;
  text: string;
  timestamp: number;
  confidence: number;
}

interface MeetingSummary {
  title: string;
  summary: string;
  keyPoints: string[];
  decisions: string[];
  actionItems: Array<{
    task: string;
    assignee?: string;
    dueDate?: string;
    priority: 'low' | 'medium' | 'high';
  }>;
  participants: string[];
  duration: number;
  sentiment: 'positive' | 'neutral' | 'negative';
}

@Injectable()
export class MeetingAIService {
  private apiKey: string;
  private apiEndpoint: string;

  constructor(private configService: ConfigService) {
    this.apiKey = this.configService.get<string>('OPENAI_API_KEY') || '';
    this.apiEndpoint = 'https://api.openai.com/v1/chat/completions';
  }

  /**
   * å³æ™‚èªéŸ³è½‰æ–‡å­— (ä½¿ç”¨ Whisper API)
   */
  async transcribeAudio(audioBuffer: Buffer): Promise<TranscriptionSegment[]> {
    try {
      // ä½¿ç”¨ OpenAI Whisper API é€²è¡Œè½‰éŒ„
      const formData = new FormData();
      formData.append('file', new Blob([audioBuffer]), 'audio.webm');
      formData.append('model', 'whisper-1');
      formData.append('language', 'zh');
      formData.append('response_format', 'verbose_json');

      const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
        },
        body: formData,
      });

      if (!response.ok) {
        throw new Error(`Whisper API error: ${response.statusText}`);
      }

      const result = await response.json();

      // è™•ç†è½‰éŒ„çµæœ
      return this.processTranscription(result);
    } catch (error) {
      console.error('Error transcribing audio:', error);
      return [];
    }
  }

  /**
   * ç”Ÿæˆå³æ™‚å­—å¹•
   */
  async generateRealTimeCaptions(
    audioChunk: Buffer,
  ): Promise<{ text: string; timestamp: number }> {
    try {
      const segments = await this.transcribeAudio(audioChunk);
      if (segments.length > 0) {
        return {
          text: segments[0].text,
          timestamp: segments[0].timestamp,
        };
      }
      return { text: '', timestamp: Date.now() };
    } catch (error) {
      console.error('Error generating captions:', error);
      return { text: '', timestamp: Date.now() };
    }
  }

  /**
   * ç”Ÿæˆæœƒè­°æ‘˜è¦
   */
  async generateMeetingSummary(
    transcripts: TranscriptionSegment[],
    meetingInfo: {
      title: string;
      participants: string[];
      duration: number;
    },
  ): Promise<MeetingSummary> {
    try {
      const transcriptText = transcripts
        .map(t => `[${t.speaker}]: ${t.text}`)
        .join('\n');

      const prompt = `è«‹ç‚ºä»¥ä¸‹æœƒè­°ç”Ÿæˆè©³ç´°æ‘˜è¦ï¼Œä»¥ JSON æ ¼å¼å›è¦†ï¼š

æœƒè­°è³‡è¨Šï¼š
- ä¸»é¡Œï¼š${meetingInfo.title}
- åƒèˆ‡è€…ï¼š${meetingInfo.participants.join(', ')}
- æ™‚é•·ï¼š${Math.floor(meetingInfo.duration / 60)} åˆ†é˜

æœƒè­°å…§å®¹ï¼š
${transcriptText}

è«‹å›è¦†ä»¥ä¸‹ JSON æ ¼å¼ï¼š
{
  "summary": "æœƒè­°æ‘˜è¦ï¼ˆ3-5å¥è©±ï¼‰",
  "keyPoints": ["è¦é»1", "è¦é»2", "è¦é»3"],
  "decisions": ["æ±ºç­–1", "æ±ºç­–2"],
  "actionItems": [
    {
      "task": "ä»»å‹™æè¿°",
      "assignee": "è² è²¬äºº",
      "priority": "high/medium/low"
    }
  ],
  "sentiment": "positive/neutral/negative"
}`;

      const response = await this.callOpenAI(prompt, 'gpt-4');
      const result = JSON.parse(response);

      return {
        title: meetingInfo.title,
        summary: result.summary || 'ç„¡æ³•ç”Ÿæˆæ‘˜è¦',
        keyPoints: result.keyPoints || [],
        decisions: result.decisions || [],
        actionItems: result.actionItems || [],
        participants: meetingInfo.participants,
        duration: meetingInfo.duration,
        sentiment: result.sentiment || 'neutral',
      };
    } catch (error) {
      console.error('Error generating meeting summary:', error);
      return this.getDefaultSummary(meetingInfo);
    }
  }

  /**
   * è­˜åˆ¥ç™¼è¨€è€… (Speaker Diarization)
   */
  async identifySpeakers(
    audioBuffer: Buffer,
    knownSpeakers?: Array<{ id: string; name: string; voiceSample?: Buffer }>,
  ): Promise<Map<string, string>> {
    // å¯¦éš›æ‡‰ç”¨ä¸­å¯ä»¥ä½¿ç”¨å°ˆé–€çš„ Speaker Diarization æœå‹™
    // ä¾‹å¦‚ AWS Transcribe, Google Cloud Speech-to-Text ç­‰

    try {
      // ç°¡åŒ–ç‰ˆæœ¬ï¼šæ ¹æ“šéŸ³é »ç‰¹å¾µè­˜åˆ¥ç™¼è¨€è€…
      // é€™è£¡è¿”å›ä¸€å€‹æ¨¡æ“¬çµæœ
      const speakerMap = new Map<string, string>();

      if (knownSpeakers) {
        knownSpeakers.forEach((speaker, index) => {
          speakerMap.set(`speaker_${index}`, speaker.name);
        });
      }

      return speakerMap;
    } catch (error) {
      console.error('Error identifying speakers:', error);
      return new Map();
    }
  }

  /**
   * æ™ºèƒ½æœƒè­°æé†’
   */
  async generateMeetingReminders(
    summary: MeetingSummary,
  ): Promise<Array<{
    type: 'action_item' | 'decision' | 'follow_up';
    content: string;
    priority: 'high' | 'medium' | 'low';
    dueDate?: string;
  }>> {
    const reminders: Array<any> = [];

    // è¡Œå‹•é …æé†’
    summary.actionItems.forEach(item => {
      reminders.push({
        type: 'action_item',
        content: `${item.assignee ? `@${item.assignee} ` : ''}${item.task}`,
        priority: item.priority,
        dueDate: item.dueDate,
      });
    });

    // æ±ºç­–æé†’
    summary.decisions.forEach(decision => {
      reminders.push({
        type: 'decision',
        content: decision,
        priority: 'medium',
      });
    });

    return reminders;
  }

  /**
   * å¯¦æ™‚æœƒè­°åˆ†æ
   */
  async analyzeConversation(
    recentTranscripts: TranscriptionSegment[],
  ): Promise<{
    engagement: number; // 0-100
    speakingTime: Map<string, number>; // æ¯å€‹äººçš„ç™¼è¨€æ™‚é–“
    topicDrift: boolean; // æ˜¯å¦åé›¢ä¸»é¡Œ
    suggestions: string[]; // æœƒè­°å»ºè­°
  }> {
    try {
      const speakingTime = new Map<string, number>();
      let totalTime = 0;

      // è¨ˆç®—ç™¼è¨€æ™‚é–“
      recentTranscripts.forEach(segment => {
        const currentTime = speakingTime.get(segment.speaker) || 0;
        speakingTime.set(segment.speaker, currentTime + 1);
        totalTime += 1;
      });

      // è¨ˆç®—åƒèˆ‡åº¦
      const engagement = this.calculateEngagement(speakingTime, totalTime);

      // ç”Ÿæˆå»ºè­°
      const suggestions = this.generateSuggestions(speakingTime, totalTime);

      return {
        engagement,
        speakingTime,
        topicDrift: false, // å¯ä»¥é€šé AI åˆ†æä¸»é¡Œæ¼‚ç§»
        suggestions,
      };
    } catch (error) {
      console.error('Error analyzing conversation:', error);
      return {
        engagement: 0,
        speakingTime: new Map(),
        topicDrift: false,
        suggestions: [],
      };
    }
  }

  /**
   * æ™ºèƒ½èƒŒæ™¯é™å™ª
   */
  async removeBackgroundNoise(audioBuffer: Buffer): Promise<Buffer> {
    // å¯¦éš›æ‡‰ç”¨ä¸­å¯ä»¥ä½¿ç”¨å°ˆé–€çš„éŸ³é »è™•ç†åº«
    // ä¾‹å¦‚ TensorFlow.js çš„é™å™ªæ¨¡å‹ã€WebRTC çš„éŸ³é »è™•ç†ç­‰

    try {
      // é€™è£¡è¿”å›åŸå§‹éŸ³é »ï¼Œå¯¦éš›æ‡‰ç”¨éœ€è¦å¯¦ç¾é™å™ªç®—æ³•
      return audioBuffer;
    } catch (error) {
      console.error('Error removing background noise:', error);
      return audioBuffer;
    }
  }

  /**
   * æƒ…æ„Ÿåˆ†æ
   */
  async analyzeSentiment(transcript: string): Promise<{
    sentiment: 'positive' | 'neutral' | 'negative';
    score: number;
    emotions: Map<string, number>; // å…·é«”æƒ…ç·’åŠå…¶å¼·åº¦
  }> {
    try {
      const prompt = `åˆ†æä»¥ä¸‹æœƒè­°å°è©±çš„æƒ…æ„Ÿï¼Œå›è¦† JSON æ ¼å¼ï¼š
{
  "sentiment": "positive/neutral/negative",
  "score": -1åˆ°1çš„åˆ†æ•¸,
  "emotions": {
    "happy": 0-1,
    "frustrated": 0-1,
    "excited": 0-1,
    "confused": 0-1
  }
}

å°è©±å…§å®¹ï¼š
${transcript}`;

      const response = await this.callOpenAI(prompt);
      const result = JSON.parse(response);

      return {
        sentiment: result.sentiment || 'neutral',
        score: result.score || 0,
        emotions: new Map(Object.entries(result.emotions || {})),
      };
    } catch (error) {
      console.error('Error analyzing sentiment:', error);
      return {
        sentiment: 'neutral',
        score: 0,
        emotions: new Map(),
      };
    }
  }

  /**
   * æ™ºèƒ½æœƒè­°åŠ©æ‰‹å»ºè­°
   */
  async getSmartSuggestions(
    context: {
      currentTopic: string;
      recentTranscripts: TranscriptionSegment[];
      timeElapsed: number;
      scheduledDuration: number;
    },
  ): Promise<string[]> {
    const suggestions: string[] = [];

    // æ™‚é–“ç®¡ç†å»ºè­°
    const remainingTime = context.scheduledDuration - context.timeElapsed;
    if (remainingTime < context.scheduledDuration * 0.1) {
      suggestions.push('â° æœƒè­°å³å°‡çµæŸï¼Œè«‹æº–å‚™ç¸½çµ');
    }

    // åƒèˆ‡åº¦å»ºè­°
    const analysis = await this.analyzeConversation(context.recentTranscripts);
    if (analysis.engagement < 50) {
      suggestions.push('ğŸ’¬ åƒèˆ‡åº¦è¼ƒä½ï¼Œå»ºè­°é¼“å‹µæ›´å¤šè¨è«–');
    }

    // ç™¼è¨€æ™‚é–“å¹³è¡¡å»ºè­°
    const speakingDistribution = this.analyzeSpeakingDistribution(analysis.speakingTime);
    if (speakingDistribution.isUnbalanced) {
      suggestions.push('ğŸ¤ ç™¼è¨€æ™‚é–“åˆ†é…ä¸å‡ï¼Œå»ºè­°è®“å…¶ä»–äººä¹Ÿåˆ†äº«è§€é»');
    }

    return suggestions;
  }

  // ============ ç§æœ‰æ–¹æ³• ============

  private async callOpenAI(
    prompt: string,
    model: string = 'gpt-3.5-turbo',
  ): Promise<string> {
    if (!this.apiKey) {
      throw new Error('OpenAI API key not configured');
    }

    const response = await fetch(this.apiEndpoint, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.apiKey}`,
      },
      body: JSON.stringify({
        model,
        messages: [
          {
            role: 'system',
            content: 'ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æœƒè­°åŠ©æ‰‹ï¼Œå¹«åŠ©åœ˜éšŠæé«˜æœƒè­°æ•ˆç‡ã€‚',
          },
          {
            role: 'user',
            content: prompt,
          },
        ],
        temperature: 0.7,
        max_tokens: 1500,
      }),
    });

    if (!response.ok) {
      throw new Error(`OpenAI API error: ${response.statusText}`);
    }

    const data = await response.json();
    return data.choices?.[0]?.message?.content?.trim() || '';
  }

  private processTranscription(result: any): TranscriptionSegment[] {
    // è™•ç† Whisper API è¿”å›çš„è½‰éŒ„çµæœ
    const segments: TranscriptionSegment[] = [];

    if (result.segments) {
      result.segments.forEach((segment: any, index: number) => {
        segments.push({
          speaker: `speaker_${index % 3}`, // ç°¡åŒ–ç‰ˆæœ¬ï¼Œå¯¦éš›éœ€è¦ Speaker Diarization
          text: segment.text,
          timestamp: segment.start * 1000,
          confidence: segment.confidence || 1.0,
        });
      });
    } else if (result.text) {
      segments.push({
        speaker: 'speaker_0',
        text: result.text,
        timestamp: Date.now(),
        confidence: 1.0,
      });
    }

    return segments;
  }

  private getDefaultSummary(meetingInfo: any): MeetingSummary {
    return {
      title: meetingInfo.title,
      summary: 'ç„¡æ³•ç”Ÿæˆæœƒè­°æ‘˜è¦',
      keyPoints: [],
      decisions: [],
      actionItems: [],
      participants: meetingInfo.participants,
      duration: meetingInfo.duration,
      sentiment: 'neutral',
    };
  }

  private calculateEngagement(
    speakingTime: Map<string, number>,
    totalTime: number,
  ): number {
    if (speakingTime.size === 0 || totalTime === 0) return 0;

    // è¨ˆç®—ç™¼è¨€åˆ†å¸ƒçš„å‡å‹»åº¦
    const avgTime = totalTime / speakingTime.size;
    let variance = 0;

    speakingTime.forEach(time => {
      variance += Math.pow(time - avgTime, 2);
    });

    variance /= speakingTime.size;
    const stdDev = Math.sqrt(variance);

    // æ¨™æº–å·®è¶Šå°ï¼Œåƒèˆ‡åº¦è¶Šé«˜
    const engagement = Math.max(0, Math.min(100, 100 - (stdDev / avgTime) * 50));

    return Math.round(engagement);
  }

  private generateSuggestions(
    speakingTime: Map<string, number>,
    totalTime: number,
  ): string[] {
    const suggestions: string[] = [];
    const avgTime = totalTime / speakingTime.size;

    speakingTime.forEach((time, speaker) => {
      if (time > avgTime * 2) {
        suggestions.push(`${speaker} ç™¼è¨€æ™‚é–“è¼ƒé•·ï¼Œå¯ä»¥è®“å…¶ä»–äººä¹Ÿåˆ†äº«è§€é»`);
      } else if (time < avgTime * 0.3) {
        suggestions.push(`${speaker} ç™¼è¨€è¼ƒå°‘ï¼Œå¯ä»¥é‚€è«‹åˆ†äº«æƒ³æ³•`);
      }
    });

    return suggestions;
  }

  private analyzeSpeakingDistribution(
    speakingTime: Map<string, number>,
  ): { isUnbalanced: boolean; details: string } {
    if (speakingTime.size === 0) {
      return { isUnbalanced: false, details: 'No speakers' };
    }

    const times = Array.from(speakingTime.values());
    const max = Math.max(...times);
    const min = Math.min(...times);

    // å¦‚æœæœ€å¤§å€¼æ˜¯æœ€å°å€¼çš„ 3 å€ä»¥ä¸Šï¼Œèªç‚ºä¸å¹³è¡¡
    const isUnbalanced = max > min * 3;

    return {
      isUnbalanced,
      details: isUnbalanced
        ? 'ç™¼è¨€æ™‚é–“åˆ†é…ä¸å‡'
        : 'ç™¼è¨€æ™‚é–“åˆ†é…è¼ƒç‚ºå‡è¡¡',
    };
  }
}
